* questions 
*** wha 

* a
*** a
  - Network Pruning
    Hidenori Tanaka

  - Continual Learning
    - Ekdeep singh

  - Effective learning rate of BatchNorm
    Zhiyuan Li, Sanjeev Arora
    

  - Brain-inspired Algorithm SynFlow

  - Lottery Ticket Hypothesis and iterative magnitude Pruning

  - pruning neural networks at initialization
    Why are we missing the mark?

* things to keep in mind
** python
*** packing & unpacking operator (single asterisk)
    - 'list-ify' when used in function definition,
    - 'argument-ify' when used on function call

** pytorch
*** using `nn` module
    import torch
    from torch import nn

*** mean
    - `torch.mean` is 'fusion' inside the specified axis

*** stack
    - `torch.stack` "tensor-ify" an array or tuple;
 
      #+BEGIN_SRC python
        my_tuple = (torch.tensor([1,2,3]), torch.tensor([4,5,6]), torch.tensor([7,8,9]))
        my_stacked = torch.stack(my_tuple)
        my_stacked

        '''
        tensor([[1,2,3,], [4,5,6], [7,8,9]])
        '''
      #+END_SRC

*** broadcast
    https://pytorch.org/docs/stable/notes/broadcasting.html
    two tensors are “broadcastable” if the following rules hold:

    Each tensor has at least one dimension.

    When iterating over the dimension sizes, 
    starting at the trailing dimension, 
    the dimension sizes must either be equal, 
    one of them is 1, or one of them does not exist.


** deep learning
*** tensor sum
    - for explanation, think of a tensor below;

      #+BEGIN_SRC python
        my_tensor = torch.tensor(
            [[[1,2],
              [3,4],
              [5,6]],
             
            [[11,12],
             [13,14],
             [15,16]],
             
            [[21,22],
             [23,24],
             [25,26]]]
        )
      #+END_SRC

    - `my_tensor.sum(dim=0)` does summation over dim0
      and remove the axis on dim0, keeping the shape of dim1 & dim2

    - `my_tensor.sum(dim=1)` does summation over dim1
      and remove the axis on dim1, keeping the shape of dim0 & dim2

    - `my_tensor.sum(dim=2)` does summation over dim2
      and remove the axis on dim2, keeping the shape of dim0 & dim1

[[/Users/shun/Development/study-fastai/lesson_note/figures/studyFastaiNote.org_20200629_155156_40670bu.png]]

    #+BEGIN_SRC python
      #memo
      import torch
      my_tensor = torch.tensor([[[1,2],[3,4],[5,6]],
                   [[11,12],[13,14],[15,16]],
                   [[21,22],[23,24],[25,26]]])
      my_tensor_dim0 = my_tensor.sum(dim=0)
      my_tensor_dim1 = my_tensor.sum(dim=1)
      my_tensor_dim2 = my_tensor.sum(dim=2)
      my_tensor, my_tensor_dim0, my_tensor_dim1, my_tensor_dim2

      '''
      # my_tensor
      tensor([[[ 1,  2],
               [ 3,  4],
               [ 5,  6]],

              [[11, 12],
               [13, 14],
               [15, 16]],

              [[21, 22],
               [23, 24],
               [25, 26]]])

      # my_tensor_dim0
      tensor([[33, 36],
              [39, 42],
              [45, 48]])

      # my_tensor_dim1
      tensor([[ 9, 12],
              [39, 42],
              [69, 72]])

      # my_tensor_dim2
      tensor([[ 3,  7, 11],
              [23, 27, 31],
              [43, 47, 51]])

      '''
    #+END_SRC    

*** normalization & initialization
    - what we care is to have "good activations" during the training
    - normalization is for activations of layers ("input" is considered as the first activation)
    - initialization is for weight, but results in good activation

*** dampening & debiasing 
    - dampening & debiasing are two different things

    - dampening is to multiply the current value by (1-beta)
      when calculating exponentially weighted moving average

    - debiasing is to devide the exponentially weighted moving average 
      by the sum of the coefficients for calculating the 
      exponentially weighted moving average.

      - 1-beta^n+1                 (with dampening)
      - (1-beta^n+1)/1-beta        (without dampening)

    - note that without the dampening, the sum of the coefficients
      becomes just a sum of a proportional sequence (等比数列)

*** statistics for a parameter are tensors, NOT a scalar
    - for example, AverageGrad

*** essential fastai codes
    - Runner ver1
    - Optimizer ver2
    - StatefulOptimizer
    - Learner ver2
    - ParamScheduler
    - BatchNorm

*** elements of a model
    - data
    - model
    - loss function
    - optimizer

*** components of deep learning
    - techniques to initialize parameters & normalize activations in order to keep activations nice
      - kaiming initialization (weight initialization)
      - lsuv (weight initialization)
      - BatchNorm (activation normalization)

    - techniques to manage hyper parameters
      - annealing
      - parameter scheduling

    - techniques to update parameters
      - ADAM
      - LAMB

    - techniques to handle noisy label
      - Label smoothing

    - techniques to augment data
      - affine transformation (for image)
      - mix up

    - techniques to improve network architecture
      - res-net
      - Xres-net

* things to understand
** pyhton
*** zip
    https://realpython.com/python-zip-function/
    
    - "Python’s zip() function is defined as zip(*iterables). 
      The function takes in iterables as arguments and 
      returns an iterator. This iterator generates a 
      series of tuples containing elements from each iterable. 
      zip() can accept any type of iterable, such as
      files, lists, tuples, dictionaries, sets, and so on."
    
*** python > install packages with conda
    https://towardsdatascience.com/a-guide-to-conda-environments-bc6180fc533#5193

*** python > if not

*** python > return value of `and`, `or`

*** function annotation
*** yield

*** iter

** pyotrch
*** auto grad
    https://towardsdatascience.com/getting-started-with-pytorch-part-1-understanding-how-automatic-differentiation-works-5008282073ec
    https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/
    https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e

*** with torch.no_grad
*** why do we zero out gradient?  (1:14:58)
*** zero_grad()
*** .grad.zero_()

*** tensor broadcasting
    https://pytorch.org/docs/master/notes/broadcasting.html#broadcasting-semantics

*** tensor related operation (stack, squeeze)
    https://deeplizard.com/learn/video/kF2AlpykJGY
    https://www.kdnuggets.com/2020/06/fundamentals-pytorch.html

*** tensor mean, std

** DNN misc
*** the relation between the size of the activation std & gradient
    - (?) a loss function is a function of activation of the final layer,
      and
    - (?) see "vanishing gradient problem"
      https://www.youtube.com/watch?v=qhXZsFVxGKo

** CNN
*** why can a kernel detects edge?
    - lecture by andres ng
      https://www.youtube.com/watch?v=XuD4C8vJzEQ&list=PLpFsSf5Dm-pd5d3rjNtIXUHT-v7bdaEIe&index=101

*** what is pooling for?
    - lecture by andres ng
      https://www.youtube.com/watch?v=8oOgPUO-TBY&list=PLpFsSf5Dm-pd5d3rjNtIXUHT-v7bdaEIe&index=108
*** how does the 'face size' of activation change by convolution
    https://deepage.net/deep_learning/2016/11/07/convolutional_neural_network.html
    - determined by the values of padding, stride 
      - Oh = 1 + (H + 2P - Fh) / S
      - Ow = 1 + (W + 2P - Fw) / S
        
        - Oh: output height
        - Ow: output width
        - H: original height
        - W: original width
        - P: padding
        - Fh: Filter height
        - Fw: Filter width
        - S: stride
      
*** how do we determine the right kernel size for CNN?
*** meaning of filters & channels
    - are they used interchangably?

*** what does AdaptiveAvgPool2d & AdaptiveMaxPool2d do?
    
** initialization & normalization
*** why does initialization work?
*** why does BatchNorm works?
    # NeurIPS 2018
    https://www.youtube.com/watch?v=ZOabsYbmBRM

    # paper author lecture
    https://www.youtube.com/watch?v=EvAVCxZJN2U

    # deep lizard
    https://www.youtube.com/watch?v=dXB-KQYkzNU

*** why is it OK not fo define `backward` on `BatchNorm`
    # stack overflow
    https://stackoverflow.com/questions/49594858/how-does-pytorch-module-do-the-back-prop


    # my guess
    - `BatchNorm` inherits `backward` from `nn.Module`,
      which calls `backwrd` on all the `nn.Parameter` instance
      
** regularlization
*** what is regularlization?
*** L2 regularlization
*** weight decay

** optimization
*** TODO how to calculate gradient w.r.t weights for a linear layer
    # 3b1b
    https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5&t=0s

    # 
    https://forums.fast.ai/t/understanding-linear-layer-gradient/63491

    # 
    http://cs231n.stanford.edu/handouts/linear-backprop.pdf
    https://modelpredict.com/batched-backpropagation-connecting-math-and-code/
    
*** TODO how autograd in pytorch works
    https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95
    https://pytorch.org/docs/stable/autograd.html

    https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/

*** L2 regularlization & weight decay
*** debiasing exponentially weighted moving average
    https://www.youtube.com/watch?v=lWzo8CajF5s

*** momentum
    https://www.youtube.com/watch?v=k8fTYJPd3_I&t=1s

*** RMSprop
    https://www.youtube.com/watch?v=_e-LFe_igno

*** ADAM
    https://www.youtube.com/watch?v=JXQT_vxqwIs

*** LAMB
    https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866
    https://arxiv.org/pdf/1904.00962.pdf
    
** architecture design
*** detail of 'bag of tricks' paper which considers
    - how we can make use of all the input we have
    - how we can take advantage of computations we are doing

*** Dropout

** misc
*** average pooling
*** maximum pooling
*** concat pooling

** handling images
*** can we flip, resize while a image is still a byte (before byte tensor, float tensor)
** BERT
*** BERT with fastai 
    https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/
    https://medium.com/@abhikjha/fastai-integration-with-bert-a0a66b1cecbe
    https://towardsdatascience.com/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2

* lesson8
** 00_exports.ipynb
*** exporting 00_exports

    #+BEGIN_SRC sh
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py

    #+END_SRC

** 01_matmul.ipynb
*** importing 01_matmal.ipynb
    #+BEGIN_SRC sh
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 01_matmul.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_01.py /content/nb_01.py

    #+END_SRC

*** unsqueeze
    # 4:49
    https://youtu.be/fCVuiW9AFzY

    https://deeplizard.com/learn/video/fCVuiW9AFzY
    

    #+BEGIN_SRC python
      c = tensor([10., 20, 30])
      '''
      tensor([10., 20., 30.])
      '''

      c.unsqueeze(0)
      '''
      tensor([[10., 20., 30.]])
      '''

      c.unsqueeze(1)
      '''
      tensor([[10.],
              [20.],
              [30.]])
      ''' 
    #+END_SRC

** 02_fully_connected.ipynb grad
*** install the notebooks

    #+BEGIN_SRC python
      # importing nb00 & nb_01
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
    #+END_SRC

*** get_data, normalize
    #+BEGIN_SRC python
      #export
      from exp.nb_01 import *

      def get_data():
          path = datasets.download_data(MNIST_URL, ext='.gz')
          with gzip.open(path, 'rb') as f:
              ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
          return map(tensor, (x_train,y_train,x_valid,y_valid))

      def normalize(x, m, s): return (x-m)/s
    #+END_SRC

*** definition of variance & standard deviation 
    - ref
      https://www.khanacademy.org/math/probability/data-distributions-a1/summarizing-spread-distributions/v/range-variance-and-standard-deviation-as-measures-of-dispersion

    - variance is the average of squared difference
      σ ={ (x1 - m)^2 + (x2 - m)^2 + (x3 - m)^2 } / 3

    - standard deviation is the root of variance
      std = √σ

*** mean & standard deviation of a TENSOR
    # mean
    https://pytorch.org/docs/stable/torch.html#torch.mean
    - mean of a tensor is a rank0-tensor(scalar), and is the average over
      ALL the elements in the input tensor.

    - especially, in lesson8, the mean of a input is
      the average of all the elements in the input
      of size 10000x784 tensor

    # std
    https://pytorch.org/docs/master/generated/torch.std.html
    - std of a tensor is a rank0-tensor(scalar), and the standard-deviation 
      over ALL the elements in the input tensor.


      #+BEGIN_SRC python
        myTensor = torch.tensor([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])
        myTensor, myTensor.mean(), myTensor.std()
        '''
        (tensor([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]]), tensor(5.), tensor(2.7386)
        '''

      #+END_SRC

*** mean & standard-deviation for data (e.g. x_train)
    - recall mean&std of a tensor is a rank0-tensor(scalar)
    - mean&std for training input are calculated 
      over all the elements of the batch which is a 
      10000x784 rank 2 tensor
    
    #+BEGIN_SRC python
      x_train.shape
      '''
      torch.Size([10000, 784])
      '''

      x_train[0].shape
      '''
      torch.Size([784])
      '''

      train_mean,train_std = x_train.mean(),x_train.std()
      train_mean,train_std

      '''
      (tensor(0.1304), tensor(0.3073))
      '''
    #+END_SRC

*** TODO [0/0] normalizing training & validation input (x_train, x_valid)
    - normalization is to subtract the maen and devide by the std
    - by definition, after normalization, mean & std of training 
      set will be 0 and 1, respectively
    - validation set must be normalized using mean & std of 
      TRAINING set
    - [ ] why will mean & std of noramlized validation be near 0 & 1
    
    #+BEGIN_SRC python
      x_train = normalize(x_train, train_mean, train_std)
      # NB: Use training, not validation mean for validation set
      x_valid = normalize(x_valid, train_mean, train_std)

      train_mean,train_std = x_train.mean(),x_train.std()
      train_mean,train_std
      '''
      (tensor(0.0001), tensor(1.))
      '''

      valid_mean, valid_std = x_valid.mean(),x_valid.std()
      valid_mean, valid_std
      '''
      (tensor(-0.0057), tensor(0.9924))
      '''
    #+END_SRC

*** initializing weight
    - initialization means, 
      - set the initial value of the weights by randn()
      - then devide by a constant

    - when the initial value of the weight matrix is set by randn(),
      it gets (mean, std) of (0, 1)

    - when devided by sqrt(m), (mean,std) will be (0, 1/sqrt(m) )

      #+BEGIN_SRC python
        #memo
        # simplified kaiming init / he init
        w1 = torch.randn(m,nh)/math.sqrt(m)
        b1 = torch.zeros(nh)
        w2 = torch.randn(nh,1)/math.sqrt(nh)
        b2 = torch.zeros(1)

        w1.shape, b1.shape, 
        w2.shape, b2.shape
        '''
        (
          torch.Size([784, 50]), torch.Size([50]), 
          torch.Size([50, 1]), torch.Size([1])
        )
        '''

        w1.mean(), w1.std()
        '''
        (tensor(0.0002), tensor(0.0358))
        '''
      #+END_SRC

*** effect of initialization
    - when multiplying the normalized input with (mean, std) of (0, 1)
      with initialized weight with (mean, std) of (0, 1/sqrt(m))
      the resulting ACTIVATION will also have (mean, std) of (0 ,1)

*** TODO why initialization matters
    # forum
    https://forums.fast.ai/t/why-0-mean-and-1-std/57211/4    

    # blog
    https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/

    https://pouannes.github.io/blog/initialization/

    # papers
    http://proceedings.mlr.press/v9/glorot10a.html
    https://arxiv.org/abs/1901.09321

*** squeeze (batch size=3)

    #+BEGIN_SRC python
      import torch
      myTensor1 = torch.tensor([[1], [2],[3]])
      myTensor2 = torch.tensor([[1], [2],[3]])

      myTensor1.squeeze(), myTensor2.squeeze(-1)
      '''
      (tensor([1, 2, 3]), tensor([1, 2, 3]))
      '''
    #+END_SRC
    - `myTensor1` is of shape: (3x1), so `myTensor1.squeeze()` will be of 
      shape (3)
    - `myTensor2.squeeze(-1)` will do squeeze operation on the last dimension
      since the shape of the input is (3x1), and the size for the last dimension 
      is 1, shape will be (3)
      
      # torch documents
      https://pytorch.org/docs/master/generated/torch.squeeze.html

    - squeeze Returns a tensor with all the dimensions of input of size 1 removed.
      For example, if input is of shape:(A×1×B×C×1×D) 
      then the out tensor will be of shape: (A×B×C×D) .

    - When dim is given, a squeeze operation is done only in the given dimension. 
      If input is of shape: (A×1×B) , squeeze(input, 0) 
      leaves the tensor unchanged, but squeeze(input, 1) 
      will squeeze the tensor to the shape (A×B) .

*** squeeze (batch size =1)
    #+BEGIN_SRC python

      myTensor1 = torch.tensor([[3]])
      myTensor2 = torch.tensor([[3]])
      myTensor1.squeeze(), myTensor2.squeeze(-1)

      '''
      (tensor(3), tensor([3]))
      '''

    #+END_SRC

*** printing inside function
    - just add 'import sys' inside cell

      #+BEGIN_SRC python
        import sys
        def forward_and_backward(inp, targ):
            # forward pass:
            l1 = inp @ w1 + b1
            l2 = relu(l1)
            out = l2 @ w2 + b2
            # we don't actually need the loss in backward!
            loss = mse(out, targ)
            
            print(l1.shape)
            print(l2.shape)
            print(out.shape)
            # backward pass:
            mse_grad(out, targ)
            lin_grad(l2, out, w2, b2)
            relu_grad(l1, l2)
            lin_grad(inp, l1, w1, b1)
        '''
        torch.Size([50000, 50])
        torch.Size([50000, 50])
        torch.Size([50000, 1])
        '''
            
      #+END_SRC

*** TODO how to calculate gradient w.r.t weights for a linear layer
    https://forums.fast.ai/t/understanding-linear-layer-gradient/63491
    http://cs231n.stanford.edu/handouts/linear-backprop.pdf
    https://modelpredict.com/batched-backpropagation-connecting-math-and-code/
    
*** TODO how autograd in pytorch works
    https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95
    https://pytorch.org/docs/stable/autograd.html

    https://blog.paperspace.com/pytorch-101-understanding-graphs-and-automatic-differentiation/

*** lin

    #+BEGIN_SRC python
      def lin(x, w, b): return x@w + b
    #+END_SRC

*** relu ver1

    #+BEGIN_SRC python
      def relu(x): return x.clamp_min(0.)
    #+END_SRC

*** relu ver2
    #+BEGIN_SRC python
      # what if...?
      def relu(x): return x.clamp_min(0.) - 0.5
    #+END_SRC

*** model

    #+BEGIN_SRC python
      def model(xb):
          l1 = lin(xb, w1, b1)
          l2 = relu(l1)
          l3 = lin(l2, w2, b2)
          return l3
    #+END_SRC

*** mse

    #+BEGIN_SRC python
      #export
      def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()
    #+END_SRC

    # detail
    - `output` is in the form of torch.Size([50000, 1])
    - `target` is in the form of torch.Size([50000])
    - `output.squeeze(-1)` is necessary to match the shape of `targ`

*** forward & backward ver1

    #+BEGIN_SRC python
      #w1, b1
      # simplified kaiming init / he init
      # w1 = torch.randn(m,nh)
      # b1 = torch.zeros(nh)
      # w2 = torch.randn(nh,1)
      # b2 = torch.zeros(1)

      w1 = torch.randn(m,nh)/math.sqrt(m)
      b1 = torch.zeros(nh)
      w2 = torch.randn(nh,1)/math.sqrt(nh)
      b2 = torch.zeros(1)

      #memo
      w1.shape, b1.shape, w2.shape, b2.shape
      '''
      (torch.Size([784, 50]), torch.Size([50]), torch.Size([50, 1]), torch.Size([1]))
      '''

      # 

      def mse_grad(inp, targ): 
          # grad of loss with respect to output of previous layer
          inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]

      def relu_grad(inp, out):
          # grad of relu with respect to input activations
          inp.g = (inp>0).float() * out.g

      def lin_grad(inp, out, w, b):
          # grad of matmul with respect to input
          inp.g = out.g @ w.t()
          w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)
          b.g = out.g.sum(0)

      def forward_and_backward(inp, targ):
          # forward pass:
          l1 = inp @ w1 + b1
          l2 = relu(l1)
          out = l2 @ w2 + b2
          # we don't actually need the loss in backward!
          loss = mse(out, targ)
          
          # backward pass:
          mse_grad(out, targ)
          lin_grad(l2, out, w2, b2)
          relu_grad(l1, l2)
          lin_grad(inp, l1, w1, b1)    
    #+END_SRC

    - illustration
[[[[/Users/shun/Development/study-fastai/lesson_note/figures/studyFastaiNote.org_20200604_141232_34024h7r.png]]]]

    - `out.g` is in the form of `torch.Size([50000, 50])`
    - `sum(0)` takes sum over the 1st axis, i.e., the batch axis
    - hence, the size of `b.g` is `torch.Size([50])`

      https://pytorch.org/docs/master/generated/torch.sum.html

    - by definition, the grad should be in the form such that
      grad * input will be of the same shape as output

    - grad for mse is rank 2 tensor 

      #+BEGIN_SRC python
        def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()
        '''
        shape for output is [[3],[1],...,[7]] (torch.Size([50000,1])
        '''

        def mse_grad(inp, targ): 
            # grad of loss with respect to output of previous layer
            inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]
        '''
        shape of inp.g is torch.Size([50000,1])
        '''
            
      #+END_SRC

*** forward & backward ver1 with `print` to check the shape of grads

    #+BEGIN_SRC python
      #memo
      import sys
      def mse_grad(inp, targ): 
          # grad of loss with respect to output of previous layer
          inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0]
          print('mse_grad')
          print(inp.g.shape)

      def relu_grad(inp, out):
          # grad of relu with respect to input activations
          inp.g = (inp>0).float() * out.g
          print('relu_grad (inp>0).float()')
          print((inp>0).float().shape)
          print('relu_grad out.g')
          print(out.g.shape)
          print('relu_grad (inp>0).float() * out.g')
          print(inp.g.shape)

      def lin_grad(inp, out, w, b):
          # grad of matmul with respect to input
          inp.g = out.g @ w.t()
          w.g = (inp.unsqueeze(-1) * out.g.unsqueeze(1)).sum(0)
          b.g = out.g.sum(0)
          print('lin_grad, w.t()')
          print(w.t().shape)
          print('lin_grad out.g')
          print(out.g.shape)
          print('lin_grad out.g@w.t()')
          print(inp.g.shape)
          print('lin_grad w.g')
          print(w.g.shape)
          print('lin_grad b.g')
          print(b.g.shape)

      #memo
      import sys
      def forward_and_backward(inp, targ):
          # forward pass:
          l1 = inp @ w1 + b1
          l2 = relu(l1)
          out = l2 @ w2 + b2
          # we don't actually need the loss in backward!
          loss = mse(out, targ)

          print('l1')
          print(l1.shape)
          print('l2')
          print(l2.shape)
          print('out')
          print(out.shape)
          # backward pass:
          mse_grad(out, targ)
          lin_grad(l2, out, w2, b2)
          relu_grad(l1, l2)
          lin_grad(inp, l1, w1, b1)

      forward_and_backward(x_train, y_train)
      '''
      l1
      torch.Size([50000, 50])
      l2
      torch.Size([50000, 50])
      out
      torch.Size([50000, 1])
      mse_grad
      torch.Size([50000, 1])
      lin_grad, w.t()
      torch.Size([1, 50])
      lin_grad out.g
      torch.Size([50000, 1])
      lin_grad out.g@w.t()
      torch.Size([50000, 50])
      lin_grad w.g
      torch.Size([50, 1])
      lin_grad b.g
      torch.Size([1])
      relu_grad (inp>0).float()
      torch.Size([50000, 50])
      relu_grad out.g
      torch.Size([50000, 50])
      relu_grad (inp>0).float() * out.g
      torch.Size([50000, 50])
      lin_grad, w.t()
      torch.Size([50, 784])
      lin_grad out.g
      torch.Size([50000, 50])
      lin_grad out.g@w.t()
      torch.Size([50000, 784])
      lin_grad w.g
      torch.Size([784, 50])
      lin_grad b.g
      torch.Size([50])
      '''
    #+END_SRC

*** forward & backward ver2, Model ver1

    #+BEGIN_SRC python
      class Relu():
          def __call__(self, inp):
              self.inp = inp
              self.out = inp.clamp_min(0.)-0.5
              return self.out
          
          def backward(self): self.inp.g = (self.inp>0).float() * self.out.g

      class Lin():
          def __init__(self, w, b): self.w,self.b = w,b
              
          def __call__(self, inp):
              self.inp = inp
              self.out = inp@self.w + self.b
              return self.out
          
          def backward(self):
              self.inp.g = self.out.g @ self.w.t()
              # Creating a giant outer product, just to sum it, is inefficient!
              self.w.g = (self.inp.unsqueeze(-1) * self.out.g.unsqueeze(1)).sum(0)
              self.b.g = self.out.g.sum(0)

      class Mse():
          def __call__(self, inp, targ):
              self.inp = inp
              self.targ = targ
              self.out = (inp.squeeze() - targ).pow(2).mean()
              return self.out
          
          def backward(self):
              self.inp.g = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / self.targ.shape[0]

      # Model ver1
      class Model():
          def __init__(self, w1, b1, w2, b2):
              self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]
              self.loss = Mse()
              
          def __call__(self, x, targ):
              for l in self.layers: x = l(x)
              return self.loss(x, targ)
          
          def backward(self):
              self.loss.backward()
              for l in reversed(self.layers): l.backward()

      # how to use
      w1.g,b1.g,w2.g,b2.g = [None]*4
      model = Model(w1, b1, w2, b2)

      loss = model(x_train, y_train)

      model.backward()
    #+END_SRC

    - `[None]*4` creates a list [None, None, None, None], 
      and assign each element to `w1.g`, `b1.g`, `w2.g`, `b2.g`
      respectively

*** forward & backward ver3, Model ver2, Module
    #+BEGIN_SRC python
      class Module():
          def __call__(self, *args):
              self.args = args
              self.out = self.forward(*args)
              return self.out
          
          def forward(self): raise Exception('not implemented')
          def backward(self): self.bwd(self.out, *self.args)

      class Relu(Module):
          def forward(self, inp): return inp.clamp_min(0.)-0.5
          def bwd(self, out, inp): inp.g = (inp>0).float() * out.g

      class Lin(Module):
          def __init__(self, w, b): self.w,self.b = w,b
              
          def forward(self, inp): return inp@self.w + self.b
          
          def bwd(self, out, inp):
              inp.g = out.g @ self.w.t()
              self.w.g = torch.einsum("bi,bj->ij", inp, out.g)
              self.b.g = out.g.sum(0)

      class Mse(Module):
          def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean()
          def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0]

      # Model ver2    
      class Model():
          def __init__(self):
              self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)]
              self.loss = Mse()
              
          def __call__(self, x, targ):
              for l in self.layers: x = l(x)
              return self.loss(x, targ)
          
          def backward(self):
              self.loss.backward()
              for l in reversed(self.layers): l.backward()

      # how to use
      w1.g,b1.g,w2.g,b2.g = [None]*4
      model = Model()
              
      loss = model(x_train, y_train)

      model.backward()
    #+END_SRC

    - `model(x_train, y_train)` calls Model::__call__()
      which recursively calls __call__() of each layer.

    - for each layer, __call__() inherited from `Module`
      is called.

    - when __call__() is called, it populates `self.args`,
      and calls forward() method implemented on the layer class.

    - `inp` refered inside `bwd` of each layer is 
      populated as follows

      - Model::backward() is called

      - for each layer, backward() is called

      - in each layer, backward() is inherited from 
        its super class Module, so Module::backward() is called

      - Module::backawrd() calls
        self.bwd(self.out, *self.args)

      - self.args is populated when Module::__call__ is called
        first time, i.e., when each layer is called in the forward path

*** TODO forward & backawrd ver4 (with Model ver2, nn.Module, nn.Linear, nn.ReLU)
    #+BEGIN_SRC python
      # Model ver2
      class Model(nn.Module):
          def __init__(self, n_in, nh, n_out):
              super().__init__()
              self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]
              self.loss = mse
              
          def __call__(self, x, targ):
              for l in self.layers: x = l(x)
              return self.loss(x.squeeze(), targ)

      # how to use
      model = Model(m, nh, 1)

      loss = model(x_train, y_train)

      loss.backward()
    #+END_SRC

    # overview
    - with `nn.Module`, there is a slight difference.
      - Model does not have `backward`, so we do not call `model.backward()`

      - instead, it return the result of calculating loss
        through calling `Model::__call__` ;

      - then we call backward on the calculated loss.

      - [ ] calling the backward on the calculated loss
        recursively calls backward of each 'layer' involved in calculating 
        the loss.

    # Model::__init__
    - `mse` is defined as below
      #+BEGIN_SRC python
        def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean()
      #+END_SRC

    # Model::__call__
    - [ ] is `x.squeeze()` not necessary inside 
      `self.loss(x.squeeze(), targ)` because 
      `output.squeeze(-1)` is called inside `mse`?

* lesson9
** 02b_initializing.ipynb
*** get_data
    #+BEGIN_SRC python

        from exp.nb_01 import *

        def get_data():
            path = datasets.download_data(MNIST_URL, ext='.gz')
            with gzip.open(path, 'rb') as f:
                ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
            return map(tensor, (x_train,y_train,x_valid,y_valid))

        def normalize(x, m, s): return (x-m)/s

      #+END_SRC

    - download and open training & validation data from pickle

*** normalize
          #+BEGIN_SRC python

        from exp.nb_01 import *

        def get_data():
            path = datasets.download_data(MNIST_URL, ext='.gz')
            with gzip.open(path, 'rb') as f:
                ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
            return map(tensor, (x_train,y_train,x_valid,y_valid))

        def normalize(x, m, s): return (x-m)/s

      #+END_SRC

    - normalize an input with mean and standard deviation
      - x is an tensor
      - m is a tensor like tensor(2.387)

** 03_minitabch.ipynb
*** importing the modules

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
    #+END_SRC

*** Model(nn.Module) ver3

      #+BEGIN_SRC python
        class Model(nn.Module):
            def __init__(self, n_in, nh, n_out):
                super().__init__()
                self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]
                
            def __call__(self, x):
                for l in self.layers: x = l(x)
                return x
      #+END_SRC

    - different from `Model(nn.Module)` ver2 implemented in lesson8,
      `Model(nn.Module)` ver3 does not have `self.loss`.

    - instead, the result of forward path is retured through
      `Model(nn.Module)::__call__` , and it will be later passed
      to a loss function

    - then we call backward on the retuned value of loss function

      #+BEGIN_SRC python
        loss = loss_func(model(xb), yb)
        loss.backward()
      #+END_SRC

*** cross entropy
    - cross entropy can be considered as 
      'amount of money' that neural net MAY pay
      if an event actually occurs

    - amount of money to pay for occurence of an event
      is equal to -log(P) where P is the predicted probability
      for event i

    - amount of money to pay decrease if P is big

    - in the context of neural net, cross entropy is 
      an array of a value (a tensor) where
      each element corresponds to the predicted probability
      of an event to occur;
      [-log(P1), -log(P2), -log(P3), ..., -log(Pn)]

*** cross entropy loss
    - cross entropy loss can be considered as
      'amount of money' that neural net must ACTUALLY pay
      on actual occurence of an event

    - cross entropy loss is calculated by multiplying
      one-hot-encoded vector representing the event that ACTUALLY occured
      and a cross entropy tensor;
      [-log(P1), -log(P2), -log(P3), ..., -log(Pn)] * [0, 0, ..., 1, ..., 0]

    - in order to minimize the amount of money to pay,
      neural net does its best to predict large probability
      for the 'events' that actually occurs, and
      small probability for the events that does not actually occur
      - predict large probaility => small payment if an event actually occurs
      - predict small probaility => big payment if an event actually occurs

    - NOTE that the sum of the probailities of events must
      add up to 1, so a neural net cannot get away with
      'paying' little amount of money by predicting 
      large probaility for all the events;
      some probaility must be small, and some probaility must be large

*** log_softmax ver1

    #+BEGIN_SRC python
      def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()
    #+END_SRC

    # overview
    - `log_softmax` calculates (-1)*cross-entropy 
      for prediction `x` 
    
    # detail
    - `x` is a batch of arrays of probatilities
      [[P11, P12, ..., P1m],
       [P21, P22, ..., P2m],
       ...,
       [Pn1, Pn2, ..., Pnm]]

    - `log_softmax(x)` represents (-1)*cross-entropy for `x`
      and is in the form of

      [[log(P11), log(P12), ..., log(P1m)],
       [log(P21), log(P22), ..., log(P2m)],
       ...,
       [log(Pn1), log(Pn2), ..., log(Pnm)]]

    - in the context of 'lesson9_03_minibatch_training.ipynb',
      `x` is prediction for ENTIRE training set, and the shape is 
      `torch.Size([50000,10])`
      - where 50000 is number of images
      - 10 corresonds to the number of labels (0,1,...,9)
      - each row contains values corresponding 10 labels

    - `x.exp()` operates element-wise (i.e. every elements of x (torch.Size([50000,10]) )
      and result is also torch.Size([50000,10]) 

    - `x.exp().sum(-1, keepdim=True)` takes sum along the last axis,
      ,which is along each row (50000 rows),

    - with `keepdim=True`, for each row, after the summation,
      the `x.exp().sum(01, keepdim=True))` will be a rank2 tensor 
      of `torch.Size([50000, 1])` instead of a rank1 tensor
      of `torch.Size([50000])`, with which `x.exp() / (x.exp().sum(-1, keepdim=True))`
      can use broadcasting.

    - x.exp() => torch.Size([50000,10])

    - x.exp().sum(...) => torch.Size([50000,1])

    - `x.exp() / (x.exp().sum(-1, keepdim=True))` is done element-wise
      using broadcasting, and the size of the result is `torch.Size([50000,10])`

    - `log()` happens element-wise, and now the *(-1) of value for
      each single element represents cross-entropy
      (how much suprizing if that heppens)

    - small cross-entropy (i.e. high probability) means it is less suprizing
      when it happnes

*** the output of log_softmax ver1

    # my log softmax with print
    #+BEGIN_SRC python
      #memo
      import sys
      def my_log_softmax(x): 
        print(x.exp().shape)
        print(x.exp().sum(-1, keepdim=True).shape)
        print((x.exp()/(x.exp().sum(-1,keepdim=True))).log())
        return (x.exp()/(x.exp().sum(-1,keepdim=True))).log()

      my_log_softmax(pred)
      '''
      torch.Size([50000, 10])
      torch.Size([50000, 1])
      tensor([[-2.2712, -2.2481, -2.4425,  ..., -2.0968, -2.3603, -2.3217],
              [-2.3567, -2.2983, -2.4205,  ..., -1.9925, -2.3187, -2.1940],
              [-2.3799, -2.2564, -2.4033,  ..., -2.1573, -2.2746, -2.3387],
              ...,
              [-2.2725, -2.3115, -2.4781,  ..., -2.0977, -2.3022, -2.3321],
              [-2.3403, -2.2541, -2.3410,  ..., -2.1249, -2.3481, -2.3359],
              [-2.2687, -2.1631, -2.3720,  ..., -2.2085, -2.3852, -2.3751]],
             grad_fn=<LogBackward>)
      '''

    #+END_SRC

*** integer array indexing
    - `sm_pred` is an array of arrays

    - sm_pred[ [0, 1, 2], [5, 0, 4]  ] grabs 0, 1, 2 of sm_pred

    - for sm_pred[0], instead of grabbing entire array,
      just grab index 5 of sm_pred[0]

    - similarly, for sm_pred[1], it grabs index 0 of sm_pred[1]

    #+BEGIN_SRC python
      sm_pred[[0,1,2], [5,0,4]]
    #+END_SRC

*** nll

    #+BEGIN_SRC python
      def nll(input, target): return -input[range(target.shape[0]), target].mean()
    #+END_SRC

    # overview
    - `nll` calculates cross-entropy LOSS from (-1)*cross-entropy
      for a batch of prediction 

    # details
    - `input` is (-1)*cross-entropy for a batch of prediction
      and is in the form of

       [[log(P11), log(P12), ..., log(P1m)],
       [log(P21), log(P22), ..., log(P2m)],
       ...,
       [log(Pn1), log(Pn2), ..., log(Pnm)]]

    - `target` is the label which is in the form of
      [0, 0, ..., 1, ..., 0] 

    - since `target` (the label) is one-hot encoded, 
      we can utilize numpy integer indexing.

    - `target.shape[0]` is same as the number of data

    - `range(target.shape[0])` returns a "range object"
      where `n` is the number of data
      https://thepythonguru.com/python-builtin-functions/range/#:~:text=The%20range()%20function%20is,efficient%20to%20handle%20large%20data.


    #+BEGIN_SRC python
      range(5)
      range(0, 5)
      list(range(5)) # list() call is not required in Python 2

      '''
      [0, 1, 2, 3, 4]
      '''

    #+END_SRC


    - using integer array indexing, 
      `-input[range(target.shape[0]), target]` extracts
      the part of cross entropy which corresponds to the
      labels for the batch

      [
       -log(P1i),
       -log(P2j),
       -log(P3k),
       ...
      ]

    - then,  `.mean()` returns the mean over the batch.

*** log_softmax ver2

    #+BEGIN_SRC python
      def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log()
    #+END_SRC

*** logsumexp (home-made)
    #+BEGIN_SRC python
      def logsumexp(x):
          m = x.max(-1)[0]
          return m + (x-m[:,None]).exp().sum(-1).log()
    #+END_SRC

    - `x` is a batch of prediction and is in the form of
       [[(P11), (P12), ..., (P1m)],
       [(P21), (P22), ..., (P2m)],
       ...,
       [(Pn1), (Pn2), ..., (Pnm)]]      

    - `x.max(-1)` returns a namedtuple (values, indices) where values
      is the maximum value of each row of the input tensor 
      in the given dimension dim. And indices is the index 
      location of each maximum value found (argmax).


      #+BEGIN_SRC python
      a = torch.randn(4, 4)
      a
      tensor([[-1.2360, -0.2942, -0.1222,  0.8475],
              [ 1.1949, -1.1127, -2.2379, -0.6702],
              [ 1.5717, -0.9207,  0.1297, -1.8768],
              [-0.6172,  1.0036, -0.6060, -0.2432]])
      torch.max(a, 1)

      '''
      torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), 
      indices=tensor([3, 0, 0, 1]))
      '''
    #+END_SRC

      # definiton of `max` from pytorch doc
      https://pytorch.org/docs/stable/generated/torch.max.html

    - `m[:,None]` adds an axis to transform the shape of
      `m` from `torch.Size([50000])` to `torch.Size([50000,1])`
      so that `m` can be subtracted from `x`.

      # 
      https://numpy.org/doc/stable/reference/arrays.indexing.html#numpy.newaxis
      https://stackoverflow.com/questions/31492699/use-of-none-in-array-indexing-in-python

*** logsumexp (PyTorch)
    https://pytorch.org/docs/master/generated/torch.logsumexp.html

*** log_softmax ver3 (with PyTorch's logsumexp)

    #+BEGIN_SRC python
      def log_softmax(x): return x - x.logsumexp(-1,keepdim=True)
    #+END_SRC

*** log_softmax ver4 (PyTorch)

*** cross_entropy loss (PyTorch)
    https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1functional_1a29daa086ce1ac3cd9f80676f81701944.html
    https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html

*** accuracy
    # code
    #+BEGIN_SRC python
      def accuracy(out, yb):
          return (torch.argmax(out, dim=1)==yb).float().mean()
    #+END_SRC

    # notes
    - the size of `out` is `torch.Size([64, 10])`
    - the size of `yb` is `torch.Size([64])`
    - torch.argmax(out, dim=1) picks up the index of the 
      largest element for each row and returns a tensor
      whose size is `torch.Size([64])`

      #+BEGIN_SRC python
      tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 8,
               8, 8, 4, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 4, 8, 8,
               8, 8, 1, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8, 8, 4],
              grad_fn=<NotImplemented>)
    #+END_SRC

    - torch.argmax(out, dim=1)==yb returns a tensor;

      #+BEGIN_SRC python
        tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
                 True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
                 True,  True,  True,  True, False,  True,  True,  True, False, False,
                 True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
                 True,  True,  True,  True,  True,  True,  True,  True, False,  True,
                 True,  True,  True,  True,  True,  True,  True,  True,  True,  True,
                 True,  True,  True,  True])
      #+END_SRC

    - torch.argmax(out, dim=1)==yb.float() converts to 
      a tensor of 0 & 1

    - [ ] 

      #+BEGIN_SRC python
        tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,
                1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,
                1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
                1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])
      #+END_SRC
      
*** TODO basic training loop (fit ver0)

    #+BEGIN_SRC python
      for epoch in range(epochs):
          for i in range((n-1)//bs + 1):
      #         set_trace()
              start_i = i*bs
              end_i = start_i+bs
              xb = x_train[start_i:end_i]
              yb = y_train[start_i:end_i]
              loss = loss_func(model(xb), yb)

              loss.backward()
              with torch.no_grad():
                  for l in model.layers:
                      if hasattr(l, 'weight'):
                          l.weight -= l.weight.grad * lr
                          l.bias   -= l.bias.grad   * lr
                          l.weight.grad.zero_()
                          l.bias  .grad.zero_()
    #+END_SRC

    - `n` is the size of the training data

    - `(n-1)//bs` returns the largest integer less than `(n-1)/bs`
      - `//` does integer devision, 
        whereas `/` does floating point devision

    - `(n-1)//bs` determines the largest value of `start_i`

    # - `(n-1)//bs` is to be read as
    #   "the data size minus one devided by the batch size"
    #   which calculates the biggest possible value for `start_i`

    - the largest value of `start_i` should be the value such that
      (batch size) * (max of `start_i`)
      is less than `n`, the size of the training data, 
      in order to make the last batch contain at least one element.

    - we can get such a number by subtracting 1 from `n` 
      and deviding it by `bs`;
      `bs` * `(n-1)//bs` will be smaller than `n` at least by 1

    - for better understanding above, think of an example below
       n=9, bs=3, last-index=8, (n-1)//bs=2, max of `start_i`=6

       |---+---+---+---+---+---+---+---+---+
       |   |   |   |   |   |   |   |   |   |
       |---+---+---+---+---+---+---+---+---+
       |   |   |   | 
       |---+---+---+

       n=8, bs=3, last-index=7, (n-1)//bs=2, max of `start_i`=6

       |---+---+---+---+---+---+---+---+
       |   |   |   |   |   |   |   |   |
       |---+---+---+---+---+---+---+---+
       |   |   |   | 
       |---+---+---+


    - `for i in range((n-1)//bs +1)` sweeps integers 
      from 0 to (n-1)//bs

      #+BEGIN_SRC python
         import sys
         my_n=10
         my_bs=3
         print((my_n-1)//my_bs)

         for i in range((my_n-1)//my_bs + 1):
           print(i)

         '''
         3
         0
         1
         2
         3

         '''
       #+END_SRC

       #+BEGIN_SRC python
         n,m = x.train.shape
       #+END_SRC

    - `xb = x_train[start_i:end_i]` returns the elements of `x_train`
      from index `start_i` up to `end_i-1`

      #+BEGIN_SRC python
         #memo
         my_array1 = [0,1,2,3]
         my_array1[0:2]

         '''
         [0, 1]
         '''
      #+END_SRC

    - `x_train[start_i:end_i]` works fine even when `end_i`
      exceeds the size of `x_train`
      
      #+BEGIN_SRC python
     #memo
     my_array1 = [0,1,2,3]
     my_array1[0:10]

     '''
     [0, 1, 2, 3]
     '''
    #+END_SRC

    - `Model` is from `Model(nn.Module)` ver3

       #+BEGIN_SRC python
         class Model(nn.Module):
             def __init__(self, n_in, nh, n_out):
                 super().__init__()
                 self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]
                
             def __call__(self, x):
                 for l in self.layers: x = l(x)
                 return x
       #+END_SRC

    - [ ] `with torch.no_grad()` is necessary since otherwise
      pytorch's auto grad mechanics would keep track of
      all the calculation where `weight` is involved, for calculating
      gradient, which costs lot of memoris

*** Model(DummyModule) (=simplified replica of Model(nn.Module) ver4)
    
    #+BEGIN_SRC python
        class Model(DummyModule):
            def __init__(self, n_in, nh, n_out):
                super().__init__()
                self.l1 = nn.Linear(n_in,nh)
                self.l2 = nn.Linear(nh,n_out)
                
            def __call__(self, x): return self.l2(F.relu(self.l1(x)))

      class DummyModule():
          def __init__(self, n_in, nh, n_out):
              self._modules = {}
              self.l1 = nn.Linear(n_in,nh)
              self.l2 = nn.Linear(nh,n_out)
              
          def __setattr__(self,k,v):
              if not k.startswith("_"): self._modules[k] = v
              super().__setattr__(k,v)
              
          def __repr__(self): return f'{self._modules}'
          
          def parameters(self):
              for l in self._modules.values():
                  for p in l.parameters(): yield p
    #+END_SRC

    # DummyModule overview
    - `DummuModule` is for getting rid of the duplicate code
      for updating the parameters for each layer in 
      the 'basic training loop'

      # redundant part of 'basic training loop' (fit ver0)
      #+BEGIN_SRC python
        loss.backward()
        with torch.no_grad():
            for l in model.layers:
                if hasattr(l, 'weight'):
                    l.weight -= l.weight.grad * lr
                    l.bias   -= l.bias.grad   * lr
                    l.weight.grad.zero_()
                    l.bias  .grad.zero_()
      #+END_SRC

      # instead, would like to do this:
      #+BEGIN_SRC python
        loss.backward()
        with torch.no_grad():
            for p in model.parameters(): p -= p.grad * lr
            model.zero_grad()
      #+END_SRC

    # DummyModule overview
    - what `DummyModule` does is, when some properties such 
      as `l1`, `l2` is set on `self` (`Model` instance),
      it will call __setattr__ which registers the property 
      and its value to `self._modules`

    - `DummyModule` also implements `parameters` which yields
      all the parameters of each of all the layers registered
      to self._modules through __setattr__, and hence enables
      us to loop through the parameters.

    # DummyModule detail
    - `super().__setattr__(k,v)` calls `__setattr__` of
      the super class of DummyModule, which is 
      Python Object class.

    - `l.parameters()` calls `parameters` method
      implemented on a PyTorch layer class such as `nn.Linear`

*** Model(nn.Module) ver4 (also look at Model(DuumyModule) above)

    #+BEGIN_SRC python
      class Model(nn.Module):
          def __init__(self, n_in, nh, n_out):
              super().__init__()
              self.l1 = nn.Linear(n_in,nh)
              self.l2 = nn.Linear(nh,n_out)
              
          def __call__(self, x): return self.l2(F.relu(self.l1(x)))
    #+END_SRC

    - luckily, the 2 features(__setattr__ , parameters)
      of `DummyModule` is also implemented  on `nn.Module`, 
      so instead of `Model(DummyModule)`, we can use
      `Model(nn.Module)` ver4

*** fit ver1
    #+BEGIN_SRC python

      # Model(nn.Module) ver4
      class Model(nn.Module):
          def __init__(self, n_in, nh, n_out):
              super().__init__()
              self.l1 = nn.Linear(n_in,nh)
              self.l2 = nn.Linear(nh,n_out)
              
          def __call__(self, x): return self.l2(F.relu(self.l1(x)))

      # create Model instance
      model = Model(m, nh, 10)

      # fit ver1
      def fit():
          for epoch in range(epochs):
              for i in range((n-1)//bs + 1):
                  start_i = i*bs
                  end_i = start_i+bs
                  xb = x_train[start_i:end_i]
                  yb = y_train[start_i:end_i]
                  loss = loss_func(model(xb), yb)

                  loss.backward()
                  with torch.no_grad():
                      for p in model.parameters(): p -= p.grad * lr
                      model.zero_grad()
    #+END_SRC

    - this refacroring is for getting rid of the duplicate code
      for updating the parameters for each layer

      # the duplicate code in 'basic training loop' (fit ver0)
      #+BEGIN_SRC python
        for l in model.layers:
            if hasattr(l, 'weight'):
                l.weight -= l.weight.grad * lr
                l.bias   -= l.bias.grad   * lr
                l.weight.grad.zero_()
                l.bias  .grad.zero_()
      #+END_SRC

*** Model(nn.Module) ver5
    
    #+BEGIN_SRC python
      class Model(nn.Module):
          def __init__(self, layers):
              super().__init__()
              self.layers = layers
              for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)
              
          def __call__(self, x):
              for l in self.layers: x = l(x)
              return x
     #+END_SRC

    # overview
    - a version of `Model` class so that we can use 'layers approach'
      as in `Model(nn.Module)` ver2, or `Model(nn.Module)` ver3,
      where we can write forward path succinctly by 
      recursively calling a layer of the models whereas
      the forward path is hard coded with `Model(nn.Module)` ver4;
      
      #+BEGIN_SRC python
        # Model(nn.Module) ver2:: __call__ 
        def __call__(self, x):
            for l in self.layers: x = l(x)
            return x
      #+END_SRC


      #+BEGIN_SRC python
      # Model(nn.Model) ver4 :: __call__
      def __call__(self, x): return self.l2(F.relu(self.l1(x)))

      #+END_SRC

    - however, with 'layers' approach, `__setattr__` which
      registers a layer to `self._modules` won't be
      called anymore as was the case with `Model(nn.Module)` ver4
      (its implementation is almost like `Model(DummyModule)` )
      and so `parameters` method which refers to `self._modules`
      would not work.

    #+BEGIN_SRC python
      # Model(DummyModule)  (simplified replica of Model(nn.Model) ver4)
      def __setattr__(self,k,v):
          if not k.startswith("_"): self._modules[k] = v
          super().__setattr__(k,v)
          

      def parameters(self):
          for l in self._modules.values():
              for p in l.parameters(): yield p
    #+END_SRC

    - in order to resolve the problem,  we need to somehow
      register each layer as a module, and `self.add_module`
      inheritted from nn.Module does the job;
      `self.add_module` is called on `__init__` and it registers
      each layer contained in `layers` in the same way as `__setattr__`

*** SequentialModel (=Model ver6)

    #+BEGIN_SRC python
      class SequentialModel(nn.Module):
          def __init__(self, layers):
              super().__init__()
              self.layers = nn.ModuleList(layers)
              
          def __call__(self, x):
              for l in self.layers: x = l(x)
              return x
    #+END_SRC

    # overview
    - it is for getting rid of cranky part of `__init__` 
      of 'Model(nn.Model) ver5' by using `nn.ModuleList`

      #+BEGIN_SRC python
        # Model(nn.Model) ver5
        class Model(nn.Module):
            def __init__(self, layers):
                super().__init__()
                self.layers = layers
                for i,l in enumerate(self.layers): self.add_module(f'layer_{i}', l)
      #+END_SRC

*** nn.Sequential
    - `nn.Sequential` is PyTorch equivalent of 'SequentialModel'  

*** Optimizer ver1

    #+BEGIN_SRC python
      class Optimizer():
          def __init__(self, params, lr=0.5): self.params,self.lr=list(params),lr
              
          def step(self):
              with torch.no_grad():
                  for p in self.params: p -= p.grad * lr

          def zero_grad(self):
              for p in self.params: p.grad.data.zero_()
    #+END_SRC

    # overview
    - `Optimizer` instance is an object which refers to the parameters 
      of a model, and is responsible for upating the parameters

    - we create optimizer to make the parameter update in fit ver1 
      more concise by factoring out the loop which sweeps through
      the model parameters.

      #+BEGIN_SRC python
        # fit ver1
        def fit():
            for epoch in range(epochs):
                for i in range((n-1)//bs + 1):
                    start_i = i*bs
                    end_i = start_i+bs
                    xb = x_train[start_i:end_i]
                    yb = y_train[start_i:end_i]
                    loss = loss_func(model(xb), yb)

                    loss.backward()
                    with torch.no_grad():
                        # `parameters` is inheritted from `nn.Module`
                        for p in model.parameters(): p -= p.grad * lr
                        model.zero_grad()


      #+END_SRC

    - we will implement more general optimizer later in the lesson

      # detail
      - `__init__` recieves `params` which are all the parameters
        for a model, and add it to `self.params`

*** training loop ver2 (cnosidered as fit ver2), refactored with Optimizer

    #+BEGIN_SRC python
      #
      model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
      opt = Optimizer(model.parameters())

      #
      for epoch in range(epochs):
          for i in range((n-1)//bs + 1):
              start_i = i*bs
              end_i = start_i+bs
              xb = x_train[start_i:end_i]
              yb = y_train[start_i:end_i]
              pred = model(xb)
              loss = loss_func(pred, yb)

              loss.backward()
              opt.step()
              opt.zero_grad()
    #+END_SRC

*** optim
    - optim is the pytorch equivalent of `Optimizer` implemented above

*** get_model ver1

    #+BEGIN_SRC python
      def get_model():
          model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
          return model, optim.SGD(model.parameters(), lr=lr)
    #+END_SRC

    - this is for creating a pair of model AND optimizer
      - optimizer is initialized with the model.parameters()

*** Dataset

    #+BEGIN_SRC python
      #export
      class Dataset():
          def __init__(self, x, y): self.x,self.y = x,y
          def __len__(self): return len(self.x)
          def __getitem__(self, i): return self.x[i],self.y[i]
    #+END_SRC

    # overview
    - `Dataset` pairs up two independent lists `x` and `y`
      so that `ds[i]` returns a tuple (x[i], y[i])

    - `Dataset` is for removing clunky part of fit ver2
      for grabbing batch `xb`, `yb`

      #+BEGIN_SRC python
        # fit ver2
        for epoch in range(epochs):
            for i in range((n-1)//bs + 1):
                start_i = i*bs
                end_i = start_i+bs
                xb = x_train[start_i:end_i]
                yb = y_train[start_i:end_i]
                pred = model(xb)
                loss = loss_func(pred, yb)

                loss.backward()
                opt.step()
                opt.zero_grad()
      #+END_SRC

    # __getitem__ detail
    - `__getitem__` defines what will be returned when
      `ds[i]` is called where `ds` is a `Dataset` instance

    - `__getitem__` returns a tuple `(self.x[i], self.y[i])`

    - when range of index, like `[i*bs : i*bs+bs]` as in train_ds[i*bs : i*bs+bs], 
      is passed to `__getitem__`, it passes along that range of
      index, and returns a tuple of part of arrays

      (self.x[i*bs : i*bs+bs], self.y[i*bs : i*bs+bs])

*** training loop ver3 (fit ver3), refactored with dataset

    #+BEGIN_SRC python
      #
      loss_func = F.cross_entropy

      #
      model,opt = get_model()
      train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)

      #
      for epoch in range(epochs):
          for i in range((n-1)//bs + 1):
              xb,yb = train_ds[i*bs : i*bs+bs]
              pred = model(xb)
              loss = loss_func(pred, yb)

              loss.backward()
              opt.step()
              opt.zero_grad()
    #+END_SRC

    # detail

*** home-made DataLoader ver1

    #+BEGIN_SRC python
      class DataLoader():
          def __init__(self, ds, bs): self.ds,self.bs = ds,bs
          def __iter__(self):
              for i in range(0, len(self.ds), self.bs): yield self.ds[i:i+self.bs]
    #+END_SRC

    # overview
    - `DataLoader` returns a subset of `ds`

    - `ds` can be an instance of a class which implements
      `__getitem__` such as `Dataset` instance, or `ListItem`

    - `DataLoader` is for removing for-loop in fit ver3 by
      returning a part of `DataSet` instance on calling `__iter__`

      # fit ver3
      #+BEGIN_SRC python
        for epoch in range(epochs):
            for i in range((n-1)//bs + 1):
                xb,yb = train_ds[i*bs : i*bs+bs]
                pred = model(xb)
                loss = loss_func(pred, yb)

                loss.backward()
                opt.step()
                opt.zero_grad()
      #+END_SRC
      
    - `DataLoader` accepts an instance of `DataSet` implemented above,
      and it will return a part of `Dataset` instance through
      `__iter__` which is called in the following way

      #+BEGIN_SRC python
        train_dl = DataLoader(train_ds, bs)

        xb,yb = next(iter(train_dl))

      #+END_SRC

    # detail
    - `ds` is an `DataSet` instance
    - `bs` is  batch size
    - `for i in range(0, len(self.ds), self.bs)` 
      is equivalent to
      `for i in range((n-1)//bs + 1)`

*** fit ver4, refactored with home-made DataLoader ver1

    #+BEGIN_SRC python
      model,opt = get_model()

      def fit():
          for epoch in range(epochs):
              for xb,yb in train_dl:
                  pred = model(xb)
                  loss = loss_func(pred, yb)
                  loss.backward()
                  opt.step()
                  opt.zero_grad()
    #+END_SRC

    # detail
    - `get_model` is implemented above

      #+BEGIN_SRC python
        def get_model():
            model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
            return model, optim.SGD(model.parameters(), lr=lr)
      #+END_SRC

    - with fit ver4, `model`, `loss_func`, `opt`, `train_dl` are not yet
      defined to be arguments of `fit`, but will be arguments with fit ver5

*** home-made Sampler

    #+BEGIN_SRC python
      class Sampler():
          def __init__(self, ds, bs, shuffle=False):
              self.n,self.bs,self.shuffle = len(ds),bs,shuffle
              
          def __iter__(self):
              self.idxs = torch.randperm(self.n) if self.shuffle else torch.arange(self.n)
              for i in range(0, self.n, self.bs): yield self.idxs[i:i+self.bs]

      # how to use
      x_train,y_train,x_valid,y_valid = get_data()
      train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)

      small_ds = Dataset(*train_ds[:10])
      s = Sampler(small_ds,3,False)
      [o for o in s]

      ''''
      [tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7, 8]), tensor([9])]
      ''''
    #+END_SRC


    # overview
    - `Sampler` is for factoring out & generalizing the logic
      in `DataLoader` to specify indexes for slicing a `DataSet` instance
      
      # logic for creating indexes in fit ver3
      #+BEGIN_SRC python
        for i in range((n-1)//bs + 1):
            xb,yb = train_ds[i*bs : i*bs+bs]
      #+END_SRC

    - `Sampler` retunrs an array of indexes (more strictly, rank1 tensor) on calling `__iter__`

    - note that this 'home-made Sampler' will be used
      only with 'home-made DataLoader'

    # - there are difference between 'home-made Sampler'
    #   and PyTorch version of `Sampler`.
    #   - 'home-made' version returns a list of list of indexes
    #   - PyTorch version returnsa a list of indexed

    # detail
    - torch.randperm generates an array of integers from an integers

      #+BEGIN_SRC python
        my_n = 10000
        my_perm_n = torch.randperm(my_n)
        my_perm_n

        '''
        tensor([2294, 6962, 6917,  ..., 7317, 2931, 3086])
        '''
      #+END_SRC

    - `for i in range(0, self.n, self.bs)` is equivalent to
      `for i in range((n-1)//bs + 1)`

    - `train_ds[:10]` returns a tuple (x[0:10], y[0:10])

    - `*train_ds[:10]` unpacks the tuple above and pass it
      to `Dataset`
      
*** home-made DataLoader ver2, collate

    #+BEGIN_SRC python
      def collate(b):
          xs,ys = zip(*b)
          return torch.stack(xs),torch.stack(ys)

      class DataLoader():
          def __init__(self, ds, sampler, collate_fn=collate):
              self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn

          def __iter__(self):
              for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])
    #+END_SRC

    # home-made DataLoader ver2 overview
    - `home-made DataLoader` ver2 picks up a part of data set
      randomly making use of `Sampler` instance whereas
      `home-made DataLoader ver1` picks up a part of data set
      in order

    - 'home-made DataLoader ver2' is just for showing the 
      idea of the implementation of PyTorch version of `DataLoader`,
      and will not be used later in the course.

    - PyTorch version of `DataLoader` is different 
      from the 'home-made DataLoader ver2 in that
      # - `sampler` should be a list of indexes, instead of 
      #   a list of list of indexes

      - `batch_size` must be passed to `DataLoader`


    # home-made DataLoader ver2 > __iter__ detail
    - `ds` is a `Dataset` instance,
      and so `self.ds[i]` is a tuple
      
      # Dataset
      #+BEGIN_SRC python
        class Dataset():
              def __init__(self, x, y): self.x,self.y = x,y
              def __len__(self): return len(self.x)
              def __getitem__(self, i): return self.x[i],self.y[i]
      #+END_SRC

    - `sampler` is a `Sampler` instance which yields
      an array of indexes(rank1 tensor) on calling its `__iter__`
      method by `for...in` syntax

    - `collate_fn` is a function like `collate` implmented above,
      and it combines the tuples in the list
      `[self.ds[i] for i in s]` and returns
      a tuple of 2 tensors (one is for x, and the other is for y)

    - `[self.ds[i] for i in s]` as in `collate_fn([self.ds[i] for i in s])`
      is a python syntax called "list construction"

    - note that `DataLoader` itself does not do transform.
      it is `ds` passed that could do transform.
      especially, `ItemList`, `ImageItemList` does transform, 
      and `DataSet` does NOT do transform

    # collate overview
    - `collate` 'batch-ify' a list of tuples;
      it creates a batch of x and corresponding y 
      out of a list of tuples of xi and yi like
      [(x1,y1), (x2,y2), ...]

    - the batch consists of a tensor for xs and a tensor for ys

    # collate detail
    - `b` is a list of tuples which looks like `my_b` below

      #+BEGIN_SRC python
        my_b = [train_ds[3], train_ds[1], train_ds[2]]
        type(my_b), type(my_b[0])

        '''
        (list, tuple)
        '''
      #+END_SRC

    - each tuple in list `b` is a pair of xi and yi
      (input and its label)

      #+BEGIN_SRC python
        my_b[0][0].size(), my_b[0][1]

        '''
        torch.Size([784]), tensor(1)
        '''
      #+END_SRC

    - `*` as in `zip(*b)`:
      the single asterisk * is called 'unpacking operator', 
      and it 'argument-ify' a list;
      it passes the elements in a list to a function 
      as its argument.

      For example, 
      #+BEGIN_SRC python
        my_b = [train_ds[3], train_ds[1], train_ds[2]]

        # below two are equivalent
        zip(*my_b) 
        zip(train_ds[3], train_ds[1], train_ds[2])
      #+END_SRC

    - zip as in `zip(*b)`:
      https://realpython.com/python-zip-function/
      https://www.youtube.com/watch?v=VbBozykILZ0

      `zip` accepts several tupples, and collects the 
      elements in 'x-axis', 'y-axis', 'z-axis', so on
      as tuples.

      To understand visually, think of the following exmaple

      | tuple1: | t1x | t1y |
      | tuple2: | t2x | t2y |
      | tuple3: | t3x | t3y |
      | ...     | ... | ... |
      | tupleN: | tNx | tNy |


      The iterator that zip returns generates a seriese of tuples
      as below;
      
      (t1x, t2x, t3x, ..., tNx), 
      (t1y, t2y, t3y, ..., tNy)

    - `torch.stack` "tensor-ify" a tuple;
      `torch.stack(xs)` converts a tuple (`xs`) to a rank2 tensor
      by stacking them together

*** validation loss

    #+BEGIN_SRC python
      def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
          for epoch in range(epochs):
              # Handle batchnorm / dropout
              model.train()
      #         print(model.training)
              for xb,yb in train_dl:
                  loss = loss_func(model(xb), yb)
                  loss.backward()
                  opt.step()
                  opt.zero_grad()

              model.eval()
      #         print(model.training)
              with torch.no_grad():
                  tot_loss,tot_acc = 0.,0.
                  for xb,yb in valid_dl:
                      pred = model(xb)
                      tot_loss += loss_func(pred, yb)
                      tot_acc  += accuracy (pred,yb)
              nv = len(valid_dl)
              print(epoch, tot_loss/nv, tot_acc/nv)
          return tot_loss/nv, tot_acc/nv
    #+END_SRC

    # detail
    - `model.train()` sets `model.training` true
    - `mode.eval()`  sets `model.training` false
    - we always call `model.train()` before training, 
      and `model.eval()` before inference, because
      these are used by layers such as `nn.BatchNorm2d`
      and `nn.Dropout` to ensure appropriate behaviour 
      for these different phases.
    - for validation, we do not need to keep track of the 
      calculations during the forward path since we do not need gradient,
      hence we call `with torch.no_grad()`

*** fit ver5 (factor out arguments from fit ver4)
    #+BEGIN_SRC python
      def get_model():
          model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10))
          return model, optim.SGD(model.parameters(), lr=lr)

      model,opt = get_model()

      def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
          for epoch in range(epochs):
              for xb,yb in train_dl:
                  pred = model(xb)
                  loss = loss_func(pred, yb)
                  loss.backward()
                  opt.step()
                  opt.zero_grad()
    #+END_SRC

    # overview
    - with fit ver5, `model`, `loss_func`, `opt`, `train_dl`
      is defined to be arguments of `fit`

*** get_dls ver1

    #+BEGIN_SRC python
      def get_dls(train_ds, valid_ds, bs, **kwargs):
          return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
                  DataLoader(valid_ds, batch_size=bs*2, **kwargs))
    #+END_SRC

    # overview
    - `get_dls` is just a convinent helper for applyinbg DataLoader
      to both of train_ds, valid_ds and combine them together

    # detail
    - `DataLoader` is from `PyTorch`, and not the 'home-made' `DataLoader`,
      but implementation is idendical to 'home-made' `DataLoader`;

    #+BEGIN_SRC python
      def collate(b):
          xs,ys = zip(*b)
          return torch.stack(xs),torch.stack(ys)

      class DataLoader():
          def __init__(self, ds, sampler, collate_fn=collate):
              self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn

          def __iter__(self):
              for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])

    #+END_SRC


    - DL for valid_ds  has bigger batch size than DL for training_ds
      because it is used with torch.no_grad(), and thus
      we have more memory space (1:14:00)

*** why we zero out gradient (1:14:58)

** 04_callbacks.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
    #+END_SRC

*** getting data

    #+BEGIN_SRC python
      x_train,y_train,x_valid,y_valid = get_data()
      train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)
      nh,bs = 50,64
      c = y_train.max().item()+1
      loss_func = F.cross_entropy
    #+END_SRC

    # detail
    - `get_data` is from 02_fully_connected.ipynb

      #+BEGIN_SRC python

        from exp.nb_01 import *

        def get_data():
            path = datasets.download_data(MNIST_URL, ext='.gz')
            with gzip.open(path, 'rb') as f:
                ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')
            return map(tensor, (x_train,y_train,x_valid,y_valid))

        def normalize(x, m, s): return (x-m)/s

      #+END_SRC

*** DataBunch ver1

    #+BEGIN_SRC python
      #export
      class DataBunch():
          def __init__(self, train_dl, valid_dl, c=None):
              self.train_dl,self.valid_dl,self.c = train_dl,valid_dl,c
              
          @property
          def train_ds(self): return self.train_dl.dataset
              
          @property
          def valid_ds(self): return self.valid_dl.dataset
    #+END_SRC

    # overview
    - DataBunch bundle together training DL & validation DL

*** get_model ver2, Learner ver1

    #+BEGIN_SRC python
      # get_dls ver1
      def get_dls(train_ds, valid_ds, bs, **kwargs):
          return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
                  DataLoader(valid_ds, batch_size=bs*2, **kwargs))

      # create data with DataBunch implemented above
      data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)

      # definition of get_model
      def get_model(data, lr=0.5, nh=50):
          m = data.train_ds.x.shape[1]
          model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))
          return model, optim.SGD(model.parameters(), lr=lr)

      class Learner():
          def __init__(self, model, opt, loss_func, data):
              self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data
    #+END_SRC

    # get_model overview
    - get_model ver2 accepts `DataBunch` object whereas
      get_model ver1 does not accept data, and just return
      model and optimizer

    - get model ver2 use DataBunch object to determine
      number of input, and the size of the final output
      for creating model, and it does not really use the `data`
      itself.

    # Learner overview
    - `Learner` is just a storage containing (model, opt, loss_func, data)
      to be passed to fit ver5, as is mentioned above

    - later in the course, `Learner` will be merged to `Runner`
      since `Learner` just stores things and does nothing

    - (model, opt) can be obtained through get_model ver2

*** fit ver6

    #+BEGIN_SRC python
      def fit(epochs, learn):
          for epoch in range(epochs):
              learn.model.train()
              for xb,yb in learn.data.train_dl:
                  loss = learn.loss_func(learn.model(xb), yb)
                  loss.backward()
                  learn.opt.step()
                  learn.opt.zero_grad()

              learn.model.eval()
              with torch.no_grad():
                  tot_loss,tot_acc = 0.,0.
                  for xb,yb in learn.data.valid_dl:
                      pred = learn.model(xb)
                      tot_loss += learn.loss_func(pred, yb)
                      tot_acc  += accuracy (pred,yb)
              nv = len(learn.data.valid_dl)
              print(epoch, tot_loss/nv, tot_acc/nv)
          return tot_loss/nv, tot_acc/nv
    #+END_SRC

    # overview
    - with `fit` ver6, the things necessary
      (model, loss_func, opt, train_dl, valid_dl) are
      packaged p as `learn`

    - since now `learn` contains all of
      (model, opt, loss_func, data),now the signature of fit is;
      `fit(epochs, learn)`
      
*** fit ver7

    #+BEGIN_SRC python
      def one_batch(xb,yb):
          pred = model(xb)
          loss = loss_func(pred, yb)
          loss.backward()
          opt.step()
          opt.zero_grad()

      def fit():
          for epoch in range(epochs):
              for b in train_dl: one_batch(*b)
    #+END_SRC

    # overview
    - with `fit` ver7, training logic of `fit` ver4 is factored out
      as `one_batch`

    - `fit` ver7 is just for illustrating how to factor out 
      training logic, and omitting features in `fit` ver6,
      such as including (model, loss_func, opt ) as its arguments,
      and having validation.
      These features will be included again in `fit` ver8

    #+BEGIN_SRC python
      # fit ver4
      model,opt = get_model()

      def fit():
          for epoch in range(epochs):
              for xb,yb in train_dl:
                  pred = model(xb)
                  loss = loss_func(pred, yb)
                  loss.backward()
                  opt.step()
                  opt.zero_grad()

    #+END_SRC

*** fit ver8

    #+BEGIN_SRC python
      def one_batch(xb, yb, cb):
          if not cb.begin_batch(xb,yb): return
          loss = cb.learn.loss_func(cb.learn.model(xb), yb)
          if not cb.after_loss(loss): return
          loss.backward()
          if cb.after_backward(): cb.learn.opt.step()
          if cb.after_step(): cb.learn.opt.zero_grad()

      def all_batches(dl, cb):
          for xb,yb in dl:
              one_batch(xb, yb, cb)
              if cb.do_stop(): return

      def fit(epochs, learn, cb):
          if not cb.begin_fit(learn): return
          for epoch in range(epochs):
              if not cb.begin_epoch(epoch): continue
              all_batches(learn.data.train_dl, cb)
              
              if cb.begin_validate():
                  with torch.no_grad(): all_batches(learn.data.valid_dl, cb)
              if cb.do_stop() or not cb.after_epoch(): break
          cb.after_fit()
    #+END_SRC

    # overview
    - with fit ver8,  the training logic of fit ver6 is factored out

      #+BEGIN_SRC python
        # fit ver6
        def fit(epochs, learn):
            for epoch in range(epochs):
                learn.model.train()
                for xb,yb in learn.data.train_dl:
                    loss = learn.loss_func(learn.model(xb), yb)
                    loss.backward()
                    learn.opt.step()
                    learn.opt.zero_grad()

                learn.model.eval()
                with torch.no_grad():
                    tot_loss,tot_acc = 0.,0.
                    for xb,yb in learn.data.valid_dl:
                        pred = learn.model(xb)
                        tot_loss += learn.loss_func(pred, yb)
                        tot_acc  += accuracy (pred,yb)
                nv = len(learn.data.valid_dl)
                print(epoch, tot_loss/nv, tot_acc/nv)
            return tot_loss/nv, tot_acc/nv
       #+END_SRC


    - with fit ver8, training logic of fit ver6
      is factored out as `all_batches` and `one_batch`

    - inside `fit`, several callbacks are called additionally
      - cb.begin_fit(learn)    (beggining of fit)
      - cb.begin_epoch(epoch)  (beggining of epoch)
      - cb.begin_validate()    (beggining of validate)
      - cb.after_epoch()       (end of epoch)
      - cb.after_fit()         (end of fit)

    # detail
    - `cb` is an instance of `Callback` which will be defined later.

    - `if not cb.begin_fit(learn): return` means
      if `cb.begin_fit(learn)` returns falsy value, 
      `fit` stops execution and returns.

    - in python, the return value of a function defaults
      to `NaN`, a falsy value, so unless `cb.begin_fit` returns 
      a trusy value, `fit` stops and returns.

    - on the other hand, 
      `if not cb.begin_epoch(epoch): continue` means
      if `cb.begin_epoch(epoch)` is falsy, `fit` continues the 
      for loop

    - `if cb.do_stop() or not cb.after_epoch(): break` means
      if `cb.do_stop()` is true or `cb.after_epoch()` is false,
      it breaks
      
*** Callback ver1

    #+BEGIN_SRC python
      class Callback():
          def begin_fit(self, learn):
              self.learn = learn
              return True
          def after_fit(self): return True
          def begin_epoch(self, epoch):
              self.epoch=epoch
              return True
          def begin_validate(self): return True
          def after_epoch(self): return True
          def begin_batch(self, xb, yb):
              self.xb,self.yb = xb,yb
              return True
          def after_loss(self, loss):
              self.loss = loss
              return True
          def after_backward(self): return True
          def after_step(self): return True
    #+END_SRC

    # overview
    - `Callback` class defines the callback methods called
      inside fit ver8 (begin_fit, after_fit, etc)

    - callback methods(begin_fit, begin_epoch, etc) 
      of `Callback` class does minimum things such as
      initializing properties such as `learn`, `epoch`,
      and more concrete logics will be implemented inside 
      callback methods of a subclass inheritting `Callback` class.

    - also, each callback returns a boolean value to signal
      either or not to stop;

*** CallbackHandler

    #+BEGIN_SRC python
      class CallbackHandler():
          def __init__(self,cbs=None):
              self.cbs = cbs if cbs else []

          def begin_fit(self, learn):
              self.learn,self.in_train = learn,True
              learn.stop = False
              res = True
              for cb in self.cbs: res = res and cb.begin_fit(learn)
              return res

          def after_fit(self):
              res = not self.in_train
              for cb in self.cbs: res = res and cb.after_fit()
              return res
          
          def begin_epoch(self, epoch):
              self.learn.model.train()
              self.in_train=True
              res = True
              for cb in self.cbs: res = res and cb.begin_epoch(epoch)
              return res

          def begin_validate(self):
              self.learn.model.eval()
              self.in_train=False
              res = True
              for cb in self.cbs: res = res and cb.begin_validate()
              return res

          def after_epoch(self):
              res = True
              for cb in self.cbs: res = res and cb.after_epoch()
              return res
          
          def begin_batch(self, xb, yb):
              res = True
              for cb in self.cbs: res = res and cb.begin_batch(xb, yb)
              return res

          def after_loss(self, loss):
              res = self.in_train
              for cb in self.cbs: res = res and cb.after_loss(loss)
              return res

          def after_backward(self):
              res = True
              for cb in self.cbs: res = res and cb.after_backward()
              return res

          def after_step(self):
              res = True
              for cb in self.cbs: res = res and cb.after_step()
              return res
          
          def do_stop(self):
              try:     return self.learn.stop
              finally: self.learn.stop = False
    #+END_SRC

    # overview
    - `CallbackHandler` bundles `Callback` subclass instances
      into `cbs` ("callback instance group"), and for each of 
      callback methods (begin_fit, begin_batch, etc),
      it sweeps through all the `Callback` subclass instances
      registered to the "callback instance group", and call
      the corresponding method.

    - in each of the callback methods (begin_fit, begin_batch, etc)
      the return value `res` is updated everytime the
      corresonding callback method of a `Callback` subclass
      instance in the "callback instance group" is called,
      and `res` will be true only when all of the callback methods
      returns true.

    # detail
    `cbs if cbs else []` is similar to ternary operator x?a:b in javascript

*** putting (fit ver8, CallbackHandler, Callback) all together

    #+BEGIN_SRC python
      # fit ver8, one_batch, all_batches (represented)
      def one_batch(xb, yb, cb):
          if not cb.begin_batch(xb,yb): return
          loss = cb.learn.loss_func(cb.learn.model(xb), yb)
          if not cb.after_loss(loss): return
          loss.backward()
          if cb.after_backward(): cb.learn.opt.step()
          if cb.after_step(): cb.learn.opt.zero_grad()

      def all_batches(dl, cb):
          for xb,yb in dl:
              one_batch(xb, yb, cb)
              if cb.do_stop(): return

      def fit(epochs, learn, cb):
          if not cb.begin_fit(learn): return
          for epoch in range(epochs):
              if not cb.begin_epoch(epoch): continue
              all_batches(learn.data.train_dl, cb)
              
              if cb.begin_validate():
                  with torch.no_grad(): all_batches(learn.data.valid_dl, cb)
              if cb.do_stop() or not cb.after_epoch(): break
          cb.after_fit()

      # an example of `Callback` subclass
      class TestCallback(Callback):
          def begin_fit(self,learn):
              super().begin_fit(learn)
              self.n_iters = 0
              return True
              
          def after_step(self):
              self.n_iters += 1
              print(self.n_iters)
              if self.n_iters>=10: self.learn.stop = True
              return True

      fit(1, learn, cb=CallbackHandler([TestCallback()]))
      '''
      1
      2
      3
      4
      5
      6
      7
      8
      9
      10
      '''
    #+END_SRC

    # TestCallback overview 
    - `TestCallback` implements simple callbacks as follows;
      - begin_fit: 
        initialize `n_iter` which keeps track of the index of the current step

      - after_step: 
        keep track of the training steps in a epoch and set learn.stop flag true
        when the current step exceeds 10, which results in throwing away the rest of the batches
        and stop the training

    - `TestCallback` is registered to the "callback instance group"
      of `CallbackHandler` just by initializing `CallbackHandler`
      instance with `TestCallback` instance

*** listify

    #+BEGIN_SRC python
      #export
      from typing import *

      def listify(o):
          if o is None: return []
          if isinstance(o, list): return o
          if isinstance(o, str): return [o]
          if isinstance(o, Iterable): return list(o)
          return [o]
    #+END_SRC

*** Runner ver1 (= fit ver9)

    #+BEGIN_SRC python
      # Runner class containing fit ver9    
      class Runner():
          def __init__(self, cbs=None, cb_funcs=None):
              cbs = listify(cbs)
              for cbf in listify(cb_funcs):
                  cb = cbf()
                  setattr(self, cb.name, cb)
                  cbs.append(cb)
              self.stop,self.cbs = False,[TrainEvalCallback()]+cbs

          @property
          def opt(self):       return self.learn.opt
          @property
          def model(self):     return self.learn.model
          @property
          def loss_func(self): return self.learn.loss_func
          @property
          def data(self):      return self.learn.data

          def one_batch(self, xb, yb):
              self.xb,self.yb = xb,yb
              if self('begin_batch'): return
              self.pred = self.model(self.xb)
              if self('after_pred'): return
              self.loss = self.loss_func(self.pred, self.yb)
              if self('after_loss') or not self.in_train: return
              self.loss.backward()
              if self('after_backward'): return
              self.opt.step()
              if self('after_step'): return
              self.opt.zero_grad()

          def all_batches(self, dl):
              self.iters = len(dl)
              for xb,yb in dl:
                  if self.stop: break
                  self.one_batch(xb, yb)
                  self('after_batch')
              self.stop=False

          def fit(self, epochs, learn):
              self.epochs,self.learn = epochs,learn

              try:
                  for cb in self.cbs: cb.set_runner(self)
                  if self('begin_fit'): return
                  for epoch in range(epochs):
                      self.epoch = epoch
                      if not self('begin_epoch'): self.all_batches(self.data.train_dl)

                      with torch.no_grad(): 
                          if not self('begin_validate'): self.all_batches(self.data.valid_dl)
                      if self('after_epoch'): break
                  
              finally:
                  self('after_fit')
                  self.learn = None

          def __call__(self, cb_name):
              for cb in sorted(self.cbs, key=lambda x: x._order):
                  f = getattr(cb, cb_name, None)
                  if f and f(): return True
              return False
    #+END_SRC
    
    # overview
    - `Runner` ver1 (=fit ver9) is for removing redundant
      reference to `cb` in `fit` ver8 when calling callbacks
      and instead call them as `self('begin_fit')`,
      `self('begin_batch')`, etc

    - with `Runner` ver1 (=fit ver9), 
      `fit`, `all_batch`, `one_batch` are methods defined
      on `Runner` class

    - callback methods are subsequently called by the following
      order from `one_batch` which is called from `all_batch`
      inside `fit`.

      - `begin_batch`
      - `after_pred`
      - `after_loss`
      - `after_backward`
      - `after_step`


    # overview > how `Learner` instance is passed
    - with `Runner` ver1 (=fit ver9), a `learn` (a `Learner` instance)
      which  contains (model, opt, loss_func, data) is passed 
      dynamically to `fit` method, which is SAME AS fit ver8.

    - however, with Runner ver1 (fit ver9), 
      `fit` method makes the `Learner` instance as the property
      of the `Runner` instance by `self.epochs,self.learn = epochs,learn`

    - with this and special @property methods (def opt, def model, etc)
      (model, opt, loss_func, data) of `learn`
      becomes accessible from within `one_batch`, `all_batches`
      and `fit` itself as if it is a property of the `Runner`
      instance; `self.model`, `self.opt`

    # overview > how callbacks are passed and accessed
    - with fit ver9, `CallbackHandler` instance which bundles
      `Callback` subclass instances is NOT passed to `fit`.

    - instead, instances of a `Callback` ver2 subclass are directly
      passed in either of the 2 ways below:
      - 1. `Callback` subclass CONSTRUCTORS are passed as a list
          to `cb_funcs` argument of  __init__ of Runner class

      - 2. `Callback` subclass INSTANCES are passed to `cbs` argument of
           __init__ of Runner class.

      (NOTE: note again that the class of each of the callback
      instances is inheritting `Callback` ver2, not `Callback` ver1)

    - with the 1st way, inside `__init__` of Runner class, an instance
      of each `Callback` subclass is created, and 2 things are done 
      for each of the `Callback` subclass instances;

      - 1. auto-register itself as a property of the Runner
        instance with key equal to `name` property of the 
        `Callback` subclass instances. 
        
        # example1: learn.Recorder in real fastai 
        # (`Learn` in real fastai is equivalent to `Runner`)

      - 2. append the instance of the `Callback` subclass to 
        `self.cbs` which is later refered to inside `__init__`
        when sweeping through  the `Callback` subclass instances
        and call 'begin_epoch', 'begin_batch' of each of the instances
        (NOTE: TrainEvalCallback is appended as a default callback)

        #+BEGIN_SRC python
          for cbf in listify(cb_funcs)
              cb = cbf()
              setattr(self, cb.name, cb)
              cbs.append(cb)
        #+END_SRC

    - when a callback is passed as a CONSTRUCTOR (the 1st way)
      the callback instance created during init becomes
      available as the property of the `Runner` instance,
      whereas when a callback is passed as an object,
      we need to refer to the callback object to
      refers to statistics which the callback calculates

      # 2 ways to pass an instance of a `Callback` subclass
      #+BEGIN_SRC python
        # pass as an object
        learn = Learner(*get_model(data), loss_func, data)
        stats = AvgStatsCallback([accuracy])
        run = Runner(cbs=stats)
        run.fit(2, learn)

        loss,acc = stats.valid_stats.avg_stats
        assert acc>0.9
        loss,acc

        # pass as a constructor
        acc_cbf = partial(AvgStatsCallback,accuracy)
        run = Runner(cb_funcs=acc_cbf)
        run.fit(1, learn)
        run.avg_stats.valid_stats.avg_stats

      #+END_SRC

    - in the very beggining of `fit`, each instance of 
      `Callback` ver2 subclass is passed an reference
      to the instance of `Runner`;

      #+BEGIN_SRC python
        `for cb in self.cbs: cb.set_runner(self)`
      #+END_SRC

    - this way, each instance of `Callback` ver2 subclass
      has a reference to the runner instance, and can
      pass it to an instance of a helper class
      which utilizes input, prediction, labels, etc
        
    # `fit` detail
    - `cb` is an instance of a subclass of `Callback` ver2 which will be
      implmeneted later.

    - `cb.set_runner` stores the reference to the `Runner` instance 
      on each instance of `Callback` ver2 subclass in order for
      it to record statistics it calculates on the `Runner`
      instance.

    - `self('begin_fit')`, `self('begin_epoch')` calls
      `__call__` method

    - `fit` is implemented to run all_batches() unless
      the return value of `self('begin_epoch')` explicitly
      returns truesy value.

      (If the return value of self('begin_epoch') returns `false`
      it keeps going. In python, return value of a function is default to NaN, 
      which is converted to `false`, so 
      - if a function does not return any value, 
        or explicitly returns a falsy value, `fit` keeps going

      - if a function returns a truesy value, `fit` stops
      
    # `__call__` detail
    - it sweeps through the `Callback` subclass instances 
      stored in `self.cbs` according to its `_order` property,
      and for each of the `Callback` subclass instances, 
      it grabs the callback method (begin_epoch, begin_batch, etc)
      by `f = getattr(cb, cb_name, None`

    - if the callback method exists, it is called, 
      and if the return value of the callback method is `True`,
      it returns `True`, otherwise `False`

*** Callback ver2
      #+BEGIN_SRC python
        # Callback ver2
        import re

        _camel_re1 = re.compile('(.)([A-Z][a-z]+)')
        _camel_re2 = re.compile('([a-z0-9])([A-Z])')
        def camel2snake(name):
            s1 = re.sub(_camel_re1, r'\1_\2', name)
            return re.sub(_camel_re2, r'\1_\2', s1).lower()

        class Callback():
            _order=0
            def set_runner(self, run): self.run=run
            def __getattr__(self, k): return getattr(self.run, k)
            @property
            def name(self):
                name = re.sub(r'Callback$', '', self.__class__.__name__)
                return camel2snake(name or 'callback')
      #+END_SRC

    # overview
    - Callback ver2 does NOT force a subclass of `Callback` ver2
      to implement callback methods (`begin_fit`, `begin_batch`, etc) any more,
      and instead, each subclass implements only those necessary.

    - with Callback ver2, an instance of `Callback` subclass
      stores reference to the `Runner instance` to which 
      it is passed.

    - this way, an instance of `Callback` subclass can record
      statistics it calculates on the `Runner` instance.

    # __getattr__ detail
    - when asked for some property which is not implemented
      on the `Callback` subclass ifself, it searchs the 
      property of the same name defined on the `Runner` instance.
      (this pattern of delegation can be seen frequently in fastai library)

    - by this delegation, an instance of `Callback` subclass 
      can read AND write a property of the `Runner` instance
      it is refering to as if it is the property of the instance
      of `Callback` subclass.

    - for example, look at
      `self.in_train` in `after_loss` of `AvgStatsCallback` (it will be implemented later);

      #+BEGIN_SRC python
        #export
        class AvgStats():
            def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train
            
            def reset(self):
                self.tot_loss,self.count = 0.,0
                self.tot_mets = [0.] * len(self.metrics)
                
            @property
            def all_stats(self): return [self.tot_loss.item()] + self.tot_mets
            @property
            def avg_stats(self): return [o/self.count for o in self.all_stats]
            
            def __repr__(self):
                if not self.count: return ""
                return f"{'train' if self.in_train else 'valid'}: {self.avg_stats}"

            def accumulate(self, run):
                bn = run.xb.shape[0]
                self.tot_loss += run.loss * bn
                self.count += bn
                for i,m in enumerate(self.metrics):
                    self.tot_mets[i] += m(run.pred, run.yb) * bn

        class AvgStatsCallback(Callback):
            def __init__(self, metrics):
                self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)
                
            def begin_epoch(self):
                self.train_stats.reset()
                self.valid_stats.reset()
                
            def after_loss(self):
                stats = self.train_stats if self.in_train else self.valid_stats
                with torch.no_grad(): stats.accumulate(self.run)
            
            def after_epoch(self):
                print(self.train_stats)
                print(self.valid_stats)
      #+END_SRC

    # `name` detail
    - `name` behaves as a property of an instance of `Callback`
      by `@property` syntax

    - `name` removes "Callback" strings from the 
      name of a callback class, and turn it into snake case

      #+BEGIN_SRC python
        TrainEvalCallback().name
        '''
        train_eval
        '''
      #+END_SRC

*** TrainEvalCallback ver1

    #+BEGIN_SRC python
      #export
      class TrainEvalCallback(Callback):
          def begin_fit(self):
              self.run.n_epochs=0.
              self.run.n_iter=0
          
          def after_batch(self):
              if not self.in_train: return
              self.run.n_epochs += 1./self.iters
              self.run.n_iter   += 1
              
          def begin_epoch(self):
              self.run.n_epochs=self.epoch
              self.model.train()
              self.run.in_train=True

          def begin_validate(self):

              self.model.eval()
              self.run.in_train=False
    #+END_SRC

    # overview
    - `TrainEvalCallback` is a default callback registered to
      `cbs` property of a `Runner`
      
      # Runner __init__
      #+BEGIN_SRC python
        def __init__(self, cbs=None, cb_funcs=None):
            cbs = listify(cbs)
            for cbf in listify(cb_funcs):
                cb = cbf()
                setattr(self, cb.name, cb)
                cbs.append(cb)
            self.stop,self.cbs = False,[TrainEvalCallback()]+cbs
      #+END_SRC

    - `TrainEvalCallback` is responsible for managing if the `Runner`
      instance is in training, or validation 
      by switching `self.in_train` flag of the `Runner` instance
      inside `begin_validate`.

    - `self.in_train` of the `Runner` instance will be refered to
      from inside an instance of a `Callback` ver2 subclass
      such as `AvgStatsCallback`

    # begin_epoch detail
    - `TrainEvalCallback` inherits `Callback` ver2, 
      and `self.run` is set to a reference to an instance of `Runner`
      inside `set_runner`

    #+BEGIN_SRC python
      # Callback ver2
        class Callback():
            _order=0
            def set_runner(self, run): self.run=run
            def __getattr__(self, k): return getattr(self.run, k)
            @property
            def name(self):
                name = re.sub(r'Callback$', '', self.__class__.__name__)
                return camel2snake(name or 'callback')
    #+END_SRC

*** AvgStats & AvgStatsCallback ver1

    #+BEGIN_SRC python
      #export
      import sys
      class AvgStats():
         def __init__(self, metrics, in_train): self.metrics,self.in_train = listify(metrics),in_train
          
          def reset(self):
              self.tot_loss,self.count = 0.,0
              self.tot_mets = [0.] * len(self.metrics)
              
          @property
          def all_stats(self): return [self.tot_loss.item()] + self.tot_mets
          @property
          def avg_stats(self): 

            # print('self.tot_loss is:')
            # print(self.tot_loss)
            # print('self.tot_loss.item() is')
            # print(self.tot_loss.item())

            return [o/self.count for o in self.all_stats]
          
          def __repr__(self):
              if not self.count: return ""
              return f"{'train' if self.in_train else 'valid'}: {self.avg_stats}"

          def accumulate(self, run):
              bn = run.xb.shape[0]
              self.tot_loss += run.loss * bn
              self.count += bn
              for i,m in enumerate(self.metrics):
                  self.tot_mets[i] += m(run.pred, run.yb) * bn

                  class AvgStatsCallback(Callback):
          def __init__(self, metrics):
              self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)
              
          def begin_epoch(self):
              self.train_stats.reset()
              self.valid_stats.reset()
              
          def after_loss(self):
              stats = self.train_stats if self.in_train else self.valid_stats
              with torch.no_grad(): stats.accumulate(self.run)
          
          def after_epoch(self):
              print(self.train_stats)
              print(self.valid_stats)


      # how to use 
      learn = Learner(*get_model(data), loss_func, data)

      stats = AvgStatsCallback([accuracy])
      run = Runner(cbs=stats)

      run.fit(2, learn)
      '''
      self.tot_loss is:
      tensor(15825.9590)
      self.tot_loss.item() is
      15825.958984375
      train: [0.3165191796875, tensor(0.9034)]

      self.tot_loss is:
      tensor(1606.2327)
      self.tot_loss.item() is
      1606.232666015625
      valid: [0.1606232666015625, tensor(0.9529)]

      self.tot_loss is:
      tensor(7192.6704)
      self.tot_loss.item() is
      7192.67041015625
      train: [0.143853408203125, tensor(0.9566)]

      self.tot_loss is:
      tensor(1330.4731)
      self.tot_loss.item() is
      1330.47314453125
      valid: [0.133047314453125, tensor(0.9616)]
      '''
    #+END_SRC

    # AvgStats overview
    - it is the machinery which actually performs the
      calculations of the averages for some metricses

    - it itself does not contain the data necessary for calculating
      the averages

    - the actual data is passed through an instance of 
      `AvgStatsCallback`, which is a subclass of `Callback` ver2

    # AvgStats:: __init__ detail
    - `metrics` is an array of functions, e.g., [accuracy]

      #+BEGIN_SRC python
        def accuracy(out, yb):
            return (torch.argmax(out, dim=1)==yb).float().mean()
      #+END_SRC

    # AvgStats:: all_stats detail
    - the return value will be an array of float numbers

    - `self.tot_loss` records the accumulated sum, over all the batches,
      of the losses weighted by batch size.
      it will be a rank0 tensor of the form `tensor(15825.9590)`.
      in order to get the raw number, we need to call `.item()`

      #+BEGIN_SRC python
        #memo
        my_tot_loss= tensor(15825.9590)
        my_tot_loss
        '''
        tensor(15825.9590)
        '''

        raw_tot_loss = my_tot_loss.item()
        raw_tot_loss
        '''
        15825.9590
        '''
      #+END_SRC

    - `self.tot_mets` is an array of tensors
      each element of which is the sum, over all the batches,
      of "some_metric_function(prediction, label)" weighted by
      the batch size

    #+BEGIN_SRC python
      import sys
      tot_mets = [0. ] * 2 
      print(tot_mets)

      '''
      [0.0, 0.0]
      '''
    #+END_SRC

      tot_mets = [0. ] * 2 
      will be 
      [0.0, 0.0]

    # AvgStats::avg_stats detail
    - it returns the weighted average of a metrics over all the batches.
      (the batch size of each batch is used as the weight)

    - `self.count` is the sum of the batch size over all the batches
      which is initialized to be 0 inside `reset`
  
    # AvgStats::accumulate detail
    - `accumulate` is called by an instance of `AvgStatsCallback` 

    - `run` is an instance of `Runner` an instance of 
      `Callback` ver2 subclass is refering to.

    - `bn` is the size of the curent batch

    - `self.tot_loss += run.loss * bn` 
      is a weighted sum of the loss over all the batches

    - `self.tot_mets[i] += m(run.pred, run.yb) * bn`
      is a weighted sum of a metrics(e.g. `accuracy`)
      over all the batches.

    # AvgStatsCallback overview
    - `AvgStatsCallback` is a subclass of `Callback` ver2,
      and an adapter for `AvgStats`

    - it contains an instance of `AvgStats`

    - it itself is just a 'mediator' of data, 
      and it delegates the calculation of the statistics
      (the average) to the instance of `AvgStats`
      by passing the reference to an instance of `Runner`
      at `stats.accumulate(self.run)`

    # AvgStatsCallback::after_loss detail
    - `with torch.no_grad` is necessary because
      we are still in training phase, but do not
      need gradient calculation for `accumulate` to run
      
      https://stackoverflow.com/questions/55627780/evaluating-pytorch-models-with-torch-no-grad-vs-model-eval

    # how to use detail
    - `accuracy` is as below

        #+BEGIN_SRC python
      def accuracy(out, yb):
          return (torch.argmax(out, dim=1)==yb).float().mean()
    #+END_SRC
      
*** using AvgStatsCallback

    #+BEGIN_SRC python
      acc_cbf = partial(AvgStatsCallback,accuracy)
      run = Runner(cb_funcs=acc_cbf)
      run.fit(1, learn)

      run.avg_stats.valid_stats.avg_stats
      '''
      [0.12098992919921875, tensor(0.9649)]
      '''
    #+END_SRC

    # overview
    - an instance of `AvgStatsCallback` is passed 
      to `Runner` instance through the constructor of
      `AvgStatsCallback`, i.e., `acc_cbf`, 
      and the instance of `AvgStatsCallback`
      will be accessible via run.avg_stats

      # Runner::__init__
      #+BEGIN_SRC python
        def __init__(self, cbs=None, cb_funcs=None):
            cbs = listify(cbs)
            for cbf in listify(cb_funcs):
                cb = cbf()
                setattr(self, cb.name, cb)
                cbs.append(cb)
            self.stop,self.cbs = False,[TrainEvalCallback()]+cbs
      #+END_SRC


      # Callback ver2::name
      #+BEGIN_SRC python
        def name(self):
            name = re.sub(r'Callback$', '', self.__class__.__name__)
            return camel2snake(name or 'callback')

        # e.g. `AvgStatsCallback` will be "avg_stats"
          
      #+END_SRC

** statistics math
*** standard deviation
    - 35:30
    - the data type of variance is not a row number.
      instead, it is a rank0 tensor like;

      #+BEGIN_SRC python
        tensor(6.8693)
      #+END_SRC

    - "standard deviation" is used over
      "mean absolute deviation" 
      because math proof will be easier

      #+BEGIN_SRC python
        # mean absolute deviation
        (t-m).abs().mean()

        # standard deviation
        (t-m).pow(2).mean().sqrt      
      #+END_SRC

*** variance
    - standard deviation is the square root of the variance

      #+BEGIN_SRC python
        # variance
        (t-m).pow(2).mean()

        # standard deviation
        (t-m).pow(2).mean().sqrt
      #+END_SRC

*** 2 ways to calculate variance

    #+BEGIN_SRC python
      # 1st way (intuitive, but hard to calculate)
      (t-m).pow(2).mean(),

      # 2nd way (efficient to calculate)
      (t*t).mean() - (m*m)
    #+END_SRC

*** covariance
    - when two variables are correlaetd, covariance is large
    - on the otherhand, when two variables are not correlated,
      covariance is small

*** when softmax should NOT be used
    - lesson10, 45:00
    - two pictures with different 'fishiness',
      one with 2, and the other only with 0.63
      will end up having same softmax value

    - softmax calculate values based on the relative magnitude
      erasing off absolute magnitude

      cat
      dog
      plane
      fish
      building

    - softmax should be used when we are sure that
      an input has at least 1 label.

*** binominal
    - lesson10, 48:30
    - when we are not sure if an image has at least 1 lalbel,
      we use binominal;e^x/(1+e^x)
      since this will map values to [0, 1], taking into account
      of the abolute magnitude

      https://www.desmos.com/calculator

*** when softmax is a good idea
    - language model
    - there is always a one word

** 05_anneal.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
    #+END_SRC

*** create learner

    #+BEGIN_SRC python
      #export
      def create_learner(model_func, loss_func, data):
          return Learner(*model_func(data), loss_func, data)
    #+END_SRC

*** get_model_func

    #+BEGIN_SRC python
      #export
      def get_model_func(lr=0.5): return partial(get_model, lr=lr)
    #+END_SRC

    - a function to retun get_model (ver2) 
      with a specified learning rate applied

    - get_model is as below;

      #+BEGIN_SRC python
        def get_model(data, lr=0.5, nh=50):
            m = data.train_ds.x.shape[1]
            model = nn.Sequential(nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,data.c))
            return model, optim.SGD(model.parameters(), lr=lr)
      #+END_SRC

*** debugging
    - 1:59:00

*** TODO Recorder ver1 

    #+BEGIN_SRC python
      #export
      class Recorder(Callback):
          def begin_fit(self): self.lrs,self.losses = [],[]

          def after_batch(self):
              if not self.in_train: return
              self.lrs.append(self.opt.param_groups[-1]['lr'])
              self.losses.append(self.loss.detach().cpu())        

          def plot_lr  (self): plt.plot(self.lrs)
          def plot_loss(self): plt.plot(self.losses)

    #+END_SRC

    # overview
    - `Recorder` records the value of `lr` of only the last 
      parameter group (which is accessed by index -1) 
      everytime `after_batch` is called
      
      #+BEGIN_SRC python
        self.lrs.append(self.opt.param_groups[-1]['lr'])
      #+END_SRC

    # after_batch detail
    - `param_groups` is groups of HYPER PARAMETERS in the form of

      [{lr: 0.3, mom:0.1}, {lr: 0.4, mom:0.1}, {lr: 0.2, mom:0.1}]

    - `lrs` is an history array for `lr` of the last parameter group

    - there are possibly more than 1 parameter groups, and 
      each group has 'lr' of different values

    - [ ] why do we need to call detach?

      #+BEGIN_SRC python
        self.losses.append(self.loss.detach().cpu())        
      #+END_SRC

*** ParamScheduler ver1

    #+BEGIN_SRC python
      class ParamScheduler(Callback):
          _order=1
          def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func

          def set_param(self):
              for pg in self.opt.param_groups:
                  pg[self.pname] = self.sched_func(self.n_epochs/self.epochs)
                  
          def begin_batch(self): 
              if self.in_train: self.set_param()
    #+END_SRC

    # overview
    - `ParamScheduler` updates the parameter specified
      by `self.pname` of all the parameter groups
      by the value calculated by passed `sched_func`, 
      in the beggining of every batch

    # set_param detail
    - as with `Recorder` ver1, `param_groups` is groups of HYPER PARAMETERS in the form of

      [{lr: 0.3, mom:0.1}, {lr: 0.4, mom:0.1}, {lr: 0.2, mom:0.1}]

    - note that the "group" refers to group of LAYERS, i.e., 
      - the 1st param group is for layer 1 to m (main body)
      - the 2nd param group is for layer m+1 to n (last few layers)

      # #+BEGIN_SRC python
      #   # the 1st param group is for layer 1 to m (main body)
      #   # the 2nd param group is for layer m+1 to n (last few layers)
      #   [
      #       [W1, B1, ..., Wm, Bm],
      #       [Wm+1, Bm+1 ..., Wn, Bn]
      #   ]
      # #+END_SRC

*** sched_lin (that does currying)

    #+BEGIN_SRC python
      def sched_lin(start, end):
          def _inner(start, end, pos): return start + pos*(end-start)
          return partial(_inner, start, end)
    #+END_SRC

    # overview
    - `sched_lin` is a "function which returns a function",
      or what I call "currying function"

    - a returned function takes only `pos`

    # detail
    - `start` is the starting value of a hyper parameter

    - `end` is the ending value of a hyper parameter

*** annealer

    #+BEGIN_SRC python
      #export
      def annealer(f):
          def _inner(start, end): return partial(f, start, end)
          return _inner

      @annealer
      def sched_lin(start, end, pos): return start + pos*(end-start)
    #+END_SRC

    # overview
    - `annealer` is a "function which returns a function",
      which accepts a NORMAL scheduler function, and it returns
      a function which is yet another "function which returns a function".

    - the returned function is a "currying function" 
      that does currying, as the version of `sched_lin`
      in the previous section

    - the point of `annealer` is that, with `annealer`, we
      do not have to manually implemente the "currying function" for
      a scheduler function.

    - in other words, annealer helps to create a
      currying function from a normal scheduler function

    # detail
    - with `@` (method decorator), the followings become equivalent
      https://realpython.com/primer-on-python-decorators/

      #+BEGIN_SRC python
        sched_lin(1,2)

        annealer(sched_lin)(1,2)
      #+END_SRC
      
*** other scheduler functions with @annealer (sched_cos, sched_no, sched_exp, cos_1cycle_anneal)

    #+BEGIN_SRC python
      #export
      @annealer
      def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2
      @annealer
      def sched_no(start, end, pos):  return start
      @annealer
      def sched_exp(start, end, pos): return start * (end/start) ** pos

      def cos_1cycle_anneal(start, high, end):
          return [sched_cos(start, high), sched_cos(high, end)]

      #This monkey-patch is there to be able to plot tensors
      torch.Tensor.ndim = property(lambda x: len(x.shape))
    #+END_SRC

    # detail
    - for visualization of `sched_cos`, use the web page below;
    https://www.desmos.com/calculator
    
    0.2+(1+cos(pi(1-x)))*(0.6-0.2)/2

*** monkey patching Tensor.ndim

    #+BEGIN_SRC python
      #This monkey-patch is there to be able to plot tensors
      torch.Tensor.ndim = property(lambda x: len(x.shape))
    #+END_SRC

    # overview
    - this is for telling matplot the dimension of a tensor
      e.g. tensor.Size([50000, 784])

*** combine scheduler

    #+BEGIN_SRC python
      #export
      def combine_scheds(pcts, scheds):
          assert sum(pcts) == 1.
          pcts = tensor([0] + listify(pcts))
          assert torch.all(pcts >= 0)
          pcts = torch.cumsum(pcts, 0)
          def _inner(pos):
              idx = (pos >= pcts).nonzero().max()
              actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])
              return scheds[idx](actual_pos)
          return _inner
    #+END_SRC

    # detail
    - `torch.cumsum`
      https://pytorch.org/docs/master/generated/torch.cumsum.html

      #+BEGIN_SRC python
        a = torch.randn(10)

        a
        '''
        tensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,
                 0.1850, -1.1571, -0.4243])
        '''

        torch.cumsum(a, dim=0)
        '''
        tensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,
                -1.8209, -2.9780, -3.4022])
        '''
      #+END_SRC

    - `pcts` in the argument is in the form of [0, 0.3, 0.7]

    - an element of `pcts` means how much percentage of
      the total steps the corresponding interval is allocated,
      e.g., when pcts = [0, 0.3, 0.7], 
      - the 1st interval is allocated 0.3
      - the 2nd interval is allocated 0.7

    - `torch.cumsum([0, 0.3, 0.7])` will be `[0, 0.3, 1.0]`

    - `pcts` after `pcts = torch.cumsum(pcts, 0)` gives 
      the 'coordinates' of the edges of the schedulling intervals,
      e.g., when pcts = [0, 0.3, 0.7], 
      - the coordinate of the edges of 1st interval is 0, 0.3
      - the coordinate of the edges of 2nd interval is 0.3, 1.0

*** how to use combine_scheds 

    #+BEGIN_SRC python
      sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)])
      plt.plot(a, [sched(o) for o in p])

    #+END_SRC

    # overview
    - here is an example: use 30% of the budget to go
      from 0.3 to 0.6 following a cosine, 
      then the last 70% of the budget to go from 0.6 to 0.2, 
      still following a cosine.

* lesson10
** 05b_early_stopping.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
    #+END_SRC
    
*** Callback ver3, TrainEvalCallback ver2, CancelTrainException, CancelEpochException, CancelBatchException, 

    https://course19.fast.ai/videos/?lesson=10&t=3230

    #+BEGIN_SRC python
      #export
      class Callback():
          _order=0
          def set_runner(self, run): self.run=run
          def __getattr__(self, k): return getattr(self.run, k)
          
          @property
          def name(self):
              name = re.sub(r'Callback$', '', self.__class__.__name__)
              return camel2snake(name or 'callback')
          
          def __call__(self, cb_name):
              f = getattr(self, cb_name, None)
              if f and f(): return True
              return False

      class TrainEvalCallback(Callback):
          def begin_fit(self):
              self.run.n_epochs=0.
              self.run.n_iter=0
          
          def after_batch(self):
              if not self.in_train: return
              self.run.n_epochs += 1./self.iters
              self.run.n_iter   += 1
              
          def begin_epoch(self):
              self.run.n_epochs=self.epoch
              self.model.train()
              self.run.in_train=True

          def begin_validate(self):
              self.model.eval()
              self.run.in_train=False

      class CancelTrainException(Exception): pass
      class CancelEpochException(Exception): pass
      class CancelBatchException(Exception): pass
    #+END_SRC

    # Callback ver3 overview
    - `Callback` ver3 is for factoring out, from `Runner` ver2,
      the logic for finding & calling a callback, and move it inside
      `Callback` class as its `__call__` method

       https://course19.fast.ai/videos/?lesson=10&t=3230

    # Runner ver1 __call__
    #+BEGIN_SRC python
      def __call__(self, cb_name):
          for cb in sorted(self.cbs, key=lambda x: x._order):
              f = getattr(cb, cb_name, None)
              if f and f(): return True
          return False
    #+END_SRC

    - with `Callback` ver3, a callback such as 'begin_batch' can be called 
      from inside an instance of `Runner` ver3 (implemented later), as below

      #+BEGIN_SRC python
        # somewhere in Runner ver3
        SomeCallbckInstance('begin_batch')
      #+END_SRC

    - now that a callback method (such as `begin_batch`) of
      `Callback` ver3 is called through `__call__` method
      from inside `Runncer` ver3 instance, a developer can design
       what happens on calling a callback method (such as `begin_batch`)
       by overriding `__call__` method of `Callback` ver3
       in `Callback` ver3 subclass

      https://course19.fast.ai/videos/?lesson=10&t=3330



    # TrainEvalCallback ver2 overview
    - the implementation is exactly the same as `TrainEvalCallback` ver1.
      the only difference is that it inherits `Callback` ver3 instead of
      `Callback` ver2

*** Cancel class

    #+BEGIN_SRC python
      class CancelTrainException(Exception): pass
      class CancelEpochException(Exception): pass
      class CancelBatchException(Exception): pass
    #+END_SRC

    # overview
    - we can make use of the exception mechanism(try-except syntax)
      to  interuppt `fit`, or `one_batch` instead of relying on the
      return value of `self(...)`, e.g., `self('begin_batch)` 
      in `if` blocks

    - to do so, make a new class which inherits `Exception`, 
      and just say `pass`

    - by saying `pass`, it has all the same behaviors and propeties,
      but have different name

*** Runner ver2 (=fit ver10)

    #+BEGIN_SRC python
      #export
      class Runner():
          def __init__(self, cbs=None, cb_funcs=None):
              cbs = listify(cbs)
              for cbf in listify(cb_funcs):
                  cb = cbf()
                  setattr(self, cb.name, cb)
                  cbs.append(cb)
              self.stop,self.cbs = False,[TrainEvalCallback()]+cbs

          @property
          def opt(self):       return self.learn.opt
          @property
          def model(self):     return self.learn.model
          @property
          def loss_func(self): return self.learn.loss_func
          @property
          def data(self):      return self.learn.data

          def one_batch(self, xb, yb):
              try:
                  self.xb,self.yb = xb,yb
                  self('begin_batch')
                  self.pred = self.model(self.xb)
                  self('after_pred')
                  self.loss = self.loss_func(self.pred, self.yb)
                  self('after_loss')
                  if not self.in_train: return
                  self.loss.backward()
                  self('after_backward')
                  self.opt.step()
                  self('after_step')
                  self.opt.zero_grad()
              except CancelBatchException: self('after_cancel_batch')
              finally: self('after_batch')

          def all_batches(self, dl):
              self.iters = len(dl)
              try:
                  for xb,yb in dl: self.one_batch(xb, yb)
              except CancelEpochException: self('after_cancel_epoch')

          def fit(self, epochs, learn):
              self.epochs,self.learn,self.loss = epochs,learn,tensor(0.)

              try:
                  for cb in self.cbs: cb.set_runner(self)
                  self('begin_fit')
                  for epoch in range(epochs):
                      self.epoch = epoch
                      if not self('begin_epoch'): self.all_batches(self.data.train_dl)

                      with torch.no_grad(): 
                          if not self('begin_validate'): self.all_batches(self.data.valid_dl)
                      self('after_epoch')
                  
              except CancelTrainException: self('after_cancel_train')
              finally:
                  self('after_fit')
                  self.learn = None

          def __call__(self, cb_name):
              res = False
              for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res
              return res
    #+END_SRC

    # overview

    # - note that callack instances are registered as propeties of
    #   `Runner` instance by `setattr` inside __init__
    
    - `Runner` ver2  makes use of `Callback` ver3, and
      so a specific callback(e.g. begin_batch) of an instance
      of `Callback` subclass can be called more succinctly
      compare to with `Runner` ver1

      # with Runner ver2
      #+BEGIN_SRC python
        def __call__(self, cb_name):
            res = False
            for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) or res
            return res

        # e.g., MyCallback('begin_batch') 
      #+END_SRC


      # with Runner ver1
      #+BEGIN_SRC python
        def __call__(self, cb_name):
            for cb in sorted(self.cbs, key=lambda x: x._order):
                f = getattr(cb, cb_name, None)
                if f and f(): return True
            return False
      #+END_SRC


    - a line of code to throw `CancelTrainException` is
      added to `fit` to catch `CancelTrainException`
      (`try-except` syntax which is the python version of try-catch)

      #+BEGIN_SRC python
        except CancelTrainException: self('after_cancel_train')
      #+END_SRC

*** AvgStatsCallback ver2

    #+BEGIN_SRC python
      #export
      class AvgStatsCallback(Callback):
          def __init__(self, metrics):
              self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)
              
          def begin_epoch(self):
              self.train_stats.reset()
              self.valid_stats.reset()
              
          def after_loss(self):
              stats = self.train_stats if self.in_train else self.valid_stats
              with torch.no_grad(): stats.accumulate(self.run)
          
          def after_epoch(self):
              print(self.train_stats)
              print(self.valid_stats)
    #+END_SRC

    # overview
    - the implementation is exactly the same as `AvgStatsCallback` ver1,
      and the only difference is that it now inherits `Callback` ver3

*** Recorder ver2 

     #+BEGIN_SRC python

      class Recorder(Callback):
          def begin_fit(self):
              self.lrs = [[] for _ in self.opt.param_groups]
              self.losses = []

          def after_batch(self):
              if not self.in_train: return
              for pg,lr in zip(self.opt.param_groups,self.lrs): lr.append(pg['lr'])
              self.losses.append(self.loss.detach().cpu())        

          def plot_lr  (self, pgid=-1): plt.plot(self.lrs[pgid])
          def plot_loss(self, skip_last=0): plt.plot(self.losses[:len(self.losses)-skip_last])
              
          def plot(self, skip_last=0, pgid=-1):
              losses = [o.item() for o in self.losses]
              lrs    = self.lrs[pgid]
              n = len(losses)-skip_last
              plt.xscale('log')
              plt.plot(lrs[:n], losses[:n])

       #+END_SRC

    # overview
    - with Recorder ver2, `lr` are recorded for all the layers,
      and with `plot_lr` function, we can choose the layer 
      for which to plot the change of `lr` 
      (defaults to the last layer [-1] )

    # begin_fit overview
    - `begin_fit` creates
      - `lrs`, array of "learning rate history array"

        (each element of `lrs` is 'learning rate history array'
        for each parameter group)

      - "loss history array" (`losses`)

    # after_batch overview
    - in the end of every batch for each parameter group, 
      `after_batch` updates "learning rate history array", 
      and it also updates "losses history array"

    # after_batch detail
    - as with `Recorder` ver1, `param_groups` is groups of
      HYPER PARAMETERS in the form of

      [{lr: 0.3, mom:0.1}, {lr: 0.4, mom:0.1}, {lr: 0.2, mom:0.1}]    

    # plot overview
    - `plot` shows (learning rate VS loss)
      for the parameter group specified by pgid
      (default to -1 = the last parameter group)

    # plot_lr overview
    - `plot_lr` shows "learning rage history array" of the
      specified paramter group (default to -1 = the last parameter group)

*** ParamScheduler ver2

    #+BEGIN_SRC python
      class ParamScheduler(Callback):
          _order=1
          def __init__(self, pname, sched_funcs): self.pname,self.sched_funcs = pname,sched_funcs
              
          def begin_fit(self):
              if not isinstance(self.sched_funcs, (list,tuple)):
                  self.sched_funcs = [self.sched_funcs] * len(self.opt.param_groups)

          def set_param(self):
              assert len(self.opt.param_groups)==len(self.sched_funcs)
              for pg,f in zip(self.opt.param_groups,self.sched_funcs):
                  pg[self.pname] = f(self.n_epochs/self.epochs)
                  
          def begin_batch(self): 
              if self.in_train: self.set_param()
    #+END_SRC

    # overview
    - `ParamScheduler` ver2 takes several schduler functions whereas
      `ParamScheduler` ver1 takes only 1 scheduler function

    - each shceduler function corrresponds to a parameter group
      (=layer group)

    - note that the "group" refers to group of LAYERS, i.e., 
      - the 1st param group is for layer 1 to m (main body)
      - the 2nd param group is for layer m+1 to n (last few layers)      

    - `param_groups` is groups of HYPER PARAMETERS in the form of

      [{lr: 0.3, mom:0.1}, {lr: 0.4, mom:0.1}, {lr: 0.2, mom:0.1}]

    # begin_fit detail
    - if `self.sched_funcs` is not `list` nor `tuple`,
      i.e., a single function, `begin_fit` creates 
      an array of scheduler functions so that 
      each parameter group has its corresponding scheduler function.

*** LR_Find ver1

    #+BEGIN_SRC python
      class LR_Find(Callback):
          _order=1
          def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):
              self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr
              self.best_loss = 1e9
              
          def begin_batch(self): 
              if not self.in_train: return
              pos = self.n_iter/self.max_iter
              lr = self.min_lr * (self.max_lr/self.min_lr) ** pos
              for pg in self.opt.param_groups: pg['lr'] = lr
                  
          def after_step(self):
              if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:
                  raise CancelTrainException()
              if self.loss < self.best_loss: self.best_loss = self.loss
    #+END_SRC

    # begin_batch overview
    - `begin_batch` calculates a new value for `lr`, 
      and set `lr` property of each parameter group
      to the calculated value.

    # begin_batch detail
    - `**` is exponentiation.

** 06_cuda_cnn_hooks_init
*** lesson video
    https://course19.fast.ai/videos/?lesson=10&t=5390

*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
    #+END_SRC
    
*** notmrlize_to

    #+BEGIN_SRC python
      #export
      def normalize_to(train, valid):
          m,s = train.mean(),train.std()
          return normalize(train, m, s), normalize(valid, m, s)
    #+END_SRC

    - normalize training & validation data set

    - `normalize` is implemented above;
      #+BEGIN_SRC python
        def normalize(x, m, s): return (x-m)/s
      #+END_SRC

*** `view` 

    #+BEGIN_SRC python
      my_x = torch.rand(4,4)
      my_x.view(16, -1)

      '''
      tensor([[0.4376],
              [0.4770],
              [0.5197],
              [0.3822],
              [0.1813],
              [0.7772],
              [0.2852],
              [0.0907],
              [0.5914],
              [0.8817],
              [0.1719],
              [0.0896],
              [0.6135],
              [0.2030],
              [0.4816],
              [0.1274]])
      '''
    #+END_SRC

    # overview
    - `view` reshapes a tensor in the way that total numbe of 
      the elements does not change. This is as if reshaping
      a rectangle so that the area of the rectangle is preserved
      https://stackoverflow.com/questions/42479902/how-does-the-view-method-work-in-pytorch
      https://pytorch.org/docs/stable/tensor_view.html

      |   |   |   |
      |   |   |   |
      |   |   |   |
      |   |   |   |

      =>

      |   |   |
      |   |   |
      |   |   |
      |   |   |
      |   |   |
      |   |   |

    - [ ] x.shape(16, -1) gives the same result as x.shape(16, 1)
      as `-1` means "whatever possible after specifying 16"

*** mnist_resize

    #+BEGIN_SRC python
      def mnist_resize(x): return x.view(-1, 1, 28, 28)
    #+END_SRC

    # overview
    - `mnist_resize` reshapes a mnist batch 
      from  tensor.Size(512, 784)
      to    tensor.Size(-1, 1, 28, 28)

    - after reshaping, chanel & width & height are (1, 28, 28)

    # detail
    - batch size `-1` means what is possible after specifying 
      (chahel:1, width:28, height:28) which is 512

    #+BEGIN_SRC python
      my_xb, my_yb = next(iter(data.train_dl))
      my_xb.shape
    #+END_SRC
      
*** flatten

    #+BEGIN_SRC python
      def flatten(x):      return x.view(x.shape[0], -1)
    #+END_SRC

    # overview
    - `flatten` removes 'horizontal' & 'vertical' axis
      and leaves only 'batch' & 'channel' axis, 
      so the input will be a rank2 tensor

      e.g. 
      torch.Size([512, 32, 1, 1]) => torch.Size([512, 32])
    
    # detail
    - `x.shape[0]` is the value for the 'batch' axis  (e.g. 512)
    - `-1` means "whatever possible" after specifying 
      the other axis (=the 'batch' axis)

*** Lambda 

    #+BEGIN_SRC python
      #export
      class Lambda(nn.Module):
          def __init__(self, func):
              super().__init__()
              self.func = func

          def forward(self, x): return self.func(x)

      def flatten(x):      return x.view(x.shape[0], -1)
    #+END_SRC

    # overview
    - `Lambda` is a 'helper' layer to run a reshaping function 
      such as `flatten`

*** convolution summary
    - the depth of a kernel should matches the depth of an input
    - the number of the kernels determines the depth of the ouput

*** signature of nn.Conv2d
    - calling nn.Conv2d is in the form like;
      nn.Conv2d(8, 16, 3, padding=1, stride=2)

    - `nn.Conv2d(8, 16, 3, padding=1, stride=2)` implies
      - input has 8 channels
      - output has 16 channels
    # - convolution kernel can be considered as sticks of "金太郎飴"
    #   stacked together

    #   - 8 is length of "金太郎飴" => number of the channels
    #   - 16 is the number of "金太郎飴" => number of the filters
    #   - 3 is the size of the face of "金太郎飴"

*** AdaptiveAvgPool2d

    #+BEGIN_SRC python
      m = nn.AdaptiveAvgPool2d((5,7))
      input = torch.randn(1, 64, 8, 9)
      output = m(input)
      '''
      torch.Size([1, 64, 5, 7])
      '''

      # target output size of 7x7 (square)
      m = nn.AdaptiveAvgPool2d(7)
      input = torch.randn(1, 64, 10, 9)
      output = m(input)
      '''
      torch.Size([1, 64, 7, 7])
      '''

      # target output size is 1x1
      my_input= torch.randn(512, 32, 2, 2)
      my_adp = nn.AdaptiveAvgPool2d(1)
      my_output = my_adp(my_input)
      my_output.shape
      '''
      torch.Size([512, 32, 1, 1])
      '''

      # target output size of 10x7
      m = nn.AdaptiveMaxPool2d((None, 7))
      input = torch.randn(1, 64, 10, 9)
      output = m(input)
      '''
      torch.Size([1, 64, 10, 7])
      '''
    #+END_SRC

    # overview
    - `AdaptiveAvgPool2d` transforms the 'height' and 'width'
      to the specified values, leaving the value of
      'batch' and 'channel' unchanged.

    - from official doc
      https://pytorch.org/docs/stable/nn.html
      "output_size – the target output size of the image of the
      form H x W. 
      Can be a tuple (H, W) or a single H for a square image H x H.
      H and W can be either a int, or None which means the size
      will be the same as that of the input."

    # - Applies a 2D adaptive average pooling over an
    #   input signal composed of several input planes.

    # - The output is of size H x W, for any input size.

    # - The number of output features is equal to the number of 
    #   input planes.
    
*** get_cnn_model ver1

    #+BEGIN_SRC python
      def get_cnn_model(data):
          return nn.Sequential(
              Lambda(mnist_resize),
              nn.Conv2d( 1, 8, 5, padding=2,stride=2), nn.ReLU(), #14
              nn.Conv2d( 8,16, 3, padding=1,stride=2), nn.ReLU(), # 7
              nn.Conv2d(16,32, 3, padding=1,stride=2), nn.ReLU(), # 4
              nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU(), # 2
              nn.AdaptiveAvgPool2d(1),
              Lambda(flatten),
              nn.Linear(32,data.c)
          )
    #+END_SRC

    # detail
    - `data` is used for defining the final layer
      `nn.Linear(32, data.c)`

    - `Lambda(mnist_resize)` reshapes an input 
      from  tensor.Size([512, 784])
      to    tensor.Size([512, 1, 28, 28]) 

    - activation for `nn.Conv2d(32,32, 3, padding=1,stride=2), nn.ReLU()`,
      is in the form of 
      torch.Size([512, 32, 2, 2]) 

    - activation for nn.AdaptiveAvgPool2d is in the form of
      torch.Size([512, 32, 1, 1])

    - Lambda(flatten) reshapes the previous activation 
      to remove the trailling (1, 1)
      turning it to the form torch.Size([512, 32])

    - the activation for nn.Linear(32, data.c) is in the form of
      torch.Size([512, data.c])

*** CudaCallack ver1

    #+BEGIN_SRC python
      class CudaCallback(Callback):
          def __init__(self,device): self.device=device
          def begin_fit(self): self.model.to(self.device)
          def begin_batch(self): self.run.xb,self.run.yb = self.xb.to(self.device),self.yb.to(self.device)
    #+END_SRC

    # overview
    - we could train a model with CPU.
      actually, the training during lesson 8 - middle of 10
      are done on CPU

    - but CPU is slow, so we want to train the model on GPU.

    # detail
    - `to` is part of a pytorch, and it moves a thing with a tensor to cuda

*** CudaCallack ver2

    #+BEGIN_SRC python
      # Somewhat less flexible, but quite convenient
      torch.cuda.set_device(device)

      class CudaCallback(Callback):
          def begin_fit(self): self.model.cuda()
          def begin_batch(self): self.run.xb,self.run.yb = self.xb.cuda(),self.yb.cuda()
    #+END_SRC

*** conv2d

    #+BEGIN_SRC python
      def conv2d(ni, nf, ks=3, stride=2):
          return nn.Sequential(
              nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), nn.ReLU())
    #+END_SRC

    # overview
    - conv2d is a Sequential module consisting of nn.Conv2d & nn.ReLU

*** get_cnn_layers ver1 & get_cnn_model ver2

    #+BEGIN_SRC python
      def get_cnn_layers(data, nfs):
          nfs = [1] + nfs
          return [
              conv2d(nfs[i], nfs[i+1], 5 if i==0 else 3)
              for i in range(len(nfs)-1)
          ] + [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]

      def get_cnn_model(data, nfs): return nn.Sequential(*get_cnn_layers(data, nfs))
    #+END_SRC

    # get_cnn_layers ver1 detail
    - note that nfs[0] = 1, and the first conv2d will be
      conv2d(1, 8, 5 if i==0 else 3)

    # get_cnn_model ver2  overview
    - `get_cnn_model` ver2 utilizes `conv2d` which bundles `nn.Conv2d` & `ReLU`
      through `get_cnn_layers` 

    - with `get_cnn_model` ver2 combined with `get_cnn_layers`,
      `Lambda(mnist_resize)` which does batch transformation 
      is factored out for generalization, and instead of
      using `Lambda(mnist_resize)`, we use more general
      `BatchTransformXCallback`, which will be implmented below.

    - note it is `nn.Sequential` that is used, and 
      NOT home made `Sequential`

*** BatchTransformXCallback, view_tfm

    #+BEGIN_SRC python
      #export
      class BatchTransformXCallback(Callback):
          _order=2
          def __init__(self, tfm): self.tfm = tfm
          def begin_batch(self): self.run.xb = self.tfm(self.xb)

      def view_tfm(*size):
          def _inner(x): return x.view(*((-1,)+size))
          return _inner
    #+END_SRC

    # BatchTransformXCallback overview
    - `BatchTransformXCallback` is a subclass of `Callback` ver2
      which implementes `begin_batch` and accepts a transformer 
      in `__init__`.

    # view_trm detail
    - `*size` 'array-fy' arguments
    - with `(-1,)+size`, an array is created by
      appending to [-1] the elements inside `size`
      e.g.

      #+BEGIN_SRC python
        size = [1, 28, 28]
        newArray = (-1,)+size
        newArray

        '''
        [-1, 1, 28, 28]
        '''
      #+END_SRC
    
*** how to use get_cnn_model ver2 with BatchTransformXCallback

    #+BEGIN_SRC python
      x_train,x_valid = normalize_to(x_train,x_valid)
      train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)

      data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)

      cbfs = [Recorder, partial(AvgStatsCallback,accuracy)]

      mnist_view = view_tfm(1,28,28)
      cbfs.append(partial(BatchTransformXCallback, mnist_view))

      nfs = [8,16,32,32]

      model = get_cnn_model(data, nfs)
      learn,run = get_runner(model, data, lr=0.4, cbs=cbfs)

    #+END_SRC

    - `DataBunch`, `get_runner` will be implemented later

*** TODO how to determine filter size (the "face size" of "金太郎飴")
    https://course19.fast.ai/videos/?lesson=10&t=4120

    - first, let's suppose the 1st conv layer is of
      conv2d(1, 8, 3)

    - so, below are "金太郎飴"
      - depth: 1
      - number: 8
      - face size: 3

    - just focus a (1x3x3) part of the entire input (1x28x28),
      and what we get out of this part by calculating 
      dot product with each of the 8 kernels

    - we get a vector consisting of 8 elements

    - this means, we barely reduce amount of information;
      9=(1x3x3) to 8  (only 1 less)
      which is a waste of computations

    - as another example, think of imagenet, where input(3x28x28) has 3 channels
      and the 1st layer often has 32 filters (kernels)

    - if the kernel size is 3x3, since there are 32 kernels,
      3x3x3 (=27) part of the input will be reduced into 32 numbers,
      which results in not reducing the amount of the information the original input has

    - on the other hand, if the kernel size is 7x7,
      3x7x7 (=147) part of the input will be reduced into 32 numbers,
      which is actually reducing the amount of the information.

    - for the similar reson, we use 5 for the kernel size of 1st layer.
      this way, the information is reduced from 25(1x5x5) to 8

*** get_runner

    #+BEGIN_SRC python
      #export
      def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):
          if opt_func is None: opt_func = optim.SGD
          opt = opt_func(model.parameters(), lr=lr)
          learn = Learner(model, opt, loss_func, data)
          return learn, Runner(cb_funcs=listify(cbs))
    #+END_SRC

    # detail
    - `Learner` is `Learner` ver1
    - `Runner` is `Runner` ver2

*** SequentialModel ver2
    https://course19.fast.ai/videos/?lesson=10&t=4353

    #+BEGIN_SRC python
      class SequentialModel(nn.Module):
          def __init__(self, *layers):
              super().__init__()
              self.layers = nn.ModuleList(layers)
              self.act_means = [[] for _ in layers]
              self.act_stds  = [[] for _ in layers]
              
          def __call__(self, x):
              for i,l in enumerate(self.layers):
                  x = l(x)
                  self.act_means[i].append(x.data.mean())
                  self.act_stds [i].append(x.data.std ())
              return x
          
          def __iter__(self): return iter(self.layers)
    #+END_SRC

    # overview
    - `SequentialModel` creates "activation mean history" for each layer
    - `SequentialModel` creates "activation std history" for each layer
    - on every call to the model, for eacn layer, 
      it records the mean&stds of the resulting activation 
      on "activation mean history" and "activation std history"
      for the layer
    - `SequentialModel` ver2 is just for showing a primitive way to
      record "activation mean history", "activation std history",
    - later, we refactor this using layer-wise callback (with pytorch's terminalogy, "hook")

    # _call_ detail
    - for a specific layer whose index is `i`, 
      "activation means history" `act_means[i]` gets
      a new entry every time `SequentialModel` is called,
      and `SequentialModel` is called for every single batch.
    - hence, `act_means[i]` for a specific layer shows
      how activation means for that specific layer was changing
      as the model is called and the parameters were updated
      for every single batch.

*** TODO how to interpret "activation mean history" graphs in the Jupyter notebook
    https://course19.fast.ai/videos/?lesson=10&t=4353

    # helpful reference?
    https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/
    https://pouannes.github.io/blog/initialization/    

    - look at a specific single batch, and see how activations change through layers
    - for good training, the means of an activation should be close to 0,
      but that is not the case for most of the batches.
      (especially, because of ReLue layer, mean will be close to 0.5)

    - also, the standard deviation of activations should be close to 1,
      but the standard deviations of activation is getting closer to 0
      as we go through the layers
      
    - [ ] (?) when the activation is small, the gradients of the
      paramters are also small, and so the model learns only
      slowly.

    # - let's look at one layer, layer5
    # - rapid change of "activation mean" means that
    #   the weights are updated a lot (=big gradient)
    #   and so activation jumps at the next step.

*** TODO why does activation mean dropp off the clif suddenly?
    - (?) a large gradient of the mean history graph implies a big change in paramters?

    - think of concaved 3 dimensional loss landscape, with only 2 parameters
    - steep curves means large gradient,
    - when gradient is very big, a parameter will change a lot
      resulting in landing onto a the opposite side of
      loss landscape where the gradient is large in the opposite direction,
      then the updated weight will be very small

*** what is Pytorch hooks
    - it is a callback which can be called back layer-wise,
      whereas the callbacks we have impelemented so far
      can be called only batch-wise, or epoch-wise

    - in Pytorch, for some reason, these "layer-wise" callbacks
      are called "hooks", instead of "callback"

*** append_stats ver1 

    #+BEGIN_SRC python
      act_means = [[] for _ in model]
      act_stds  = [[] for _ in model]

      def append_stats(i, mod, inp, outp):
          act_means[i].append(outp.data.mean())
          act_stds [i].append(outp.data.std())

      for i,m in enumerate(model): m.register_forward_hook(partial(append_stats, i))    
    #+END_SRC

    # append_stats overview
    - `append_stats` is a "hook"(=layer-wise callback) passed to `m.register_forward_hook`
    - since a `append_stats` is a callback passed to pytorch `m.register_forward_hook`,
      it is implemented to accept 3 arguents that `m.register_forward_hook` passes
      - `mod`: the layer(module) the callback is registered on
      - `inp`: the input for the layer
      - `outp`: the ouput for the layer

*** Hook, children, append_stats ver2

    #+BEGIN_SRC python
      #export
      def children(m): return list(m.children())

      class Hook():
          def __init__(self, m, f): self.hook = m.register_forward_hook(partial(f, self))
          def remove(self): self.hook.remove()
          def __del__(self): self.remove()

      def append_stats(hook, mod, inp, outp):
          if not hasattr(hook,'stats'): hook.stats = ([],[])
          means,stds = hook.stats
          means.append(outp.data.mean())
          stds .append(outp.data.std())

    #+END_SRC

    # Hook overview
    - `Hook` is for generalizing the process of registering 
      a layer-wise callback to a layer(module)

    - in __init__, Hook instance registers the passed function `f`
      to the passed layer(module) `m`

    - for this purpose, 
      - `__init__` registers any passed callback to the passed layer(module)
      - `__init__` passes the instance of `Hook` itself to 
        the passed callback so that the callback can record
        the result of a calculation it does.

    - we create a dedicated `Hook` instance for each single layer(module)

    # Hook::__init__ detail
    - `f` is the layer-wise callback (in pytorch's terminalogy, hook) 
      to be registered to a module

    - `f` will be called every time a layer calculates
      forward path, receiving (layer/module, input, output)

    - `f` is passed a reference to the `Hook` instance so that
      it can record the result of a calculation on the `Hook`
      instance by `partial(f, self)`

    - for an example of `f`, see `append_stats`

    # append_stats overview
    - since `append_stats` is a layer-wise callback,
      and expects to be passed to `__init__` of `Hook`,
      it has 4 arguments
      - `hook`: an instance of `Hook`
      - `mod`: the layer(module) the callback is registered on
      - `inp`: the input for the layer
      - `outp`: the ouput for the layer

*** using Hook

    #+BEGIN_SRC python
      # how to use Hook
      model = get_cnn_model(data, nfs)
      learn,run = get_runner(model, data, lr=0.5, cbs=cbfs)

      hooks = [Hook(l, append_stats) for l in children(model[:4])]
    #+END_SRC

    # detail
    - `get_cnn_model` is `get_cnn_model` ver2
    - `hooks = [Hook(l, append_stats)...]` is a list construction syntax
      and `Hook` instance is created for each of the layers(modules)
      of `model`

*** ListContainer

    #+BEGIN_SRC python
      #export
      class ListContainer():
          def __init__(self, items): self.items = listify(items)
          def __getitem__(self, idx):
              if isinstance(idx, (int,slice)): return self.items[idx]
              if isinstance(idx[0],bool):
                  assert len(idx)==len(self) # bool mask
                  return [o for m,o in zip(idx,self.items) if m]
              return [self.items[i] for i in idx]
          def __len__(self): return len(self.items)
          def __iter__(self): return iter(self.items)
          def __setitem__(self, i, o): self.items[i] = o
          def __delitem__(self, i): del(self.items[i])
          def __repr__(self):
              res = f'{self.__class__.__name__} ({len(self)} items)\n{self.items[:10]}'
              if len(self)>10: res = res[:-1]+ '...]'
              return res
    #+END_SRC

    # overview
    - an instance of `ListContainer` has behavors from python List, and some from numpy.

    - `isinstance(idx, (int,slice))` means if `idx` is
      either of type `int`, or type `slice` (such as slice(0,4))

      #+BEGIN_SRC python
        type(slice(0,4))
        '''
        slice
        '''
      #+END_SRC

    # __init__ overview
    - `__init__` makes sure that `self.items` is a list

*** Hooks

    #+BEGIN_SRC python
      #export
      from torch.nn import init

      class Hooks(ListContainer):
          def __init__(self, ms, f): super().__init__([Hook(m, f) for m in ms])
          def __enter__(self, *args): return self
          def __exit__ (self, *args): self.remove()
          def __del__(self): self.remove()

          def __delitem__(self, i):
              self[i].remove()
              super().__delitem__(i)
              
          def remove(self):
              for h in self: h.remove()
    #+END_SRC

    # overview
    - `Hooks` is for refactoring the process of manually 
      registering layer-wise callback
      (with pytorch's terminalogy, hook), for each of the
      layers(modules)
      
    # Hooks::remove detail
    - since `Hooks` inherits `ListContainer` implemented above,
      we can iterate through the `Hooks` instance by
      `for h in self`

*** using Hooks

    #+BEGIN_SRC python
      with Hooks(model, append_stats) as hooks:
          run.fit(2, learn)
          fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))
          for h in hooks:
              ms,ss = h.stats
              ax0.plot(ms[:10])
              ax1.plot(ss[:10])
          plt.legend(range(6));
          
          fig,(ax0,ax1) = plt.subplots(1,2, figsize=(10,4))
          for h in hooks:
              ms,ss = h.stats
              ax0.plot(ms)
              ax1.plot(ss)
          plt.legend(range(6));
    #+END_SRC

    # overview
    - using `with`, the Hooks instance will be destroyed
      after the code is exceuted.

*** how to interpret mean & std historys for layers with initialization
    # mean history
    - for layer 0-3, we do not have exponential grouth & crush
      although we still have for layer 4&5

    # std history
    - for layers other than layer 4, std for the first 10 steps
      are much closer to 0, where as they are almost near zero
      without initilization

*** initialize weights 

    #+BEGIN_SRC python
      for l in model:
          if isinstance(l, nn.Sequential):
              init.kaiming_normal_(l[0].weight)
              l[0].bias.data.zero_()
    #+END_SRC

    - in the initialization above, l[0]
      corresponds to the Conv2d layer of each Sequential module

    - a model consists of several Sequential modules as below;

      #+BEGIN_SRC python
        Sequential(
          (0): Sequential(
            (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
            (1): ReLU()
          )
          (1): Sequential(
            (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (1): ReLU()
          )
          (2): Sequential(
            (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (1): ReLU()
          )
          (3): Sequential(
            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
            (1): ReLU()
          )
          (4): AdaptiveAvgPool2d(output_size=1)
          (5): Lambda()
          (6): Linear(in_features=32, out_features=10, bias=True)
        )
      #+END_SRC

    - `l` in `l in model` is `conv2d` instance, which is nn.Sequential
      instance consisting of nn.Conv2d & nnReLU instance
      
    - so, l[0] is a single nn.Conv2d instance

*** append_stats ver3 (creating histogram)

    #+BEGIN_SRC python
      def append_stats(hook, mod, inp, outp):
          if not hasattr(hook,'stats'): hook.stats = ([],[],[])
          means,stds,hists = hook.stats
          means.append(outp.data.mean().cpu())
          stds .append(outp.data.std().cpu())
          hists.append(outp.data.cpu().histc(40,0,10)) #histc isn't implemented on the GPU
    #+END_SRC

    # detail
    - `hists` is a history of the histogram of the output(=activations),
      and is  in the form of
      [[23, 12, ..., 3], [20, 11, ..., 1], ...,  [10, 9, ..., 2]]

      - hist[0] is the distribution of an activation for the layer(module) for the 1st batch,
        hist[1] is the distribution of an activation for the layer(module) for the 2nd batch,
      hist[2] is the distribution of an activation for the layer(module) for the 3rd batch,        

    - the first bin corresponds to the frequency of the value near 0
      in an acitivation tensor 
      - hists[t][0] always corresponds to the 1st bin

    - If lots of INDIVIDUAL elements of an acitivation tensor
      are near zero, the calculations there after will be a waste
      of computations since they do not contribute to loss
      and hence gradient
      (NOTE that we are not talking about mean of the activation
       but individual activations)      
      
*** get_hist

    #+BEGIN_SRC python
      # Thanks to @ste for initial version of histgram plotting code
      def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()
    #+END_SRC

    # detail

    - `h` is an instance of `Hook`
    - `get_hist` is expecting `h` to have been initialized 
      with `append_stats` ver3, and have `stats` property
    - `h.stats[2]` is a history of histogram of the activation
      for the layer(module) which `h` received

*** looking at the histogram

    #+BEGIN_SRC python
      fig,axes = plt.subplots(2,2, figsize=(15,6))
      for ax,h in zip(axes.flatten(), hooks[:4]):
          ax.imshow(get_hist(h), origin='lower')
          ax.axis('off')
      plt.tight_layout()
    #+END_SRC

    # overview
    - the code is showing the history of the histogram of the 
      activation for the first 4 layers

    # detail
    - `hooks[:4]` retrieves the `Hook` instances for the first 
      4 layers(modules)

*** get_min ver1

    #+BEGIN_SRC python
      def get_min(h):
          h1 = torch.stack(h.stats[2]).t().float()
          return h1[:2].sum(0)/h1.sum(0)
    #+END_SRC

    # overview
    - `get_min` shows the history of the proportion of 
      an activation entries with small value (near 0)
    
    # detail
    - `h` is an instance of `Hook`
    - `get_hist` asumes that `h` has been initialized 
      with `append_stats` ver3 and that `append_stats` ver3
      implemented later has been called once to
      write `stats` property on `h` 

    - `h.stats[2]` is a history of histogram of the activation
      for the layer(module) which `h` received

    - `h.stats[2]` is "activation histogram history"

    - `torch.stack(h.stats[2])` converts the history of 
      the distribution of the activation into rank2 tensor

    - h1[0], h1[1], h1[2] are the frequencies of the 1st-3rd bin,
      respectively, where activation is small

    - h1[:2].sum(0)/h1.sum(0) represents 
      the history of the proportion of the occurences of 
      the 1st-3rd bin to the sum of the occurences of all the bins.

*** showing the proportion of the bins for small values

    #+BEGIN_SRC python
      fig,axes = plt.subplots(2,2, figsize=(15,6))
      for ax,h in zip(axes.flatten(), hooks[:4]):
          ax.plot(get_min(h))
          ax.set_ylim(0,1)
      plt.tight_layout()
    #+END_SRC

    # overview
    - the code is showing the result of `get_min` 
      for `h` for the first 4 layers(modules)

    # detail
    - `hooks[:4]` retrieves the `Hook` instances for the first 
      4 layers(modules)

*** get_cnn_layers ver2,  get_cnn_model ver3, conv_layer ver1, GeneralRelu, init_cnn ver1

    #+BEGIN_SRC python
      #export
      def get_cnn_layers(data, nfs, layer, **kwargs):
          nfs = [1] + nfs
          return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs)
                  for i in range(len(nfs)-1)] + [
              nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]

      def conv_layer(ni, nf, ks=3, stride=2, **kwargs):
          return nn.Sequential(
              nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride), GeneralRelu(**kwargs))

      class GeneralRelu(nn.Module):
          def __init__(self, leak=None, sub=None, maxv=None):
              super().__init__()
              self.leak,self.sub,self.maxv = leak,sub,maxv

          def forward(self, x): 
              x = F.leaky_relu(x,self.leak) if self.leak is not None else F.relu(x)
              if self.sub is not None: x.sub_(self.sub)
              if self.maxv is not None: x.clamp_max_(self.maxv)
              return x

      def init_cnn(m, uniform=False):
          f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
          for l in m:
              if isinstance(l, nn.Sequential):
                  f(l[0].weight, a=0.1)
                  l[0].bias.data.zero_()

      def get_cnn_model(data, nfs, layer, **kwargs):
          return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))
    #+END_SRC

    # get_cnn_layers ver2 overview
    - `get_cnn_layers` ver2 accepts `layer`, a function 
      to create a layer such as `conv_layer`, 
      whereas `get_cnn_layers` ver1 always uses `conv2d`
      for creating a layer

    - `get_cnn_layers` ver2 takes `**kwags` which will be
      eventually passed to GeneralRelu

    # get_cnn_layers ver2 detail
    - `layer` is a function such as `conv2d`, `conv_layer`

    - `layer` in `get_cnn_layers(data, nfs, layer, **kwargs)` is
      a function, such as `conv_layer` to create a convolution layer

    # conv_layer
    - `conv_layer` creates a layer which consists of `nn.Conv2d`,
      and `GeneralRelu` instead of normal `nn.Relu`

    # GeneralRelu overview
    - with `GeneralRelu`, we can improve the ratio of non-zero
      activation elements in an activation tensor

    # get_cnn_model ver3 overview
    - `get_cnn_model` ver3 accepts `layer` which will be
      passed along to `get_cnn_layers` ver2 which will be called inside 

    - `get_cnn_model` ver3 takes `**kwags` which will be
      eventually passed to GeneralRelu

    # init_cnn detail
    - `a` 
      https://pytorch.org/docs/stable/nn.init.html


*** append_stat ver4

    #+BEGIN_SRC python
      def append_stats(hook, mod, inp, outp):
          if not hasattr(hook,'stats'): hook.stats = ([],[],[])
          means,stds,hists = hook.stats
          means.append(outp.data.mean().cpu())
          stds .append(outp.data.std().cpu())
          hists.append(outp.data.cpu().histc(40,-7,7))
    #+END_SRC

    # overview
    - now that `GeneralRelu` is used in the conv layers,
      the activations will be distributed around 0, instead of 0.5,
      so the histogram counts the values between -7 and 7
      instead of 0 and 10

*** TODO leaky relu
    - why does leaky relu work better ?

*** get_min ver2

    #+BEGIN_SRC python
      def get_min(h):
          h1 = torch.stack(h.stats[2]).t().float()
          return h1[19:22].sum(0)/h1.sum(0)
    #+END_SRC

    # overview
    - now that we use `GeneralRelu`, and changed
      the range for the histogram to (-7, 7),
      the bin corresponding to small value has changed from 
      (0, 1, 2) to (19, 20, 21)

*** get_learn_run ver1

    #+BEGIN_SRC python
      #export
      def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):
          model = get_cnn_model(data, nfs, layer, **kwargs)
          init_cnn(model, uniform=uniform)
          return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)
    #+END_SRC

    # get_learn_run ver1 overview
    - `get_learn_run` works as below;
      - first ceate a model with `get_cnn_model`
      - initialize the model with `init_cnn`
      - create instances of `Learner` & `Runner` with `get_runner`
    
** 07_batchnorm.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py

    #+END_SRC
    
*** why BatchNorm works
    # article (2020)
    https://towardsdatascience.com/batch-normalization-in-3-levels-of-understanding-14c2da90a338

    # forum
    https://forums.fast.ai/t/why-0-mean-and-1-std/57211/4    

*** BatchNorm
    # video
    https://course19.fast.ai/videos/?lesson=6?t=2700
    

    #+BEGIN_SRC python
      class BatchNorm(nn.Module):
          def __init__(self, nf, mom=0.1, eps=1e-5):
              super().__init__()
              # NB: pytorch bn mom is opposite of what you'd expect
              self.mom,self.eps = mom,eps
              self.mults = nn.Parameter(torch.ones (nf,1,1))
              self.adds  = nn.Parameter(torch.zeros(nf,1,1))
              self.register_buffer('vars',  torch.ones(1,nf,1,1))
              self.register_buffer('means', torch.zeros(1,nf,1,1))

          def update_stats(self, x):
              m = x.mean((0,2,3), keepdim=True)
              v = x.var ((0,2,3), keepdim=True)
              self.means.lerp_(m, self.mom)
              self.vars.lerp_ (v, self.mom)
              return m,v
              
          def forward(self, x):
              if self.training:
                  with torch.no_grad(): m,v = self.update_stats(x)
              else: m,v = self.means,self.vars
              x = (x-m) / (v+self.eps).sqrt()
              return x*self.mults + self.adds
    #+END_SRC

    # BatchNorm overview
    - to understand batch normalizaton, keep in mind that
      there are 2 kind of things in deep learning;
      - activation: things to be calculated: 
      - parameter : things to be learned: 

    - `BatchNorm` layer does "normalization" for the activation
      of the layer preceding it

    - "normalization" means subtracting mean and deviding 
      by std for `x`

    - there are 2 group of things appearing in batch normalization
      - 1. relating to activation
        `mean` and `vars`  which are updated
        for every new batch of activation, using 
        the memory of "mean" and "var" of the current batch
        and all the preceding batches of activations

      - 2. relating to parameters
        `mults` and `adds` which are "parameters" for
        a `BatchNorm` layer and since they are parameters,
        they can be learned

    # BatchNorm::__init__ detail
    - `nf` corresponds to the value of the channel axis of
      the activations of the precedeing layer

    - `mults` and `adds` are  "γ" and "β", respectively
      for batch normalization

    - since `mults` is a parameter as mentioned above,
      we use nn.Parameter

    - since `mults` is a type of weight and
      `adds` is a type of bias,
      BatchNorm is considered as special kind of a linear layer

    - `mult` is a rank3 tensor of torch.Size([nf, 1, 1]),
      and when multiplied by `x`, broadcasting occurs

    - ?? note that the channel axis of the input and the
      activation are different

    - we use self.register_buffer('vars', torch.ones(1, nf, 1,1))
      to automatically move `vars` and `means` onto GPU on
      moving the model to GPU

      when (model, inputs) are on GPU, in order to use `vars`
      and `means` in any calculation within the model, 
      these variables must be on GPU too because we
      can't add things on GPU with things on CPU

      also, when we save a trained model, variable 
      defined with register_buffer will also be saved.

    - [ ] `self.eps`, and `self.mom` are scalars, 
      and different from tensors, they will go to GPU automatically

      https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723
      https://kite.com/python/docs/torch.nn.Module.register_buffer

    # BatchNorm::update_stats detail
    - `m` and `v` is mean and vars respectively for the
      current batch of activations, and `means`, `vars`
      are updated using `m`, `v`.

    - `x.mean((0,2,3), keepdim=True)` takes mean over
      (batch, width, height).

      - firstly, it does 'fusion' along batch axis
      - secondly, it does 'fusion' along width axis
      - thirdly, it does 'fusion' along height axis
      # - to understand this, imagine a number of cubes alinged along batch axis
      # - first, we take mean over batch axis, which corresponds to (0)
      # - then, we have one 3 x <width> x <height> cubes 
      # - this can be considered as "mean input among the batch"
      # - next, take mean over (width, height) which corresponds to (2, 3)
      # - now, we have three 1x1 cube aligned along the chanel axis
      # - each of 1x1 cube represents the mean value over
      #   (batch, width ,height) for the chanel
      # - keepdim is for broadcasting when subtracting the mean from
      each x

      #+BEGIN_SRC python

      # example of `mean((0, 2, 3), keepdim=True)`
      # note we calculate the mean for incoming activation,
      # we really do not calculate the mean for input, but
      # can get the idea of what `mean((0, 2, 3), keepdim=True)` does

      my_xb, my_yb = next(iter(data.train_dl))
      my_xb.shape, my_yb.shape;
      my_xb_reshaped = my_xb.view(-1, 1, 28, 28)
      my_xb_reshaped.shape, my_xb_reshaped.mean((0, 2, 3), keepdim=True)
      '''
      (torch.Size([512, 1, 28, 28]), tensor([[[[-0.0046]]]]))
      '''
      #+END_SRC

    - `lerp` is torch's linear interpolation 
      ( little bit of this, and little bit of that; a*0.9 + b*0.1)

    - we use `lerp`, exponentially weighted moving average,
      instead of normal moving average because normal 
      moving average requires keeping the history of activations
      for every single `BatchNorm` layer and the amount of data
      will grows quickly as the model starts to contain more
      `BatchNorm` layers

    # BatchNorm::forward detail
    - `x` is the activation coming out from the layer
      preceding the `BatchNorm` layer, and is a rank4 tensor
      with 4 axis (batch, channel, width, height)

    - [ ] (?) `with torch.no_grad()` is necessary because
      `self.update_stats(x)` involves `x` but does not 
      contribute to the calculation of the gradients

    - for tarining time, we use mean & vars of that specific batch
      of activations, whereas for inference time, we use
      averaged means & vars over many batches of activations

    - `means` is a rank4 tensor of torch.Size([1,nf,1,1])
      since it is an exponentially moving average of `m`,
      which is also a rank4 tensor of torch.Size([1,nf,1,1])

    - `mults` is a rank3 tensor of torch.Size([nf,1,1]),
      `x` is a rank4 tensor of torch.Size([bs,nf,h,w]),
      and so `x*self.mults` is possible according to the
      broadcasting rule:
      - Each tensor has at least one dimension.
      - When iterating over the dimension sizes, 
        starting at the trailing dimension, 
        the dimension sizes must either
        - be equal, 
        - one of them is 1
        - or one of them does not exist.

*** conv_layer ver2

    #+BEGIN_SRC python
      def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
          # No bias needed if using bn
          layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
                    GeneralRelu(**kwargs)]
          if bn: layers.append(BatchNorm(nf))
          return nn.Sequential(*layers)
    #+END_SRC

    # conv_layer detail
    - `conv_layer` now contains `BatchNorm` layer

    - when BathcNorm layer is inserted by setting `bn=True`,
      we do not need `bias` anymore since it is same 
      as `add` in the BatchNorm layer

    - `not bn` of `nn.Conv2d(... bias=not bn)` is just the negation of `bn`;
      if `bn` is True, then `not bn` is `False`

    - `bn` is default to be true

*** init_cnn ver2, get_learn_run ver2

    #+BEGIN_SRC python
      #export
      def init_cnn_(m, f):
          if isinstance(m, nn.Conv2d):
              f(m.weight, a=0.1)
              if getattr(m, 'bias', None) is not None: m.bias.data.zero_()
          for l in m.children(): init_cnn_(l, f)

      def init_cnn(m, uniform=False):
          f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
          init_cnn_(m, f)

      def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, uniform=False, **kwargs):
          model = get_cnn_model(data, nfs, layer, **kwargs)
          init_cnn(model, uniform=uniform)
          return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)

      # how to use
      learn,run = get_learn_run(nfs, data, 1., conv_layer, cbs=cbfs)
      run.fit(3, learn)

    #+END_SRC

    # init_cnn ver2 overview
    - `init_cnn` ver2 factors out the logic, in `init_cnn` ver1 for
      recursively initializing a laeyr, as `init_cnn_`

    # init_cnn_ overview
    - the name of `init_cnn_` and `init_cnn` are slightly different!
      look at the last "_"

    # get_learn_run ver2 overview
    - `get_learn_run` ver2 makes use of `get_cnn_model` ver3
      which accepts `layer` 

    - the implemntation of `get_learn_run` ver2 is same as
      `get_learn_run` ver1, but now it is using
      `init_cnn` ver2 instead of `init_cnn` ver1

    # get_learn_run ver2 detail
    - `get_cnn_model` & `get_cnn_layers` & `get_runner` are as below

      #+BEGIN_SRC python
        # get_cnn_model ver3
        def get_cnn_model(data, nfs, layer, **kwargs):
            return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))

        # get_cnn_layers ver2
        def get_cnn_layers(data, nfs, layer, **kwargs):
            nfs = [1] + nfs
            return [layer(nfs[i], nfs[i+1], 5 if i==0 else 3, **kwargs)
                    for i in range(len(nfs)-1)] + [
                nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], data.c)]
        # layer is a function, such as `conv_layer`, to create a cnn layer

        # get_runner
        def get_runner(model, data, lr=0.6, cbs=None, opt_func=None, loss_func = F.cross_entropy):
            if opt_func is None: opt_func = optim.SGD
            opt = opt_func(model.parameters(), lr=lr)
            learn = Learner(model, opt, loss_func, data)
            return learn, Runner(cb_funcs=listify(cbs))

      #+END_SRC

*** using get_learn_run ver2

    #+BEGIN_SRC python
      # data, loss_func, nfs
      x_train,y_train,x_valid,y_valid = get_data()
      x_train,x_valid = normalize_to(x_train,x_valid)
      train_ds,valid_ds = Dataset(x_train, y_train),Dataset(x_valid, y_valid)

      nh,bs = 50,512
      c = y_train.max().item()+1
      loss_func = F.cross_entropy

      data = DataBunch(*get_dls(train_ds, valid_ds, bs), c)

      nfs = [8,16,32,64,64]

      # callbacks
      mnist_view = view_tfm(1,28,28)
      cbfs = [Recorder,
              partial(AvgStatsCallback,accuracy),
              CudaCallback,
              partial(BatchTransformXCallback, mnist_view)]

      learn,run = get_learn_run(nfs, data, 0.9, conv_layer, cbs=cbfs)

    #+END_SRC

    - `get_learn_run` ver2 makes use of `conv_layer` ver2
      which contains batchnorm layer

*** conv_layer ver3

    #+BEGIN_SRC python
      #export
      def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
          layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
                    GeneralRelu(**kwargs)]
          if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))
          return nn.Sequential(*layers)
    #+END_SRC

    # overview
    - `conv_layer` ver3 uses builtin nn.BatchNorm2d
      whereas `conv_layer` ver2 uses home-made BatchNorm

*** LayerNorm

    #+BEGIN_SRC python
      class LayerNorm(nn.Module):
          __constants__ = ['eps']
          def __init__(self, eps=1e-5):
              super().__init__()
              self.eps = eps
              self.mult = nn.Parameter(tensor(1.))
              self.add  = nn.Parameter(tensor(0.))

          def forward(self, x):
              m = x.mean((1,2,3), keepdim=True)
              v = x.var ((1,2,3), keepdim=True)
              x = (x-m) / ((v+self.eps).sqrt())
              return x*self.mult + self.add
    #+END_SRC

    - with x.mean(1,2,3), keepdim=True)
      - firstly, it does 'fusion' along channel
      - secondly, it does 'fusion' along width
      - thirdly, it does 'fusion along height
      
*** conv_ln

    #+BEGIN_SRC python
      def conv_ln(ni, nf, ks=3, stride=2, bn=True, **kwargs):
          layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True),
                    GeneralRelu(**kwargs)]
          if bn: layers.append(LayerNorm())
          return nn.Sequential(*layers)
    #+END_SRC

    # overview
    - conv layer with layer norm

*** InstanceNorm
    #+BEGIN_SRC python
      class InstanceNorm(nn.Module):
          __constants__ = ['eps']
          def __init__(self, nf, eps=1e-0):
              super().__init__()
              self.eps = eps
              self.mults = nn.Parameter(torch.ones (nf,1,1))
              self.adds  = nn.Parameter(torch.zeros(nf,1,1))

          def forward(self, x):
              m = x.mean((2,3), keepdim=True)
              v = x.var ((2,3), keepdim=True)
              res = (x-m) / ((v+self.eps).sqrt())
              return res*self.mults + self.adds
    #+END_SRC

    # overview
    - with instance norm,
      - firstly, it does 'fusion' along width axis
      - secondly, it does 'fusion' along height axis

    - with instance norm, for every channel of every single input in a batch,
      the mean is 0, and standard deviation is 1

*** conv_in

    #+BEGIN_SRC python
      def conv_in(ni, nf, ks=3, stride=2, bn=True, **kwargs):
          layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True),
                    GeneralRelu(**kwargs)]
          if bn: layers.append(InstanceNorm(nf))
          return nn.Sequential(*layers)
    #+END_SRC

    # overview
    - conv layer with instance norm

*** conv_layer ver4 (?? same as conv_layer ver3)

    #+BEGIN_SRC python
      def conv_layer(ni, nf, ks=3, stride=2, bn=True, **kwargs):
          layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
                    GeneralRelu(**kwargs)]
          if bn: layers.append(nn.BatchNorm2d(nf, eps=1e-5, momentum=0.1))
          return nn.Sequential(*layers)

    #+END_SRC

*** TODO RunningBatchNorm
    # video
    https://course19.fast.ai/videos/?lesson=10&t=7740

    # debias (Andrew Ng)
    https://www.youtube.com/watch?v=lWzo8CajF5s

    #+BEGIN_SRC python
      class RunningBatchNorm(nn.Module):
          def __init__(self, nf, mom=0.1, eps=1e-5):
              super().__init__()
              self.mom,self.eps = mom,eps
              self.mults = nn.Parameter(torch.ones (nf,1,1))
              self.adds = nn.Parameter(torch.zeros(nf,1,1))
              self.register_buffer('sums', torch.zeros(1,nf,1,1))
              self.register_buffer('sqrs', torch.zeros(1,nf,1,1))
              self.register_buffer('batch', tensor(0.))
              self.register_buffer('count', tensor(0.))
              self.register_buffer('step', tensor(0.))
              self.register_buffer('dbias', tensor(0.))

          def update_stats(self, x):
              bs,nc,*_ = x.shape
              self.sums.detach_()
              self.sqrs.detach_()
              dims = (0,2,3)
              s = x.sum(dims, keepdim=True)
              ss = (x*x).sum(dims, keepdim=True)
              c = self.count.new_tensor(x.numel()/nc)
              mom1 = 1 - (1-self.mom)/math.sqrt(bs-1)
              self.mom1 = self.dbias.new_tensor(mom1)
              self.sums.lerp_(s, self.mom1)
              self.sqrs.lerp_(ss, self.mom1)
              self.count.lerp_(c, self.mom1)
              self.dbias = self.dbias*(1-self.mom1) + self.mom1
              self.batch += bs
              self.step += 1

          def forward(self, x):
              if self.training: self.update_stats(x)
              sums = self.sums
              sqrs = self.sqrs
              c = self.count
              if self.step<100:
                  sums = sums / self.dbias
                  sqrs = sqrs / self.dbias
                  c    = c    / self.dbias
              means = sums/c
              vars = (sqrs/c).sub_(means*means)
              if bool(self.batch < 20): vars.clamp_min_(0.01)
              x = (x-means).div_((vars.add_(self.eps)).sqrt())
              return x.mul_(self.mults).add_(self.adds)
    #+END_SRC

    # overview
    - with `RunningBatchNorm` addresses the issue of 
      varying batch size

    - with `RunningBatchNorm`, moving aveage version of
      batch mean and variance are used for training time as well
      not only for inference time.

    - however, when batch size varies, simply taking 
      moving average of variance does not make sense,
      since the contribution of variance of a batch
      to the moving average should be weighted according to 
      its batch size (=the number of the elements in a batch)

    - however, with the original definiton of the variance
      var = (xi-mean)^2 / N,
      mean is already included in the calculation before taking
      the moving average

    - so, the following formula is used instead
      variance  = E(X^2) - (E(x))^2 
      variance  = x*x / c  - (x/c)^2

      where the moving average, weighted by the batch size, is 
      calculated for `x*x`, `x`, and `c` using the weighting factor

      (`mom1 = 1 - (1-self.mom)/math.sqrt(bs-1)`)


    # update_stats detail
    - `c` is a number of the elements in the current batch over which
      `x` and `x*x` is summed

    - with `mom1 = 1 - (1-self.mom)/math.sqrt(bs-1)`,
      the variance of a small batch size(e.g. bs=2) 
      will be less weighted

      # from the forum
      This what i understood.
      We are updating the weight(mom1) that we pass into the
      linear interpolation formula(lerp). mom1 depends on mom and bs.
      When bs=2 ; mom1 = mom =0.1 (default value of mom=0.1).
      When bs=512; mom1 =0.96
      So as bs increases mom1 increases.
      Now let us look at lerp: 
      old_value * (1-wt) + new_value * wt ; where wt = mom1
      So as bs increases, mom1 increases, so weight for your
      current sample increases.
      
      As you batch size increases the computed value is 
      more “trustworthy” so you can weigh it more heavily 
      in the linear interpolation.

      4: when number of samples are not multiples of your batch size. 
      Say you have 1030 samples with a bs=512. So batch 1 and batch 2 
      will 512 samples each and the last batch will only have 6 samples.

      # forward
      - as `sums` and `sqrs` are the weighted moving average version of `s` and `s*s`,
        `count` is the weighted moving average version of 

*** conv_rnb

    #+BEGIN_SRC python
      def conv_rbn(ni, nf, ks=3, stride=2, bn=True, **kwargs):
          layers = [nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=not bn),
                    GeneralRelu(**kwargs)]
          if bn: layers.append(RunningBatchNorm(nf))
          return nn.Sequential(*layers)
    #+END_SRC

*** TODO RunningBatchNorm by Joseph
    https://forums.fast.ai/t/questions-about-runningbatchnorm-in-lesson-10/56331

    #+BEGIN_SRC python
      # initialize
      def __init__(self, nf, mom=0.1, eps=1e-5):
          super().__init__()
          
          # constants
          self.mom,self.eps = mom,eps
          
          # add scale and offset parameters to the model
          # note: nf is the number of channels
          # Q1: shouldn't self.mults and self.adds have size [1,nf,1,1]?
          self.mults = nn.Parameter(torch.ones (nf,1,1))
          self.adds = nn.Parameter(torch.zeros(nf,1,1))
          
          # register_buffer adds a persistent buffer to the module, usually used for a non-model parameter
          self.register_buffer('sums', torch.zeros(1,nf,1,1))
          self.register_buffer('sqrs', torch.zeros(1,nf,1,1))
          self.register_buffer('batch', tensor(0.))
          self.register_buffer('count', tensor(0.))
          self.register_buffer('step', tensor(0.))
          self.register_buffer('dbias', tensor(0.))

      # compute updates to buffered tensors
      def update_stats(self, x):
          
          # batchsize, number of channels
          bs,nc,*_ = x.shape
          
          # Note: for a tensor t, t.detach_() means detach t from the computation graph, i.e. don't keep track of its gradients;
          #     the '_' prefix means do it "in place"
          # Q2: why don't we also use .detach_() for self.batch, self.count, self.step, and self.dbias?
          self.sums.detach_()
          self.sqrs.detach_()
          
          # the input x is a four-dimensional tensor: 
          #    dimensions 0, 2, 3 refer to batch samples, weight matrix rows, and weight matrix columns, respectively
          #    dimension 1 refers to channels
          dims = (0,2,3)
          
          # compute s and ss, which are the sum of the weights and the sum of the squares of the weights 
          #     over dimensions (0,2,3) for this batch. s and ss each consist of one number for each channel;
          #     because keepdim=True s, and ss are each of size [1,nf,1,1]
          s = x.sum(dims, keepdim=True)
          ss = (x*x).sum(dims, keepdim=True)
          
          # Notes: 
          #   x.numel() is the number of elements in the 4-D tensor x,
          #       which is the total number of weights in the batch
          #   x.numel()/nc is the number of weights per channel in the batch
          #   y = tensor.new_tensor(x) is equivalent to y = x.clone().detach(), 
          #       the latter is the preferred way to make a copy of a tensor
          #   c is a one-dimensional tensor with a value equal to the number of weights per channel for this batch
          #       note that the number of weights per channel of a batch depends on the number of samples in the 
          #       batch; not all batches have the same number of samples
          c = self.count.new_tensor(x.numel()/nc)
          
          # momentum
          # mom1 is the 'weight' to be used in lerp_() to compute EWMAs
          #     -- see pytorch documentation for lerp_
          # if mom is 0.1 and batch size is 2, then mom1 ~ 1 - 0.9/1 = 0.1
          # if mom is 0.1 and batch size is 64, then mom1 ~ 1 - 0.9/7 ~ 0.9; 
          #     in general, mom1 increases with batch size
          # Q3: What's the logic behind the following formula for mom1?
          mom1 = 1 - (1-self.mom)/math.sqrt(bs-1)
          # self.mom1 is a one-dimensional tensor, with a value equal to mom1
          self.mom1 = self.dbias.new_tensor(mom1)
          
          # update EWMAs of sums, sqrs, which, like s and ss, have size [1,1,nf,1] 
          self.sums.lerp_(s, self.mom1)
          self.sqrs.lerp_(ss, self.mom1)

          # update EWMA of count
          # self.count keeps track of the EWMA of c,
          #     which is the number of weights per channel for a batch
          # Q4: why do we need the EWMA of c?  Aren't batch sizes always the same, except for the last batch?
          self.count.lerp_(c, self.mom1)
          
       
          # Q5: what is the logic behind the following formula for dbias?
          self.dbias = self.dbias*(1-self.mom1) + self.mom1
          
          # update the total number of samples that have been processed up till now, 
          #     i.e. the number of samples in this batch and all previous batches so far
          self.batch += bs
          
          # update the total number of batches that have been processed
          self.step += 1

      # apply a forward pass to the current batch
      def forward(self, x):
          
          # main idea of RunningBatchNorm:
          #     to normalize the batch:
          #     in training mode, use the current EWMAs accumulated in the buffers at this step (batch), 
          #         and the *current* fitted values of the model parameters mults and adds at this step
          #     in validation mode, use the final values of the EWMAs accumulated in the buffers after training,
          #         and the final fitted values of mults and adds
          if self.training: self.update_stats(x)
              
          # get the current values of the EWMAs of sums, sqrs and count from the buffers
          sums = self.sums
          sqrs = self.sqrs
          c = self.count
          
          # if the current batch number is less than 100, scale the EWMAs by 1/self.dbias
          # Q6: Why?
          if self.step<100:
              sums = sums / self.dbias
              sqrs = sqrs / self.dbias
              c    = c    / self.dbias
              
          # scale sums by 1/c to get the mean of the weights
          means = sums/c
          
          # scale sqrs by 1/c to get the mean of the squared weights
          #     then subtract the square of the mean weight from the mean of the squared weights
          # note: we recognize this as the 'computationally efficient' formula for the variance that we've seen before 
          vars = (sqrs/c).sub_(means*means)
          
          # if there are less than 20 samples so far, clamp vars to 0.01 (in case any of them becomes very small)
          if bool(self.batch < 20): vars.clamp_min_(0.01)
              
          # normalize the batch in the usual way, i.e. subtract the mean and divide by std
          # Q7: but why do we need to add eps, when we've already clamped the vars to 0.01? 
          x = (x-means).div_((vars.add_(self.eps)).sqrt())
          
          # return a scaled and offset version of the normalized batch, where the
          #     scale factors (self.mults) and offsets (self.adds) are parameters in the model
          #     Note: there's a size mismatch: self.mults and self.adds have size [nf,1,1], while x has size [1,nf,1,1]
          return x.mul_(self.mults).add_(self.adds)
    #+END_SRC

*** TODO BatchNorm with L2 regularlization
    https://blog.janestreet.com/l2-regularization-and-batch-norm/
    https://forums.fast.ai/t/great-article-on-the-interaction-between-batchnorm-and-l2-regularization-discussed-in-lesson-11/43949

* lesson11
** 07a_lsuv.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py

    #+END_SRC
    
*** ConvLayer

    #+BEGIN_SRC python
      class ConvLayer(nn.Module):
          def __init__(self, ni, nf, ks=3, stride=2, sub=0., **kwargs):
              super().__init__()
              self.conv = nn.Conv2d(ni, nf, ks, padding=ks//2, stride=stride, bias=True)
              self.relu = GeneralRelu(sub=sub, **kwargs)
          
          def forward(self, x): return self.relu(self.conv(x))
          
          @property
          def bias(self): return -self.relu.sub
          @bias.setter
          def bias(self,v): self.relu.sub = -v
          @property
          def weight(self): return self.conv.weight
    #+END_SRC

    - note that ConvLayer does not contain a BatchNorm layer

*** get_batch ver1

    #+BEGIN_SRC python
      #export
      def get_batch(dl, run):
          run.xb,run.yb = next(iter(dl))
          for cb in run.cbs: cb.set_runner(run)
          run('begin_batch')
          return run.xb,run.yb
    #+END_SRC

*** find_modules & is_lin_layer

    #+BEGIN_SRC python
      #export
      def find_modules(m, cond):
          if cond(m): return [m]
          return sum([find_modules(o,cond) for o in m.children()], [])

      def is_lin_layer(l):
          lin_layers = (nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.Linear, nn.ReLU)
          return isinstance(l, lin_layers)
    #+END_SRC

    # find_modules detail
    - `sum`:

      sum(iterable, start)
      The sum() function adds start and items of the given 
      iterable from left to right.


    #+BEGIN_SRC python

      sum([[1,2],[11,12],[21, 22]], [])

      '''
      [1, 2, 11, 12, 21, 22]
      '''
    #+END_SRC

*** append_stat ver5

    #+BEGIN_SRC python
      def append_stat(hook, mod, inp, outp):
          d = outp.data
          hook.mean,hook.std = d.mean().item(),d.std().item()
    #+END_SRC

    # overview
    - `append_stats` ver5 is a hook, and record the statistics
      on the `hook` istance everytime it is callled 
      overwriting the previous ones.

*** TODO lsuv_module

    #+BEGIN_SRC python
      #export
      def lsuv_module(m, xb):
          h = Hook(m, append_stat)

          while mdl(xb) is not None and abs(h.mean)  > 1e-3: m.bias -= h.mean
          while mdl(xb) is not None and abs(h.std-1) > 1e-3: m.weight.data /= h.std

          h.remove()
          return h.mean,h.std
    #+END_SRC

    # overview
    - [ ] why use `m.weight.data`
      https://discuss.pytorch.org/t/layer-weight-vs-weight-data/24271

    - in each of the `while` loops of ,lsuv_module, the same 
      one `xb` is used over and over again until we met the
      condition for the `while` loop.

    - all lsuv_module does is "initialization".

    - note that ConvLayer does NOT contain a BatchNorm layer
      for normalizing the outgoing activation of the layer.

    - note that adjusting the weights and biases of a layer does not
      affect the outputs of the previous layers up to that layer,
      hence lsuv_module can be applied one by one throuth all
      the layers

    # - "initialization" for each of (Conv2d, GeneralRelu) layers
    #   by gradually do the following;
    #   - the weight of Conv2d layer
    #   - the leakyness of GeneralRelu layer

        #+BEGIN_SRC python
          for m in mods: print(lsuv_module(m, xb))
        #+END_SRC

** 08_data_block.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
    #+END_SRC

*** Path

    #+BEGIN_SRC python
      #export
      import PIL,os,mimetypes
      Path.ls = lambda x: list(x.iterdir())
    #+END_SRC

*** image_extensions

    #+BEGIN_SRC python
      #export
      image_extensions = set(k for k,v in mimetypes.types_map.items() if v.startswith('image/'))
    #+END_SRC

*** setify
    #+BEGIN_SRC python
      #export
      def setify(o): return o if isinstance(o,set) else set(listify(o))
    #+END_SRC

    - python set
      https://realpython.com/python-sets/

      #+BEGIN_SRC python
        x = set(['foo', 'bar', 'baz', 'foo', 'qux'])
        x
        '''
        {'qux', 'foo', 'bar', 'baz'}
        '''
      #+END_SRC

      # overview
      - `setify` creates a set out of a string, list, or iterable

      # detail
      - listify is as below

      #+BEGIN_SRC python
        #export
        from typing import *

        def listify(o):
            if o is None: return []
            if isinstance(o, list): return o
            if isinstance(o, str): return [o]
            if isinstance(o, Iterable): return list(o)
            return [o]
      #+END_SRC

*** _get_files

    #+BEGIN_SRC python
      #export
      def _get_files(p, fs, extensions=None):
          p = Path(p)
          res = [p/f for f in fs if not f.startswith('.')
                 and ((not extensions) or f'.{f.split(".")[-1].lower()}' in extensions)]
          return res
    #+END_SRC

    # overview
    - `_get_files` recieves a list of file names, and
      creates a list of file paths by prefixing 
      the path to the file names for the file names 
      with the specified extension

    # detail
    - note that the list on the right hand side of `res = [...]`
      contains just a single long if-statement in the form of
      "if not A and (B or C)".
      #+BEGIN_SRC python
        if not f.startswith('.') and ((not extensions) or f'.{f.split(".")[-1].lower()}' in extensions)
      #+END_SRC

    - "B or C" part is saying
      "`extensions` is `None`" 
      or
      "the exptension of `f` is in `extensions`"

*** get_files
    #+BEGIN_SRC python
      #export
      def get_files(path, extensions=None, recurse=False, include=None):
          path = Path(path)
          extensions = setify(extensions)
          extensions = {e.lower() for e in extensions}
          if recurse:
              res = []
              for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)
                  if include is not None and i==0: d[:] = [o for o in d if o in include]
                  else:                            d[:] = [o for o in d if not o.startswith('.')]
                  res += _get_files(p, f, extensions)
              return res
          else:
              f = [o.name for o in os.scandir(path) if o.is_file()]
              return _get_files(path, f, extensions)
    #+END_SRC

    # overview
    - `get_files` grabs the names of the files unders the specified directory path,
      and creates a list of file paths by prefixing 
      the path to the file names for the file names 
      with the specified extension

*** compose, ItemList, ImageList

    #+BEGIN_SRC python
      #export
      def compose(x, funcs, *args, order_key='_order', **kwargs):
          key = lambda o: getattr(o, order_key, 0)
          for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)
          return x

      class ItemList(ListContainer):
          def __init__(self, items, path='.', tfms=None):
              super().__init__(items)
              self.path,self.tfms = Path(path),tfms

          def __repr__(self): return f'{super().__repr__()}\nPath: {self.path}'
          
          def new(self, items, cls=None):
              if cls is None: cls=self.__class__
              return cls(items, self.path, tfms=self.tfms)
          
          def  get(self, i): return i
          def _get(self, i): return compose(self.get(i), self.tfms)
          
          def __getitem__(self, idx):
              res = super().__getitem__(idx)
              if isinstance(res,list): return [self._get(o) for o in res]
              return self._get(res)

      class ImageList(ItemList):
          @classmethod
          def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):
              if extensions is None: extensions = image_extensions
              return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)
          
          def get(self, fn): return PIL.Image.open(fn)
    #+END_SRC

    # compose overview
    - `compose` applys a passed `funcs` in "pipe-line"
      x2 = f1(x1)
      x3 = f2(x2)
      
      - `funcs` are arranged according to `_order` property

    # ItemList overview
    - `ItemList` is a 'template' to create a `ListContainer`
      subclass which implements various `get` depending on
      a type of item to contain

    - `ItemList` inherits `ListContainer`, and overwrites
      `__getitem__`

    - notice that so far, an `ItemList` subclass instance  that `ItemList` expcets 
      is just an array of file names, and not yet associates with a label;
      label will be given by `CategoryProcessor`

    # ItemList::__init__ overview
    - `__init__` expects a list of someting

    # ItemList::__getitem__ overview
    - `__getitem__` delegate to `_get` to retrieve the item

    # ItemListf::_get overview
    - `_get` is just a version of `get` extended with `self.tfms`
      
    # ItemList::get detail
    - `get` method is expected to be overwritten by each concrete subclass
      inheritting `ItemList` such as `ImageList`

    - in the case of `ItemList`, `get` is as below;
      #+BEGIN_SRC python
        def get(self, fn): return PIL.Image.open(fn)
      #+END_SRC

    # ItemList::new detail
    - for a `ItemList` subclass that does not overwrites
      `new` methods, `self.__class__` will be that subclass


    # ImageList::from_files overview
    - `from_files` recieves a directory path, an extension
      and then returns an `ImageList` instance

    # ImageList::from_files detail
    - `from_files` is decorated by `@classmethod`, and 
      the constructor (i.e., `__init__`) is passed as `cls`

    - (??)since `__init__` is not defined on `ImageList`,
      it will be one created automatically inside which
      `__init__` inherited from `ItemList` is called
      
*** Transform, MakeRGB

    #+BEGIN_SRC python
      #export
      class Transform(): _order=0

      class MakeRGB(Transform):
          def __call__(self, item): return item.convert('RGB')

      def make_rgb(item): return item.convert('RGB')
    #+END_SRC

    - `_order` is used to sort transfoms in `compose` 
      which is called from `_get` of `ItemList`
      (`_get` is called from `__getitem__`)

      # ItemList
      #+BEGIN_SRC python
        compose(x, funcs, *args, order_key='_order', **kwargs):
        key = lambda o: getattr(o, order_key, 0)
        for f in sorted(listify(funcs), key=key): x = f(x, **kwargs)
        return x

        class ItemList(ListContainer):
        def __init__(self, items, path='.', tfms=None):
            super().__init__(items)
            self.path,self.tfms = Path(path),tfms

        def __repr__(self): return f'{super().__repr__()}\nPath: {self.path}'

        def new(self, items, cls=None):
            if cls is None: cls=self.__class__
            return cls(items, self.path, tfms=self.tfms)

        def  get(self, i): return i
        def _get(self, i): return compose(self.get(i), self.tfms)

        def __getitem__(self, idx):
            res = super().__getitem__(idx)
            if isinstance(res,list): return [self._get(o) for o in res]
                return self._get(res)
      #+END_SRC
    
*** grandparent_splitter, split_by_func
    #+BEGIN_SRC python
      #export
      def grandparent_splitter(fn, valid_name='valid', train_name='train'):
          gp = fn.parent.parent.name
          return True if gp==valid_name else False if gp==train_name else None

      def split_by_func(items, f):
          mask = [f(o) for o in items]
          # `None` values will be filtered out
          f = [o for o,m in zip(items,mask) if m==False]
          t = [o for o,m in zip(items,mask) if m==True ]
          return f,t
    #+END_SRC

    - `grandparent_splitter` judges if a file is under
      the directory for validation set, or the directory for training set

    - split_by_func splits `items` and creates two lists
      out of it based on the `mask` created by splitter `f`

*** SplitData

    #+BEGIN_SRC python
      #export
      class SplitData():
          def __init__(self, train, valid): self.train,self.valid = train,valid
              
          def __getattr__(self,k): return getattr(self.train,k)
          #This is needed if we want to pickle SplitData and be able to load it back without recursion errors
          def __setstate__(self,data:Any): self.__dict__.update(data) 
          
          @classmethod
          def split_by_func(cls, il, f):
              lists = map(il.new, split_by_func(il.items, f))
              return cls(*lists)

          def __repr__(self): return f'{self.__class__.__name__}\nTrain: {self.train}\nValid: {self.valid}\n'
    #+END_SRC

    # overview
    - `SplitData` is just a storage to store a pair of 
      training & validation data each of which is a `Dataset` instance

    # split_by_func detail
    - `SplitData::split_by_func` is a factory method to create a
      `SplitData` instance out of a `ItemList` subclass instance

    - `split_by_func` is a global function previously defined, 
        which returns two lists created out of one list

        #+BEGIN_SRC python
        def split_by_func(items, f):
            mask = [f(o) for o in items]
            # `None` values will be filtered out
            f = [o for o,m in zip(items,mask) if m==False]
            t = [o for o,m in zip(items,mask) if m==True ]
            return f,t
      #+END_SRC

    - since the global function `split_by_func` returns 
      pair of plain lists each of which can be a different type 
      of list than the original `il.items`, the class method
      `SplitData::split_by_func` calls
      `map(il.new, split_by_func(il.items,f))`

    - `il.new` is defined on `ItemList` as below:

      #+BEGIN_SRC python
        def new(self, items, cls=None):
            if cls is None: cls=self.__class__
            return cls(items, self.path, tfms=self.tfms)
      #+END_SRC

*** uniqueify

    #+BEGIN_SRC python
      #export
      from collections import OrderedDict

      def uniqueify(x, sort=False):
          res = list(OrderedDict.fromkeys(x).keys())
          if sort: res.sort()
          return res
    #+END_SRC

    # uniqueify overview
    - `uniqueify` returns a list of dictionary keys

    # uniqueify detail
    - OrderedDict.fromkeys
      https://docs.python.org/3/library/collections.html#collections.OrderedDict
      https://appdividend.com/2019/04/13/python-dictionary-fromkeys-example-fromkeys-method-tutorial/


    #+BEGIN_SRC python
      #export
      class ListContainer():
          def __init__(self, items): self.items = listify(items)
          def __getitem__(self, idx):
              if isinstance(idx, (int,slice)): return self.items[idx]
              if isinstance(idx[0],bool):
                  assert len(idx)==len(self) # bool mask
                  return [o for m,o in zip(idx,self.items) if m]
              return [self.items[i] for i in idx]
          def __len__(self): return len(self.items)
          def __iter__(self): return iter(self.items)
          def __setitem__(self, i, o): self.items[i] = o
          def __delitem__(self, i): del(self.items[i])
          def __repr__(self):
              res = f'{self.__class__.__name__} ({len(self)} items)\n{self.items[:10]}'
              if len(self)>10: res = res[:-1]+ '...]'
              return res

      myList = ListContainer(['tench', 'saba', 'saba','aji'])
      myList
      '''
      ListContainer (4 items)
      ['tench', 'saba', 'saba', 'aji']
      '''

      from collections import OrderedDict
      myDict = OrderedDict.fromkeys(myList)
      myKeys = myDict.keys()      print(myDict.keys())
      print(list(myDict.keys()))
      '''
      odict_keys(['tench', 'saba', 'aji'])
      ['tench', 'saba', 'aji']
      '''

      myObj = {v:k for k,v in enumerate(myKeys)}
      myObj
      '''
      {'aji': 2, 'saba': 1, 'tench': 0}

      '''

    #+END_SRC

*** Processor, CategoryProcessor

    #+BEGIN_SRC python
      #export
      class Processor(): 
          def process(self, items): return items

      class CategoryProcessor(Processor):
          def __init__(self): self.vocab=None
          
          def __call__(self, items):
              #The vocab is defined on the first use.
              if self.vocab is None:
                  self.vocab = uniqueify(items)
                  self.otoi  = {v:k for k,v in enumerate(self.vocab)}
              return [self.proc1(o) for o in items]
          def proc1(self, item):  return self.otoi[item]
          
          def deprocess(self, idxs):
              assert self.vocab is not None
              return [self.deproc1(idx) for idx in idxs]
          def deproc1(self, idx): return self.vocab[idx]
    #+END_SRC

    # CategoryProcessor overivew
    - `CategoryProcessor` "numer-ify" string labels, 
      and thanks to `_call__` method, it can be used as

    #+BEGIN_SRC python
      MyCategoryProcessorInstance = CategoryProcessor()
      MyCategoryProcessorInstance(someItem)
    #+END_SRC


      `MyCategoryProcessorInstance(someItem)`


    # CategoryProcessor::__call__ detail
    - `__call__` of `CategoryProcessor` will receive
      a ItemList subclass instance corresponding to 
      `ys`, i.e., labels such as 'tench', 'dog', 'cat', etc.

    - applying `uniqueify` to `items` will create a new list
      without any redundant value in the form of 
      ['tench', 'dog', 'cat', ...]
      and set it to `self.vocab`

    - `{v:k for k,v in enumerate(self.vocab }` creates a set in the form of
      {'tench':1 , 'dog':2, 'cat':3, ...}
      and set it to `self.otoi`

    - in the end, `__call__` returns a list of numbers in the form of
      [1, 2, 3, 3, 1, ...]

    # CategoryProcessor::proc1 overview    
    - `proc1` just translates a string(=label name) to a number using `self.otoi`

    # CategoryProcessor::deproc1 overview    
    - `deproc1` translates an index to a string(=label name)

*** parent_labeler, LabeledData, label_by_func

    #+BEGIN_SRC python
      #export
      def parent_labeler(fn): return fn.parent.name

      def _label_by_func(ds, f, cls=ItemList): return cls([f(o) for o in ds.items], path=ds.path)

      #This is a slightly different from what was seen during the lesson,
      #   we'll discuss the changes in lesson 11
      class LabeledData():
          def process(self, il, proc): return il.new(compose(il.items, proc))

          def __init__(self, x, y, proc_x=None, proc_y=None):
              self.x,self.y = self.process(x, proc_x),self.process(y, proc_y)
              self.proc_x,self.proc_y = proc_x,proc_y
              
          def __repr__(self): return f'{self.__class__.__name__}\nx: {self.x}\ny: {self.y}\n'
          def __getitem__(self,idx): return self.x[idx],self.y[idx]
          def __len__(self): return len(self.x)
          
          def x_obj(self, idx): return self.obj(self.x, idx, self.proc_x)
          def y_obj(self, idx): return self.obj(self.y, idx, self.proc_y)
          
          def obj(self, items, idx, procs):
              isint = isinstance(idx, int) or (isinstance(idx,torch.LongTensor) and not idx.ndim)
              item = items[idx]
              for proc in reversed(listify(procs)):
                  item = proc.deproc1(item) if isint else proc.deprocess(item)
              return item

          @classmethod
          def label_by_func(cls, il, f, proc_x=None, proc_y=None):
              return cls(il, _label_by_func(il, f), proc_x=proc_x, proc_y=proc_y)

      def label_by_func(sd, f, proc_x=None, proc_y=None):
          train = LabeledData.label_by_func(sd.train, f, proc_x=proc_x, proc_y=proc_y)
          valid = LabeledData.label_by_func(sd.valid, f, proc_x=proc_x, proc_y=proc_y)
          return SplitData(train,valid)
    #+END_SRC

    # `parent_labeler` overview
    - `parent_labeler` can tell the label of a file
      since the name of the parent directory is same as the label

    # `_label_by_func` overview
    - `_labe_by_func` is a facotry method for creating labels
      from an instance of `ItemList` subclass such as `ImageList
      by applying `f`
    
    # `_label_by_func` detail
    - ds: an instance of an ItemList subclass such as `ImageList`
    - `f` is a function such as `parent_labeler`
    - return value: an ItemList subclass instance corresponding to
      labels such as 'tench', 'cat', 'dog', etc

    # `LabeledData` overview
    - `LabeledData` is for creating labels from an instance of
      `ItemList` subclass such as `ImageList`, and associate it
      with the passed instance of `ItemList` subclass,
      additionaly applying `CategoryProcessor` to the created 
      labels in order to "numer-ify" the labels.

    - `LabeledData` class is expected to be used in the following way
      - call the global function `label_by_func`
      - the global function `label_by_func` calls the class method
        `LabeledData::label_by_func`
      - `LabeledData::label_by_func` calls `LabeledData::__init__`

    - since `LabeledData` implements `__getitem__` similar to 
      that of `Dataset`, a `LabeledData` instance can be
      passed to `ds` of `DataLoader::__init__` inside `get_dls`

      - `get_dls` is as below. (note `DataLoader` is from PyTorch.)

        # get_dls
        #+BEGIN_SRC python
          def get_dls(train_ds, valid_ds, bs, **kwargs):
              return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
                      DataLoader(valid_ds, batch_size=bs*2, **kwargs))
        #+END_SRC

        
        # home made DataLoader ver2 for showing the implementation of pytorch `DataLoader`
        #+BEGIN_SRC python
          def collate(b):
              xs,ys = zip(*b)
              return torch.stack(xs),torch.stack(ys)

          class DataLoader():
              def __init__(self, ds, sampler, collate_fn=collate):
                  self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn

              def __iter__(self):
                  for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])

        #+END_SRC

        # DataSet
        #+BEGIN_SRC python
          #export
          class Dataset():
              def __init__(self, x, y): self.x,self.y = x,y
              def __len__(self): return len(self.x)
              def __getitem__(self, i): return self.x[i],self.y[i]
        #+END_SRC


    # `LabeledData::process` overview        
    - `LabeledData::process` creates "processed" version of 
      the passed instance of `ItemList` subclass such as `ImageList`

    # `LabeledData::process` detail
    - `il`: an instance of `ItemList` subclass such as `ImageList`
    - `proc`: an array of functions
    - `il.new` is necessary since the result of `compose(il.items, proc)`
      may be just a plain list different from the type of the 
      original instance of a `ItemList` subclass

    # `LabeledData::__init__` detail
    - `x`: an instance of `ItemList` subclass such as `ImageList`

    - `y`: an instance of `ItemList` subclass corresponding to labels
      which was created by `_label_by_func` from the passed instance
      of a `ItemList` subclass

    - `LabeledData::__init__` applys `proc_x` & `proc_y`
      to `x` & `y`, respectively

    - `proc_y` is for example, an instance of 
      `CategoryProcessor` for "numer-ifying" the label strings

    # `LabeledData::label_by_func` overview        
    - `LabeledData::label_by_func` is a factory method to create
      a `LabeledData` instance which stores "x" & "y", out of `x`

    # `LabeledData::label_by_func` detail
    - `cls`: `LabeledData`
    - `il` will be `x` for `LabeledData::__init__`
    - `f` will be `parent_labeler`, for exmaple.
    - `_label_by_func(il, f)` will be `y` for `LabeledData::__init__`

    # `label_by_func` (global function) overview              
    - a global function `label_fy_func` is a factory method
      to create a `SplitData` instance from the original
      `SplitData` instance by applying LabeledData::label_fy_func to 
      `.train` & `.valid` of the original `SplitData` instance

*** Putting SplitData & LabeledData & CategoryProcessor altogether

    #+BEGIN_SRC python
      path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)
      il = ImageList.from_files(path, tfms=make_rgb)
      splitter = partial(grandparent_splitter, valid_name='val')

      sd = SplitData.split_by_func(il, splitter); sd
      ll = label_by_func(sd, parent_labeler, proc_y=CategoryProcessor())


    #+END_SRC

    - `sd` is an instance having `self.train`, `self.valid`
      which is created by splitting the `il` instance (e.g. ImageList instance)
      based on if a file is under training folder or validation folder

*** ResizeFixed, to_byte_tensor, to_float_tensor

    #+BEGIN_SRC python
      #export
      class ResizeFixed(Transform):
          _order=10
          def __init__(self,size):
              if isinstance(size,int): size=(size,size)
              self.size = size
              
          def __call__(self, item): return item.resize(self.size, PIL.Image.BILINEAR)

      def to_byte_tensor(item):
          res = torch.ByteTensor(torch.ByteStorage.from_buffer(item.tobytes()))
          w,h = item.size
          return res.view(h,w,-1).permute(2,0,1)
      to_byte_tensor._order=20

      def to_float_tensor(item): return item.float().div_(255.)
      to_float_tensor._order=30
    #+END_SRC

    - for the raw image, a data type(=dtype) is unsigned int 8 (=uint8)

      #+BEGIN_SRC python
        import numpy
        imga = numpy.array(img)

        imga.shape # (160, 213, 3)

        imga[:10,:10,0]
        '''
        array([[239, 239, 239, 239, ..., 239, 239, 246, 239],
               [239, 239, 239, 239, ..., 239, 239, 234, 240],
               [238, 238, 238, 238, ..., 238, 238, 238, 235],
               [237, 237, 237, 237, ..., 237, 237, 244, 236],
               ...,
               [235, 235, 235, 235, ..., 235, 235, 239, 233],
               [235, 235, 235, 235, ..., 235, 235, 233, 241],
               [239, 238, 235, 232, ..., 238, 234, 236, 239],
               [234, 236, 235, 233, ..., 236, 232, 230, 235]], dtype=uint8)
        '''
      #+END_SRC

    - `to_byte_tensor` is barrowed from torch vision

    - `to_float_tensor` is for turning the integer values
      ranging [0,255] into float values ranging [0, 1]

*** show_image

    #+BEGIN_SRC python
      #export
      def show_image(im, figsize=(3,3)):
          plt.figure(figsize=figsize)
          plt.axis('off')
          plt.imshow(im.permute(1,2,0))
    #+END_SRC

    - for the detail, watch the lecture video arround '53:42
      https://course19.fast.ai/videos/?lesson=11
      
*** DataBunch ver2

    #+BEGIN_SRC python
      #export
      class DataBunch():
          def __init__(self, train_dl, valid_dl, c_in=None, c_out=None):
              self.train_dl,self.valid_dl,self.c_in,self.c_out = train_dl,valid_dl,c_in,c_out

          @property
          def train_ds(self): return self.train_dl.dataset

          @property
          def valid_ds(self): return self.valid_dl.dataset

    #+END_SRC

    # overview
    - `DataBunch` is just a storage to store 
      - `DataLoader` instance for training
      - `DataLoader` instance for validation

*** databunchify (SplitData.to_databunchify)

    #+BEGIN_SRC python
      #export
      def databunchify(sd, bs, c_in=None, c_out=None, **kwargs):
          dls = get_dls(sd.train, sd.valid, bs, **kwargs)
          return DataBunch(*dls, c_in=c_in, c_out=c_out)

      SplitData.to_databunch = databunchify
    #+END_SRC

    
    # detail
    - `get_dls` is as below

      #+BEGIN_SRC python

        def get_dls(train_ds, valid_ds, bs, **kwargs):
        # DataLoader is from pytorch    
            return (DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs),
                    DataLoader(valid_ds, batch_size=bs*2, **kwargs)) 

      #+END_SRC

      # home made DataLoader ver2
      #+BEGIN_SRC python
      def collate(b):
          xs,ys = zip(*b)
          return torch.stack(xs),torch.stack(ys)

      class DataLoader():
          def __init__(self, ds, sampler, collate_fn=collate):
              self.ds,self.sampler,self.collate_fn = ds,sampler,collate_fn

          def __iter__(self):
              for s in self.sampler: yield self.collate_fn([self.ds[i] for i in s])

      #+END_SRC

*** DONE normalize_chan

    #+BEGIN_SRC python
      #export
      def normalize_chan(x, mean, std):
          return (x-mean[...,None,None]) / std[...,None,None]

      _m = tensor([0.47, 0.48, 0.45])
      _s = tensor([0.29, 0.28, 0.30])
      norm_imagenette = partial(normalize_chan, mean=_m.cuda(), std=_s.cuda())
    #+END_SRC
    
    - [X] why are we indexing `[...,None,None]` ?
      => for broadcasting. `mean` is rank1 tensor
      whereas `x` is a rank4 tensor in the form of
      torch.Size([64, 3, 128, 128]

      - broadcasting rule:
        - Each tensor has at least one dimension.
        - When iterating over the dimension sizes, 
          starting at the trailing dimension, 
          the dimension sizes must either
          - be equal, 
          - or, one of them is 1
          - or, one of them does not exist.

***  get_cnn_layers ver3, get_cnn_model ver4, get_learn_run ver2

    #+BEGIN_SRC python
      #export
      import math
      def prev_pow_2(x): return 2**math.floor(math.log2(x))

      def get_cnn_layers(data, nfs, layer, **kwargs):
          def f(ni, nf, stride=2): return layer(ni, nf, 3, stride=stride, **kwargs)
          l1 = data.c_in
          l2 = prev_pow_2(l1*3*3)
          layers =  [f(l1  , l2  , stride=1),
                     f(l2  , l2*2, stride=2),
                     f(l2*2, l2*4, stride=2)]
          nfs = [l2*4] + nfs
          layers += [f(nfs[i], nfs[i+1]) for i in range(len(nfs)-1)]
          layers += [nn.AdaptiveAvgPool2d(1), Lambda(flatten), 
                     nn.Linear(nfs[-1], data.c_out)]
          return layers

      def get_cnn_model(data, nfs, layer, **kwargs):
          return nn.Sequential(*get_cnn_layers(data, nfs, layer, **kwargs))

      def get_learn_run(nfs, data, lr, layer, cbs=None, opt_func=None, **kwargs):
          model = get_cnn_model(data, nfs, layer, **kwargs)
          init_cnn(model)
          return get_runner(model, data, lr=lr, cbs=cbs, opt_func=opt_func)
    #+END_SRC


     # get_cnn_layers ver3 overview
     - `get_cnn_layers` ver3 inserts a preceding 3 layers into 
       a cnn layer, for a better feature representation

       - it calculates, for the first layer, the right number
         of kernels(numbers of "金太郎飴")  GIVEN
         the kernel size (=3x3, face size of "金太郎飴")

       - the reson for the kernel size (face size of "金太郎飴") of
         3x3 is also discussed around 1:00:00
         (see "Bag of tricks" paper for more detail)
         
       - note that "1:09:05" & "1:17:05" of lesson10 was
         discussing about the right kernel size
         (face size of "金太郎飴") for the first layer 
         GIVEN the number of the kernels  (numbers of "金太郎飴")

       - in summary, in lesson11, the number of the
         kernels for the 1st layer is chosen so that the
         computation is not wasted, and in addition, 
         the right kernel size is also discussed 

     # get_cnn_layers ver3 detail
     - `l2 = prev_pow_2(l1*3*3)` is based for producing
       less number of a cells than the number of the
       cells to which 3x3xc_in kernel is applied
       
       https://course19.fast.ai/videos/?lesson=11&t=3500

       - think of 128*128*3 image where `c_in` = 3.
         For 3*3*3 kernel with stride 1,
         we have originally have roughly 128 groups of 3*3*3 cells
         to each which the kernel is applied, and to produce less
         numberof the cells, number of the kernel (=filter)
         should be less than 3x3x3

     - `f` is a function to create conv2d layer with stride 2,
       kernel size 3, and `layer` is like `nn.Conv2d`


     # get_cnn_model ver4 overview
     - `get_cnn_model` ver4 is same as ver3, but refering to `get_cnn_layers` ver3

     # get_learn_run ver2 overview
     - `get_learn_run` ver2 is same as ver1, but refering to `get_cnn_model` ver4

     # get_learn_run ver2 detail
     - `init_cnn` is `init_cnn` ver2 and as below

       #+BEGIN_SRC python
         def init_cnn_(m, f):
             if isinstance(m, nn.Conv2d):
                 f(m.weight, a=0.1)
                 if getattr(m, 'bias', None) is not None: m.bias.data.zero_()
             for l in m.children(): init_cnn_(l, f)

         def init_cnn(m, uniform=False):
             f = init.kaiming_uniform_ if uniform else init.kaiming_normal_
             init_cnn_(m, f)
       #+END_SRC

*** putting get_cnn_layers ver3, get_cnn_model ver4, get_learn_run ver2 all together       

    #+BEGIN_SRC python
      sched = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.1,0.3,0.05))

      learn,run = get_learn_run(nfs, data, 0.2, conv_layer, cbs=cbfs+[
          partial(ParamScheduler, 'lr', sched)
      ])
    #+END_SRC

*** model_summary

    #+BEGIN_SRC python
      #export
      def model_summary(run, learn, data, find_all=False):
          xb,yb = get_batch(data.valid_dl, run)
          device = next(learn.model.parameters()).device#Model may not be on the GPU yet
          xb,yb = xb.to(device),yb.to(device)
          mods = find_modules(learn.model, is_lin_layer) if find_all else learn.model.children()
          f = lambda hook,mod,inp,out: print(f"{mod}\n{out.shape}\n")
          with Hooks(mods, f) as hooks: learn.model(xb)
    #+END_SRC

** 09_optimizers.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
    #+END_SRC

*** TODO Optimizer ver2

    #+BEGIN_SRC python
      class Optimizer():
          def __init__(self, params, steppers, **defaults):
              # might be a generator
              self.param_groups = list(params)
              # ensure params is a list of lists
              if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]
              self.hypers = [{**defaults} for p in self.param_groups]
              self.steppers = listify(steppers)

          def grad_params(self):
              return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)
                  for p in pg if p.grad is not None]

          def zero_grad(self):
              for p,hyper in self.grad_params():
                  p.grad.detach_()
                  p.grad.zero_()

          def step(self):
              for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)
    #+END_SRC


    - params refers to the weights & bias tensor for each layer.
      param groups looks like below

      #+BEGIN_SRC python
        # the 1st param group is for layer 1 to m (main body)
        # the 2nd param group is for layer m+1 to n (last few layers)
        [
            [W1, B1, ..., Wm, Bm],
            [Wm+1, Bm+1 ..., Wn, Bn]
        ]
      #+END_SRC

    - if `params` is a list of list, it means we chose to
      split up parameters into groups. 

    - if `params` is not a list of list, it means we chose to
      create only a single parameter group that contains the
      parameters for all the layers.

      #+BEGIN_SRC python
        [W1, B1, ..., Wn, Bn]
      #+END_SRC

    - in order to handle both cases (split & non-split),
      we make sure that non-split parameters is tunred into
      a list of list.

      #+BEGIN_SRC python
        if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]
      #+END_SRC

    - each parameter group associates with 1 dictionary which
      determines hyper parameters for the parameter group.
      These dictionaries are stored in self.hypers.

      #+BEGIN_SRC python
        self.hypers = [{**defaults} for p in self.param_groups]

        # a hyper parameter dictionary looks like
        {'lr': 0.5, 'mom':0.1 ...}

        # self.hypers looks like
        [{'lr': 0.5, 'mom':0.1 ...}, {'lr': 0.4, 'mom':0.1...}]

      #+END_SRC

    - the dictionary will be default to `{**defaults}` ,
      where `defaults` is a dictionary created based on 
      the argument passed.

      #+BEGIN_SRC python
        Optimizer(..., lr=0.6, mom=0.1, ...)

        # {**defaults} looks like
        {'lr':0.6, 'mom':0.1}
      #+END_SRC

    - we do `{**defaults}` to clone `defaults` and
      not refering to the same dicrionary

    - `zip` part inside `grad_params` creates a list of tuples
      from `self.param_groups` and `self.hypers`

      #+BEGIN_SRC python
        # self.param_groups
        [
            [W1, B1, ..., Wm, Bm],
            [Wm+1, Bm+1, ..., Wn, Bn]
        ]

        # self.hypers
        [{'lr': 0.5, 'mom':0.1 ...}, {'lr': 0.4, 'mom':0.1...}]


        # zip(self.param_groups, self.hypers)
        [
            ([W1, B1, ..., Wm, Bm],  {'lr': 0.5, 'mom':0.1 ...})
            ([Wm+1, Bm+1,  ..., Wn,Bn],  {'lr': 0.4, 'mom':0.1...})
        ]
      #+END_SRC

    - `grad_params` has double for-loops. The outer for-loop
      is looped through `zip(self.param_groups,self.hypers)` and
      the inner for-loop is looped through `pg`, and the
      returned list consists of `(p, hyper)` created in
      each of the inner for loop.

    - in the end, `grad_params` returns below;

      #+BEGIN_SRC python

        [
            (W1, {'lr': 0.5, 'mom':0.1 ...}),
            (B1, {'lr': 0.5, 'mom':0.1 ...}),
            ...
            (Wm, {'lr': 0.4, 'mom':0.1 ...}),
            (Bm, {'lr': 0.4, 'mom':0.1 ...}),

            (Wm+1, {'lr': 0.4, 'mom':0.1 ...}),
            (Bm+1, {'lr': 0.4, 'mom':0.1 ...}),    
            ...
            (Wn, {'lr': 0.4, 'mom':0.1 ...}),
            (Bn, {'lr': 0.4, 'mom':0.1 ...}),    

        ]
      #+END_SRC

    - inside `step`, `hyper` is passed down as **kwargs of `compose`

    - `step` applys each stepper inside `steppers` one by one
      to `p` in a "pip-line" way

    - [ ] what `p.grad.detach_()` does 
      - 1:09:00
      - `detach` removes any gradient computational history

      http://www.bnikolic.co.uk/blog/pytorch-detach.html


      https://discuss.pytorch.org/t/detach-no-grad-and-requires-grad/16915/2

      "detach() detaches the output from the computationnal graph. So no gradient will be backproped along this variable.
      torch.no_grad says that no operation should build the graph.
      The difference is that one refers to only a given variable on which it’s called. The other affects all operations taking place within the with statement."

*** sgd_step

    #+BEGIN_SRC python
      #export
      def sgd_step(p, lr, **kwargs):
          p.data.add_(-lr, p.grad.data)
          return p
    #+END_SRC

*** Recorder ver3, ParamScheduler ver3

    #+BEGIN_SRC python
      #export
      class Recorder(Callback):
          def begin_fit(self): self.lrs,self.losses = [],[]

          def after_batch(self):
              if not self.in_train: return
              self.lrs.append(self.opt.hypers[-1]['lr'])
              self.losses.append(self.loss.detach().cpu())        

          def plot_lr  (self): plt.plot(self.lrs)
          def plot_loss(self): plt.plot(self.losses)
              
          def plot(self, skip_last=0):
              losses = [o.item() for o in self.losses]
              n = len(losses)-skip_last
              plt.xscale('log')
              plt.plot(self.lrs[:n], losses[:n])

      class ParamScheduler(Callback):
          _order=1
          def __init__(self, pname, sched_funcs):
              self.pname,self.sched_funcs = pname,listify(sched_funcs)

          def begin_batch(self): 
              if not self.in_train: return
              fs = self.sched_funcs
              if len(fs)==1: fs = fs*len(self.opt.param_groups)
              pos = self.n_epochs/self.epochs
              for f,h in zip(fs,self.opt.hypers): h[self.pname] = f(pos)
              
      # LR_Find ver1      
      class LR_Find(Callback):
          _order=1
          def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10):
              self.max_iter,self.min_lr,self.max_lr = max_iter,min_lr,max_lr
              self.best_loss = 1e9
              
          def begin_batch(self): 
              if not self.in_train: return
              pos = self.n_iter/self.max_iter
              lr = self.min_lr * (self.max_lr/self.min_lr) ** pos
              for pg in self.opt.hypers: pg['lr'] = lr
                  
          def after_step(self):
              if self.n_iter>=self.max_iter or self.loss>self.best_loss*10:
                  raise CancelTrainException()
              if self.loss < self.best_loss: self.best_loss = self.loss
    #+END_SRC

    - with ParamScheduler ver3, hyper parameters are stored 
      on `self.opt.hypers`, whereas with ParamScheduler ver2, 
      we used to store hyper parameters as properties of a
      parameter group

    - self.opt.hypers is in the form of 

    #+BEGIN_SRC python
      [{'lr': 0.5, 'mom':0.1 ...}, {'lr': 0.4, 'mom':0.1...}]
    #+END_SRC

*** using Optimizer ver2

    #+BEGIN_SRC python
      #export
      def sgd_step(p, lr, **kwargs):
          p.data.add_(-lr, p.grad.data)
          return p

      opt_func = partial(Optimizer, steppers=[sgd_step])

      sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)])

      cbfs = [partial(AvgStatsCallback,accuracy),
              CudaCallback, Recorder,
              partial(ParamScheduler, 'lr', sched)]

      learn,run = get_learn_run(nfs, data, 0.4, conv_layer, cbs=cbfs, opt_func=opt_func)

      run.fit(1, learn)

    #+END_SRC

*** TODO weight decay & l2 regularization
    - "l2 regularization" refers to add extra wd*weight to `weight.grad`
      to the gradient 

    - "weight decay" refers to add extra term '-lr*wd*weight' 
      when updating weight

      #+BEGIN_SRC python
        # l2 regularization
        weight.grad += wd * weight

        # weight decay
        new_weight = weight - lr*(weight.grad + wd*weight)
      #+END_SRC

    - the above two become equivalent for SGD, but NOT
      for RMSProp and Adam.

    - with RMSProp and Adam, updating equation is different than
      `new_weight = weight - lr * (weight.grad )`

    - so substituting `weight.grad` with `weight.grad + wd * weight` 
      in the weight updating equation yields an equation different than
      `new_weight = weight - lr * (weight.grad )`

    - for Adam optimizer, 
      https://arxiv.org/pdf/1711.05101.pdf

*** weight_decay

    #+BEGIN_SRC python
      #export
      def weight_decay(p, lr, wd, **kwargs):
          p.data.mul_(1 - lr*wd)
          return p
      weight_decay._defaults = dict(wd=0.)
    #+END_SRC

    - to introduce weight decay, we can either modify the loss function or
      the equation for updating gradient.
      `weight_decay` corresponds to modifying the equation for
      updating gradient.

    - modifying the equation for updating gradient is done by
      adding `weight_decay` as a stepper

*** TODO l2_reg

    #+BEGIN_SRC python
      #export
      def l2_reg(p, lr, wd, **kwargs):
          p.grad.data.add_(wd, p.data)
          return p
      l2_reg._defaults = dict(wd=0.)
    #+END_SRC

    - `add_` adds up `p.data` after multiplying it by `wd`
    - [ ] how to use `l2_reg`?

*** maybe_update

    #+BEGIN_SRC python
      #export
      def maybe_update(os, dest, f):
          for o in os:
              for k,v in f(o).items():
                  if k not in dest: dest[k] = v

      def get_defaults(d): return getattr(d,'_defaults',{})
    #+END_SRC

    - `getattr`
    - `items()` returns tuples of key&value pairs

*** Optimizer ver3

    #+BEGIN_SRC python
      #export
      class Optimizer():
          def __init__(self, params, steppers, **defaults):
              self.steppers = listify(steppers)
              maybe_update(self.steppers, defaults, get_defaults)
              # might be a generator
              self.param_groups = list(params)
              # ensure params is a list of lists
              if not isinstance(self.param_groups[0], list): self.param_groups = [self.param_groups]
              self.hypers = [{**defaults} for p in self.param_groups]

          def grad_params(self):
              return [(p,hyper) for pg,hyper in zip(self.param_groups,self.hypers)
                  for p in pg if p.grad is not None]

          def zero_grad(self):
              for p,hyper in self.grad_params():
                  p.grad.detach_()
                  p.grad.zero_()

          def step(self):
              for p,hyper in self.grad_params(): compose(p, self.steppers, **hyper)
    #+END_SRC

    - `defaults` provides a dictionary of hyper parameters used as default
      #+BEGIN_SRC python
        Optimizer(..., lr=0.6, mom=0.1, ...)

        # {**defaults} looks like
        {'lr':0.6, 'mom':0.1}
      #+END_SRC

    - `maybe_update` populates `defaults` with a default 
      value of a hyper parameter that is stored on the 
      stepper that uses the hyper parameter 
      (if the default is missing)

      e.g., 
      {'wd': 0.} is stored on `_defaults` of `weight_decay` or `l2_reg`

    - thanks to this, we do not have to think about 
      for which hyper parameters we have to provide default values
      looking through all the steppers.

    - instead, a stepper that requires a hyper parameter provides
      the default value by itself.

*** sgd_opt

    #+BEGIN_SRC python
      #export 
      sgd_opt = partial(Optimizer, steppers=[weight_decay, sgd_step])
    #+END_SRC

*** Stat
    #+BEGIN_SRC python
      #export
      class Stat():
          _defaults = {}
          def init_state(self, p): raise NotImplementedError
          def update(self, p, state, **kwargs): raise NotImplementedError    
    #+END_SRC

*** AverageGrad ver1

    #+BEGIN_SRC python
      class AverageGrad(Stat):
          _defaults = dict(mom=0.9)

          def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}
          def update(self, p, state, mom, **kwargs):
              state['grad_avg'].mul_(mom).add_(p.grad.data)
              return state
    #+END_SRC

    - `torch.zeros_like` returns a tensor, filled with 0, 
      of the same size as the passed argument
      https://pytorch.org/docs/master/generated/torch.zeros_like.html

    - why use p.grad.data
      https://discuss.pytorch.org/t/layer-weight-vs-weight-data/24271

*** momentum_step

    #+BEGIN_SRC python
      #export
      def momentum_step(p, lr, grad_avg, **kwargs):
          p.data.add_(-lr, grad_avg)
          return p
    #+END_SRC

*** StatefulOptimizer

    #+BEGIN_SRC python
      #export
      class StatefulOptimizer(Optimizer):
          def __init__(self, params, steppers, stats=None, **defaults): 
              self.stats = listify(stats)
              maybe_update(self.stats, defaults, get_defaults)
              super().__init__(params, steppers, **defaults)
              self.state = {}
              
          def step(self):
              for p,hyper in self.grad_params():
                  if p not in self.state:
                      #Create a state for p and call all the statistics to initialize it.
                      self.state[p] = {}
                      maybe_update(self.stats, self.state[p], lambda o: o.init_state(p))
                  state = self.state[p]
                  for stat in self.stats: state = stat.update(p, state, **hyper)
                  compose(p, self.steppers, **state, **hyper)
                  self.state[p] = state
                 
    #+END_SRC

    - a single stat in `stats` are, for example, `AverageGrad()`.

    - stat is used for each single parameter to create some statistics
      for it.

    - `super().__init__(params, steppers, **defaults)` does
      `__init__` of Optimizer ver3 which is to set
      `self.param_groups`, `self.hypers` and `self.steppers`

    - for each single parameter such as 
      `W1`(the weight tensor of the 1st layer),
      we register a state on `self.state[p]` based on 
      some statistics of a property of the parameter,
      such as "moving average" of property `grad` 

      - self.state[p] looks like, for example
        {'grad_avg'}

      - note `p.__repr__` is called when using `p` as an indexing key.

    - for better readability, `self.state[p]` is stored on `state`
      inside `step`

    - for each step, `self.state[p]` (=`state`) is updated by `stat.update`.
      #+BEGIN_SRC python
        for stat in self.stats: state = stat.update(p, state, **hyper)
      #+END_SRC

    - for example, below is `update` of `AverageGrad`

      #+BEGIN_SRC python
        def update(self, p, state, mom, **kwargs):
            state['grad_avg'].mul_(mom).add_(p.grad.data)
            return state
      #+END_SRC

    - and then `p` is updated by `self.steppers` utilizing 
      `self.state[p]` (=`state`)

      #+BEGIN_SRC python
        compose(p, self.steppers, **state, **hyper)
      #+END_SRC

    - for example, `momentum_step` utilizes `grad_avg` in `state`

      #+BEGIN_SRC python
        #export
        def momentum_step(p, lr, grad_avg, **kwargs):
            p.data.add_(-lr, grad_avg)
            return p
      #+END_SRC

*** how to use StatefulOptimizer

    #+BEGIN_SRC python
      # how to use
      sgd_mom_opt = partial(StatefulOptimizer, steppers=[momentum_step,weight_decay],
                        stats=AverageGrad(), wd=0.01)
    #+END_SRC

*** TODO L2 Regularrization vs BatchNorm
    - Suppose we set a very large weight decay of 1e6.
      Now doing following gives us the same activation, making the weight penalty term negligible
      which is calculated by `wd * Σw^2`
      - set a very large number around 1e6 for parameter 'multi' for BatchNorm layer.
      - devide all the weight parameter by 1e6

    - ref
      https://vitalab.github.io/article/2020/01/24/L2-reg-vs-BN.html
      https://blog.janestreet.com/l2-regularization-and-batch-norm/
      https://www.semion.io/doc/l2-regularization-versus-batch-and-weight-normalization

*** plot_mom

    #+BEGIN_SRC python
      x = torch.linspace(-4, 4, 200)
      y = torch.randn(200) + 0.3
      betas = [0.5, 0.7, 0.9, 0.99]

      def plot_mom(f):
          _,axs = plt.subplots(2,2, figsize=(12,8))
          for beta,ax in zip(betas, axs.flatten()):
              ax.plot(y, linestyle='None', marker='.')
              avg,res = None,[]
              for i,yi in enumerate(y):
                  avg,p = f(avg, beta, yi, i)
                  res.append(p)
              ax.plot(res, color='red')
              ax.set_title(f'beta={beta}')
    #+END_SRC

    - `y` is a list of 200 random numbers with (mean, std) = (0.3, 1)
    - `plot_mom` uses only `y`
    - `f` is a function which 
      - calculates the next value of `avg` based on the current value of `avg`
      - calculates some value `p` based on `avg` & `yi`
      - when `f` is `mom1`, `avg` and `p` are same 

*** mom1

    #+BEGIN_SRC python
      def mom1(avg, beta, yi, i): 
          if avg is None: avg=yi
          res = beta*avg + yi
          return res,res
      plot_mom(mom1)

    #+END_SRC

*** lin_comb

    #+BEGIN_SRC python
      #export
      def lin_comb(v1, v2, beta): return beta*v1 + (1-beta)*v2
    #+END_SRC

*** mom2

    #+BEGIN_SRC python
      def mom2(avg, beta, yi, i):
          if avg is None: avg=yi
          avg = lin_comb(avg, yi, beta)
          return avg, avg
      plot_mom(mom2)
    #+END_SRC

    - note that `avg` = y0, and "avg0" will be equal to "y0"
      avg0 = beta * avg + (1-beta) * y0
           = beta * y0 + (1-beta) * y0
           = y0

*** mom3

    #+BEGIN_SRC python
      def mom3(avg, beta, yi, i):
          if avg is None: avg=0
          avg = lin_comb(avg, yi, beta)
          return avg, avg/(1-beta**(i+1))
      plot_mom(mom3)
    #+END_SRC

    - mom3 is "debiased exponentially weighted moving average"
    - `(1-beta**(i+1))` is the correction term

    - note that different from mom2, `avg` is set to 0 instead of y0,
      and "avg0" will be equal to (1-beta) * y0, and
      debiased "avg0" will be "y0"

    - avg0 = beta*avg + (1-beta) * y0
           = (1-beta) * y0

    - debiased avg0 = (1-beta) * y0 / (1-beta)^(0+1)
                    = y0

    - note that 
      - debiased exponentially weighted moving average"
        still uses `avg` to calculates the next value of `avg`

      - however, what we plot in the end is the debiased version of `avg`, 
        and so `avg` is just a "by-product"
      
*** Adam
    - Adam is

      (dampened debiased momentum)
      ---------------------------------------------
      (dampened debiased root sum of square gradient)

*** AverageGrad ver2

    #+BEGIN_SRC python
      #export
      class AverageGrad(Stat):
          _defaults = dict(mom=0.9)
          
          def __init__(self, dampening:bool=False): self.dampening=dampening
          def init_state(self, p): return {'grad_avg': torch.zeros_like(p.grad.data)}
          def update(self, p, state, mom, **kwargs):
              state['mom_damp'] = 1-mom if self.dampening else 1.
              state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data)
              return state
    #+END_SRC

    - note that state['grad_avg'] is NOT a scalar, 
    but a tensor which looks like

    #+BEGIN_SRC python
      [[g11, g12, g13]
       [g21, g22, g23],
       [g31, g32, g33]]

    #+END_SRC

    - `AverageGrad` ver2 is almost same as `AverageGrad` ver1
      except `AverageGrad` ver2 has the dampening term

    - keep in mind that `AverageGrad` is just another
      `State` class which calculates some statistics
      for each weight parameter based on some 
      property (e.g. grad) of the parameter

    - `AverageGrad` calculates damped/not-damped
      exponentially weighted moving average

*** AverageSqrGrad

    #+BEGIN_SRC python
      #export
      class AverageSqrGrad(Stat):
          _defaults = dict(sqr_mom=0.99)
          
          def __init__(self, dampening:bool=True): self.dampening=dampening
          def init_state(self, p): return {'sqr_avg': torch.zeros_like(p.grad.data)}
          def update(self, p, state, sqr_mom, **kwargs):
              state['sqr_damp'] = 1-sqr_mom if self.dampening else 1.
              state['sqr_avg'].mul_(sqr_mom).addcmul_(state['sqr_damp'], p.grad.data, p.grad.data)
              return state
    #+END_SRC

    - note that, same as 'grad_avg', 'sqr_avg' is NOT a scalar,
      but a tensor which looks like

      #+BEGIN_SRC python
        [[s11, s12, s13]
         [s21, s22, s23],
         [s31, s32, s33]]

       #+END_SRC

    - `addcmul`
      https://pytorch.org/docs/master/generated/torch.addcmul.html
      - multiply, in  ELEMENT-WISE, `p.grad.data` and  `p.grad.data` element-wise
      - mulitply the result by `state['sqr_mom']`

*** StepCount

    #+BEGIN_SRC python
      #export
      class StepCount(Stat):
          def init_state(self, p): return {'step': 0}
          def update(self, p, state, **kwargs):
              state['step'] += 1
              return state
    #+END_SRC

*** debias

    #+BEGIN_SRC python
      #export
      def debias(mom, damp, step): return damp * (1 - mom**step) / (1-mom)
    #+END_SRC

    - keep in mind that dampening and debiasing are two 
      different things, so it is possible to apply debiasing
      to no-dampening momentum.

    - `debias` function covers both cases below
      - with dampening    : damp = (1-mom), so debias is (1 - mom**step)
      - without dampening : damp=1, so debias is (1 - mom**step) / (1-mom)

    - inside `StatefulOptimizer`, when the first time `step` is called, 
      `stat.update` is called before any steppers.

    - especially, `StepCount` is called to increment `step` by 1.

    - so by the time a stepper that refers to `step`
      (e.g. adam_stepper) is called for the first time,
      `step` is already 1.

    - for step=1 to calculate EWMA of grad,
      avg1 = beta*avg0 + (1-beta)*grad1, 
      where avg0(=initial value of grad_avg') is defined to be zero

      debiasing term is defined to be 1-beta^i according to 
      Andrew Ng's lecture
      https://www.youtube.com/watch?v=lWzo8CajF5s

      # this means, exponentially weighted moving averate is now indexed
      # such that the index starts from 1, and the very first term is avg1,
      # and so the equation for debiasing now becomes 1-beta^step
      # intead of 1-beta^(step+1)
      
      avg0 = beta*grad_avg + (1-beta)*grad
      = (1-beta) * grad

      debiased avg0 = avg0/(1-beta)^(0+1) = avg0/(1-beta)^step

      so, debias function should contain `1 - mom**step`
      instead of `1-mom**(step+1)`

*** adam_step

    #+BEGIN_SRC python
      #export
      def adam_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, **kwargs):
          debias1 = debias(mom,     mom_damp, step)
          debias2 = debias(sqr_mom, sqr_damp, step)
          p.data.addcdiv_(-lr / debias1, grad_avg, (sqr_avg/debias2).sqrt() + eps)
          return p
      adam_step._defaults = dict(eps=1e-5)
    #+END_SRC
    
    - note again that `grad_avg` and `sqr_avg` are tensor, not a scalar,
      so each single element of p (= Wij of a weight tensor W )
      is updated according to
      each single element of tensors `grad_avg`, `sqr_avg`
      (= Gij of a gradient tensor G, Sij of a gradient square tensor S)

    - addcdiv
      https://pytorch.org/docs/master/generated/torch.addcdiv.html
      - devide `grad_avg` by `(sqr_avg/debias2).sqrt() + eps)`
      - multiply by `-lr / debias1`

*** adap_opt

    #+BEGIN_SRC python
      #export
      def adam_opt(xtra_step=None, **kwargs):
          return partial(StatefulOptimizer, steppers=[adam_step,weight_decay]+listify(xtra_step),
                         stats=[AverageGrad(dampening=True), AverageSqrGrad(), StepCount()], **kwargs)
    #+END_SRC

*** TODO lamb_step

    #+BEGIN_SRC python
      def lamb_step(p, lr, mom, mom_damp, step, sqr_mom, sqr_damp, grad_avg, sqr_avg, eps, wd, **kwargs):
          debias1 = debias(mom,     mom_damp, step)
          debias2 = debias(sqr_mom, sqr_damp, step)
          r1 = p.data.pow(2).mean().sqrt()
          step = (grad_avg/debias1) / ((sqr_avg/debias2).sqrt()+eps) + wd*p.data
          r2 = step.pow(2).mean().sqrt()
          p.data.add_(-lr * min(r1/r2,10), step)
          return p
      lamb_step._defaults = dict(eps=1e-6, wd=0.)
    #+END_SRC

    - note that r1, r2 are scalar, NOT a tensor

    - different from adam_step, we normalize the amount of an update
      for a single step, using scalars r1, r2, each of which is
      an average of some value over all the element
      of a parameter (= a weight tensor)

    - ⊙ which appears in the algorithm of LAMB in 09_optimizers.ipynb
      means element-wise multiplication

    - [ ] what is the idea behind the algorithm?

      https://towardsdatascience.com/an-intuitive-understanding-of-the-lamb-optimizer-46f8c0ae4866
      https://arxiv.org/pdf/1904.00962.pdf

*** the right value for `eps` of ADAM
    - 1:44:00

** 09b_learner.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
    #+END_SRC

*** Learner ver2 (= Runner ver3)
   #+BEGIN_SRC python
     #export
     def param_getter(m): return m.parameters()

     class Learner():
         def __init__(self, model, data, loss_func, opt_func=sgd_opt, lr=1e-2, splitter=param_getter,
                      cbs=None, cb_funcs=None):
             self.model,self.data,self.loss_func,self.opt_func,self.lr,self.splitter = model,data,loss_func,opt_func,lr,splitter
             self.in_train,self.logger,self.opt = False,print,None
             
             # NB: Things marked "NEW" are covered in lesson 12
             # NEW: avoid need for set_runner
             self.cbs = []
             self.add_cb(TrainEvalCallback())
             self.add_cbs(cbs)
             self.add_cbs(cbf() for cbf in listify(cb_funcs))

         def add_cbs(self, cbs):
             for cb in listify(cbs): self.add_cb(cb)
                 
         def add_cb(self, cb):
             cb.set_runner(self)
             setattr(self, cb.name, cb)
             self.cbs.append(cb)

         def remove_cbs(self, cbs):
             for cb in listify(cbs): self.cbs.remove(cb)
                 
         def one_batch(self, i, xb, yb):
             try:
                 self.iter = i
                 self.xb,self.yb = xb,yb;                        self('begin_batch')
                 self.pred = self.model(self.xb);                self('after_pred')
                 self.loss = self.loss_func(self.pred, self.yb); self('after_loss')
                 if not self.in_train: return
                 self.loss.backward();                           self('after_backward')
                 self.opt.step();                                self('after_step')
                 self.opt.zero_grad()
             except CancelBatchException:                        self('after_cancel_batch')
             finally:                                            self('after_batch')

         def all_batches(self):
             self.iters = len(self.dl)
             try:
                 for i,(xb,yb) in enumerate(self.dl): self.one_batch(i, xb, yb)
             except CancelEpochException: self('after_cancel_epoch')

         def do_begin_fit(self, epochs):
             self.epochs,self.loss = epochs,tensor(0.)
             self('begin_fit')

         def do_begin_epoch(self, epoch):
             self.epoch,self.dl = epoch,self.data.train_dl
             return self('begin_epoch')

         def fit(self, epochs, cbs=None, reset_opt=False):
             # NEW: pass callbacks to fit() and have them removed when done
             self.add_cbs(cbs)
             # NEW: create optimizer on fit(), optionally replacing existing
             if reset_opt or not self.opt: self.opt = self.opt_func(self.splitter(self.model), lr=self.lr)
                 
             try:
                 self.do_begin_fit(epochs)
                 for epoch in range(epochs):
                     if not self.do_begin_epoch(epoch): self.all_batches()

                     with torch.no_grad(): 
                         self.dl = self.data.valid_dl
                         if not self('begin_validate'): self.all_batches()
                     self('after_epoch')
                 
             except CancelTrainException: self('after_cancel_train')
             finally:
                 self('after_fit')
                 self.remove_cbs(cbs)

         ALL_CBS = {'begin_batch', 'after_pred', 'after_loss', 'after_backward', 'after_step',
             'after_cancel_batch', 'after_batch', 'after_cancel_epoch', 'begin_fit',
             'begin_epoch', 'begin_validate', 'after_epoch',
             'after_cancel_train', 'after_fit'}
         
         def __call__(self, cb_name):
             res = False
             assert cb_name in self.ALL_CBS
             for cb in sorted(self.cbs, key=lambda x: x._order): res = cb(cb_name) and res
             return res
   #+END_SRC
    - note that callack instances are registered as propeties of `Runner` instance    
      by `setattr` inside `add_cb`

      #+BEGIN_SRC python
        cb.set_runner(self)
        setattr(self, cb.name, cb)
        self.cbs.append(cb)
      #+END_SRC

    - `Leaner` ver2 is a slightly different verwion of `Runner` ver2 with 
      every property of `Learner` ver1 moved into `Runner` ver2

      #+BEGIN_SRC python
        class Learner():
            def __init__(self, model, opt, loss_func, data):
                self.model,self.opt,self.loss_func,self.data = model,opt,loss_func,data
      #+END_SRC

    - so, properties `model`, `loss_func`, `data` 
      defined with `@property` becomes unnecessary and are removed.

    - also, in `Runner` ver2, we used to call `set_runner` in `fit`
      to set on each callback a reference to `Runner` object,
      whereas in `Learner` ver2 (=`Runner` ver3), that part is 
      factored out as `add_cbs` & `add_cb` which are called
      in two places in `__init__` and `fit`

    - in `Runner` ver2, we can pass callbacks only to `__init__`,
      whereas with `Learner` ver2 (=`Runner` ver3), 
      there is a chance to pass callbacks when calling `fit`

    - in `Runner` ver2, optimizer object is stored in `Learner`
      object which is passed on calling `fit`,
      whereas in `Learner` ver2 (=`Runner` ver3), 
      a CONSTRUCTOR to create optimizer is passed on calling `__init__`
      and an optimizer object is created and set to `self.opt`
      on the fly in `fit`

    - note that the default for `splitter` is `param_getter`
      #+BEGIN_SRC python
        def param_getter(m): return m.parameters()
      #+END_SRC

*** get_learner ver1

    #+BEGIN_SRC python
      #export
      def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,
                      cb_funcs=None, opt_func=sgd_opt, **kwargs):
          model = get_cnn_model(data, nfs, layer, **kwargs)
          init_cnn(model)
          return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)
    #+END_SRC

** 09c_add_progress_bar.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
    #+END_SRC

*** AvgStatsCallback ver3

    #+BEGIN_SRC python
      # export 
      class AvgStatsCallback(Callback):
          def __init__(self, metrics):
              self.train_stats,self.valid_stats = AvgStats(metrics,True),AvgStats(metrics,False)
          
          def begin_fit(self):
              met_names = ['loss'] + [m.__name__ for m in self.train_stats.metrics]
              names = ['epoch'] + [f'train_{n}' for n in met_names] + [
                  f'valid_{n}' for n in met_names] + ['time']
              self.logger(names)
          
          def begin_epoch(self):
              self.train_stats.reset()
              self.valid_stats.reset()
              self.start_time = time.time()
              
          def after_loss(self):
              stats = self.train_stats if self.in_train else self.valid_stats
              with torch.no_grad(): stats.accumulate(self.run)
          
          def after_epoch(self):
              stats = [str(self.epoch)] 
              for o in [self.train_stats, self.valid_stats]:
                  stats += [f'{v:.6f}' for v in o.avg_stats] 
              stats += [format_time(time.time() - self.start_time)]
              self.logger(stats)
    #+END_SRC

*** ProgressCallback

    #+BEGIN_SRC python
      # export 
      class ProgressCallback(Callback):
          _order=-1
          def begin_fit(self):
              self.mbar = master_bar(range(self.epochs))
              self.mbar.on_iter_begin()
              self.run.logger = partial(self.mbar.write, table=True)
              
          def after_fit(self): self.mbar.on_iter_end()
          def after_batch(self): self.pb.update(self.iter)
          def begin_epoch   (self): self.set_pb()
          def begin_validate(self): self.set_pb()
              
          def set_pb(self):
              self.pb = progress_bar(self.dl, parent=self.mbar)
              self.mbar.update(self.epoch)
    #+END_SRC

** 10_augmentation.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
    #+END_SRC

*** do augmentation in byte, not in float
    - 1:58:30
    - should do a augmentation by flip while a data is still a byte,
      hence the transformation function should be given an `_order` property higher than
      that of `to_byte_tensor`,  `to_float_tensor` (20, and 30 respectively)

*** get_il

    #+BEGIN_SRC python
      def get_il(tfms): return ImageList.from_files(path, tfms=tfms)
    #+END_SRC

*** pil_random_flip

    #+BEGIN_SRC python
      def pil_random_flip(x):
          return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<0.5 else x
    #+END_SRC

*** show_image, show_batch

    #+BEGIN_SRC python
      #export
      def show_image(im, ax=None, figsize=(3,3)):
          if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize)
          ax.axis('off')
          ax.imshow(im.permute(1,2,0))

      def show_batch(x, c=4, r=None, figsize=None):
          n = len(x)
          if r is None: r = int(math.ceil(n/c))
          if figsize is None: figsize=(c*3,r*3)
          fig,axes = plt.subplots(r,c, figsize=figsize)
          for xi,ax in zip(x,axes.flat): show_image(xi, ax)
    #+END_SRC

*** PilRandomFlip

    #+BEGIN_SRC python
      class PilRandomFlip(Transform):
          _order=11
          def __init__(self, p=0.5): self.p=p
          def __call__(self, x):
              return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<self.p else x
    #+END_SRC

    - flipping is a kind of transforamtion that does not break
      a fine feature of a image when applied even while a image is 
      still a byte, so we set `_order` to 11 so that it will be
      applied before `to_byte_tensor` or `to_byte_tensor` are applied

*** PilTransform, PilRandomFlip

    #+BEGIN_SRC python
      #export
      class PilTransform(Transform): _order=11

      class PilRandomFlip(PilTransform):
          def __init__(self, p=0.5): self.p=p
          def __call__(self, x):
              return x.transpose(PIL.Image.FLIP_LEFT_RIGHT) if random.random()<self.p else x
    #+END_SRC

*** PilRandomDihedral

    #+BEGIN_SRC python
      #export
      class PilRandomDihedral(PilTransform):
          def __init__(self, p=0.75): self.p=p*7/8 #Little hack to get the 1/8 identity dihedral transform taken into account.
          def __call__(self, x):
              if random.random()>self.p: return x
              return x.transpose(random.randint(0,6))
    #+END_SRC

*** do several image data augumentations at once
    - 2:04:00

      #+BEGIN_SRC python
        %timeit -n 10 img.crop(cnr2).resize((128,128), resample=resample)

        img.transform((128,128), PIL.Image.EXTENT, cnr2, resample=resample)
      #+END_SRC

    - Although it is faster to do transformation while a
      image is still a byte, for destuctive transformation
      like cropping (or especially multiple destuctive transformations)
      wait it until the data becomes float since otherwise it 
      would break a fine feature (such as check pattern of a shirt) of a image

    - on transformation, byte rounds off and disappear will saturate,
      and so destuctive transformation on byte looses some feature

    - on the other hand, float does not disappear, so transformation
      on float does not loose a fine feature of a image 

    - if destuctive transformations are really necessary on byte, 
      at least do it at one go.

*** thinking about time budget
    - 2:06:00

*** GeneralCrop

    #+BEGIN_SRC python
      #export
      from random import randint

      def process_sz(sz):
          sz = listify(sz)
          return tuple(sz if len(sz)==2 else [sz[0],sz[0]])

      def default_crop_size(w,h): return [w,w] if w < h else [h,h]

      class GeneralCrop(PilTransform):
          def __init__(self, size, crop_size=None, resample=PIL.Image.BILINEAR): 
              self.resample,self.size = resample,process_sz(size)
              self.crop_size = None if crop_size is None else process_sz(crop_size)
              
          def default_crop_size(self, w,h): return default_crop_size(w,h)

          def __call__(self, x):
              csize = self.default_crop_size(*x.size) if self.crop_size is None else self.crop_size
              return x.transform(self.size, PIL.Image.EXTENT, self.get_corners(*x.size, *csize), resample=self.resample)
          
          def get_corners(self, w, h): return (0,0,w,h)

      class CenterCrop(GeneralCrop):
          def __init__(self, size, scale=1.14, resample=PIL.Image.BILINEAR):
              super().__init__(size, resample=resample)
              self.scale = scale
              
          def default_crop_size(self, w,h): return [w/self.scale,h/self.scale]
          
          def get_corners(self, w, h, wc, hc):
              return ((w-wc)//2, (h-hc)//2, (w-wc)//2+wc, (h-hc)//2+hc)
    #+END_SRC

    - `GeneralCrop` inherits `PilTransform`, so it has `_order`
      set to 11

      #+BEGIN_SRC python
        class PilTransform(Transform): _order=11
      #+END_SRC

*** RandomResizedCrop

    #+BEGIN_SRC python
      # export
      class RandomResizedCrop(GeneralCrop):
          def __init__(self, size, scale=(0.08,1.0), ratio=(3./4., 4./3.), resample=PIL.Image.BILINEAR):
              super().__init__(size, resample=resample)
              self.scale,self.ratio = scale,ratio
          
          def get_corners(self, w, h, wc, hc):
              area = w*h
              #Tries 10 times to get a proper crop inside the image.
              for attempt in range(10):
                  area = random.uniform(*self.scale) * area
                  ratio = math.exp(random.uniform(math.log(self.ratio[0]), math.log(self.ratio[1])))
                  new_w = int(round(math.sqrt(area * ratio)))
                  new_h = int(round(math.sqrt(area / ratio)))
                  if new_w <= w and new_h <= h:
                      left = random.randint(0, w - new_w)
                      top  = random.randint(0, h - new_h)
                      return (left, top, left + new_w, top + new_h)
              
              # Fallback to squish
              if   w/h < self.ratio[0]: size = (w, int(w/self.ratio[0]))
              elif w/h > self.ratio[1]: size = (int(h*self.ratio[1]), h)
              else:                     size = (w, h)
              return ((w-size[0])//2, (h-size[1])//2, (w+size[0])//2, (h+size[1])//2)
    #+END_SRC

*** apply RandomResizedCrop for NLP
    - 2:07:10

*** warping > find_coeffs

    #+BEGIN_SRC python

      # export
      from torch import FloatTensor,LongTensor

      def find_coeffs(orig_pts, targ_pts):
          matrix = []
          #The equations we'll need to solve.
          for p1, p2 in zip(targ_pts, orig_pts):
              matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])
              matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])

          A = FloatTensor(matrix)
          B = FloatTensor(orig_pts).view(8, 1)
          #The 8 scalars we seek are solution of AX = B
          return list(torch.solve(B,A)[0][:,0])
    #+END_SRC

*** warping > warp

    #+BEGIN_SRC python
      # export
      def warp(img, size, src_coords, resample=PIL.Image.BILINEAR):
          w,h = size
          targ_coords = ((0,0),(0,h),(w,h),(w,0))
          c = find_coeffs(src_coords,targ_coords)
          res = img.transform(size, PIL.Image.PERSPECTIVE, list(c), resample=resample)
          return res
    #+END_SRC

*** warping > PilTiltRandomCrop ver1

    #+BEGIN_SRC python
      class PilTiltRandomCrop(PilTransform):
          def __init__(self, size, crop_size=None, magnitude=0., resample=PIL.Image.NEAREST): 
              self.resample,self.size,self.magnitude = resample,process_sz(size),magnitude
              self.crop_size = None if crop_size is None else process_sz(crop_size)
              
          def __call__(self, x):
              csize = default_crop_size(*x.size) if self.crop_size is None else self.crop_size
              up_t,lr_t = uniform(-self.magnitude, self.magnitude),uniform(-self.magnitude, self.magnitude)
              left,top = randint(0,x.size[0]-csize[0]),randint(0,x.size[1]-csize[1])
              src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]])
              src_corners = src_corners * tensor(csize).float() + tensor([left,top]).float()
              src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners])
              return warp(x, self.size, src_corners, resample=self.resample)
    #+END_SRC

*** warping > PilTiltRandomCrop ver2

    #+BEGIN_SRC python
      # export
      class PilTiltRandomCrop(PilTransform):
          def __init__(self, size, crop_size=None, magnitude=0., resample=PIL.Image.BILINEAR): 
              self.resample,self.size,self.magnitude = resample,process_sz(size),magnitude
              self.crop_size = None if crop_size is None else process_sz(crop_size)
              
          def __call__(self, x):
              csize = default_crop_size(*x.size) if self.crop_size is None else self.crop_size
              left,top = randint(0,x.size[0]-csize[0]),randint(0,x.size[1]-csize[1])
              top_magn = min(self.magnitude, left/csize[0], (x.size[0]-left)/csize[0]-1)
              lr_magn  = min(self.magnitude, top /csize[1], (x.size[1]-top) /csize[1]-1)
              up_t,lr_t = uniform(-top_magn, top_magn),uniform(-lr_magn, lr_magn)
              src_corners = tensor([[-up_t, -lr_t], [up_t, 1+lr_t], [1-up_t, 1-lr_t], [1+up_t, lr_t]])
              src_corners = src_corners * tensor(csize).float() + tensor([left,top]).float()
              src_corners = tuple([(int(o[0].item()), int(o[1].item())) for o in src_corners])
              return warp(x, self.size, src_corners, resample=self.resample)
    #+END_SRC


*** faster conversion to float_tensor

    #+BEGIN_SRC python
      #export
      import numpy as np

      def np_to_float(x): return torch.from_numpy(np.array(x, dtype=np.float32, copy=False)).permute(2,0,1).contiguous()/255.
      np_to_float._order = 30
    #+END_SRC
    
    - this converts numpy array to float_tensor directly and
      much faster than applying `to_byte_tensor` and `to_float_tensor`
      successively

*** data augumentation for batch
    - [ ] in order to perform transformations on GPU, 
      that is, after calling x.cuda(), we would like to
      have a batch of image without applying any transform yet.

    - For that purpose, we need to store images in `DataSet` 
      instead of `ImageItemList` when passing to `DataLoader` object
      since otherwise `ImageItemList` would do transform on
      creating a batch before we move a batch to GPU

    - fastai v1 is not equipped with data augumentation for batch

* lesson12
** 10b_mixup_label_smoothing.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
      !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb
      !python notebook2script.py 10_augmentation.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
      !cp /content/exp/nb_10c.py /content/exp.nb_10c.py
    #+END_SRC

*** loss function for mixup
    - the loss function for mixup is defined to be just a cross-entropy loss
      where target is in the form of (0.7, 0.3, 0, 0, 0).

    - the cross-entropy loss is just a linear combination of
      cross-entropy loss for label1 and cross-entropy loss for label2
      so can be writen as 

      #+BEGIN_SRC python
        loss(output, new_target) = t * loss(output, target1) + (1-t) * loss(output, target2)
      #+END_SRC

    - why does cross-entropy loss make sense for mixup?
    - 
    - for simplicity, think of a case where there are only 2 labels (such as dog or cat), and t = 0.7

    - keep in mind that cross-entropy loss is calculated for a batch of image

    - for a specific mixedup image, cross entropy loss is 0.7*(-logP1) + 0.3*(-logP2) where P1+P2=1

    - when will it be smallest?

    - naive guess is that we may choose a value close to 1 for P1, since it makes 0.7*(-logP1) small
      but since P1+P2 must add up to 1, P1 getting closer to 1 means P2 getting closer to 0, which
      makes (-logP2) goes to inifnity!

    - P1 should be not too close to 1, but somewhat bigger than P2, 
      reflecting how much the coefficient of "-logP1" (0.7) is
      bigger than that of "-logP2" (0.3)

    - this means, the neural net will be trained to predict label(i)-ishness properly

    - _old

    # - to calculate the loss for the mixuped image,
    #   we use TWO target for ONE mixedup image,
    #   and calculate a cross-entropy loss for each of the two targets
    #   - each of the targets is one-hot encoding targets

    # - in English, let's say the new target is consisted of target1 and target2
    #   - firstly we pretend the new target is just a target1 and compute loss
    #     but because the new target has just "t" of target1-ness,
    #     we scale down the loss by factor of "t"

    #   - then, we pretend the new target is just a target2 and compute loss,
    #     but because the new target has just "1-t" of target2-ness,
    #     we scale down the loss by factor of "1-t"

*** gamma 

    #+BEGIN_SRC python
      # PyTorch has a log-gamma but not a gamma, so we'll create one
      Γ = lambda x: x.lgamma().exp()
    #+END_SRC

*** NoneReduce

    #+BEGIN_SRC python
      #export
      class NoneReduce():
          def __init__(self, loss_func): 
              self.loss_func,self.old_red = loss_func,None
              
          def __enter__(self):
              if hasattr(self.loss_func, 'reduction'):
                  self.old_red = getattr(self.loss_func, 'reduction')
                  setattr(self.loss_func, 'reduction', 'none')
                  return self.loss_func
              else: return partial(self.loss_func, reduction='none')
              
          def __exit__(self, type, value, traceback):
              if self.old_red is not None: setattr(self.loss_func, 'reduction', self.old_red)    
    #+END_SRC

*** unsqueeze, reduce_loss

    #+BEGIN_SRC python
      #export
      from torch.distributions.beta import Beta

      def unsqueeze(input, dims):
          for dim in listify(dims): input = torch.unsqueeze(input, dim)
          return input

      def reduce_loss(loss, reduction='mean'):
          return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss    
    #+END_SRC

    - reduce loss 

*** MixUp

    #+BEGIN_SRC python
      #export
      class MixUp(Callback):
          _order = 90 #Runs after normalization and cuda
          def __init__(self, α:float=0.4): self.distrib = Beta(tensor([α]), tensor([α]))
          
          def begin_fit(self): self.old_loss_func,self.run.loss_func = self.run.loss_func,self.loss_func
          
          def begin_batch(self):
              if not self.in_train: return #Only mixup things during training
              λ = self.distrib.sample((self.yb.size(0),)).squeeze().to(self.xb.device)
              λ = torch.stack([λ, 1-λ], 1)
              self.λ = unsqueeze(λ.max(1)[0], (1,2,3))
              shuffle = torch.randperm(self.yb.size(0)).to(self.xb.device)
              xb1,self.yb1 = self.xb[shuffle],self.yb[shuffle]
              self.run.xb = lin_comb(self.xb, xb1, self.λ)
              
          def after_fit(self): self.run.loss_func = self.old_loss_func
          
          def loss_func(self, pred, yb):
              if not self.in_train: return self.old_loss_func(pred, yb)
              with NoneReduce(self.old_loss_func) as loss_func:
                  loss1 = loss_func(pred, yb)
                  loss2 = loss_func(pred, self.yb1)
              loss = lin_comb(loss1, loss2, self.λ)
              return reduce_loss(loss, getattr(self.old_loss_func, 'reduction', 'mean'))
    #+END_SRC

    - define 'base' and 'mixing pair' to be the image corresponding to
      x1 and x2 respectively where
      x(mixup) = t * x1 + (1-t) * x2

    - Although `Runner` ver3 is actually called `Learner`, 
      `Callback` ver3 still uses term `run` in `self.run` or
      `set_runner`.

    - for a batch of 'base' images, a batch of 'mixing pair' images
      is created just by randomly permiting the batch of 'base' images

    - `self.yb` is a batch of the labels for the corresponding
      batch of `mixing-pair` images

    - `self.run.xb = lin_comb(self.xb, xb1, self.λ)` sets
      a batch of mixed up images to `xb` property  of the `Learner` instance

    - in `loss_func`, 
      - `yb` is a batch of the labels for the corresponding 
        batch of 'base' images

      - `self.yb1` is a batch of the labels for the corresponding
        batch of `mixing pair` images

      - a loss function such as F.cross_entropy returns 
        the reduced version of the rank1 tensor consisting 
        of losses for a batch in the form of either sum or mean.

      - However, while calculating the loss for each of
        'base', or 'mixing pair' to calculate the loss
        for mixedup image, we would like to keep the result
        before reduction.

      - In order to do that, we apply a context manager 
        NoneReduce to self.old_loss_func (= F.cross_entropy ), 
        which sets to self.loss_func  the non-reduction
        version of self.old_loss_func

      - in the end of `MixUp::loss_func`, we reduce the 
        rank1 tensor for the loss for a batch of mixedup images
        by `reduce_loss` and return.

*** get_learner ver2

    #+BEGIN_SRC python
      def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,
                      cb_funcs=None, opt_func=optim.SGD, **kwargs):
          model = get_cnn_model(data, nfs, layer, **kwargs)
          init_cnn(model)
          return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)
    #+END_SRC

*** what is Label smoothing
    - label smoothing is technique to handle data with noisy label.
      with this technique, we can skip all the cleaning task

*** LabelSmoothingCrossEntropy

    #+BEGIN_SRC python
      #export
      class LabelSmoothingCrossEntropy(nn.Module):
          def __init__(self, ε:float=0.1, reduction='mean'):
              super().__init__()
              self.ε,self.reduction = ε,reduction
          
          def forward(self, output, target):
              c = output.size()[-1]
              log_preds = F.log_softmax(output, dim=-1)
              loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)
              nll = F.nll_loss(log_preds, target, reduction=self.reduction)
              return lin_comb(loss/c, nll, self.ε)
    #+END_SRC

    - `LabelSmoothing` works well with MixUp and substitutes
      the calculation of cross-entropy loss for the label of
      `base` iamge and label of `mixing-up` image.

    - the loss for LabelSmoothing is defined as
      𝑙𝑜𝑠𝑠=(1−ε)𝑐𝑒(𝑖)+ε∑𝑐𝑒(𝑗)/𝑁

    - the definition of `LabelSmoothing` is for a single image,
      so if we calculate it for a batch, we end up with
      rank1 tensor consisting of losses for each of the single images

    - however, `forward` needs to return some scalar value,
      so we reduce the rank1 tensor to a scalar by taking 
      the the average over the batch

    - since the loss for LabelSmoothing is a linear combination of
      two terms, it is ok to take the average over the batch
      independently and add them up together in the end.

    - `c` is number of the labels

    - `-log_preds` is a rank2 tensor.
      - each row corresponds to a single image in a batch
      - each row contains the cros entropies for corresponding labels

    - `loss` is sum of cross entropies
        
    - `log_preds.sum(dim=-1)` is a rank1 tensor
      with each row containing the sum of the cross-entropies 
      for the labels

    - `reduce_loss(-log_preds.sum(dim=-1), self.reduction)`
      takes the average (over a batch) of the sum of the
      cross-entropies, which corresponds to taking the average
      over a batch for the 2nd term of the definition of
      the loss for `LabelSmoothing`
      

    - `F.nll_loss(log_preds, target, reduction=self.reduction)` 
      calculates cross-entropy LOSS for each row, and then
      takes the average of the cross-entropy losses over the batch

    - 

** 10c_fp16.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # install apex
      !pip install git+https://github.com/NVIDIA/apex

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git



      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
      !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/
      !cp course-v3/nbs/dl2/10b_mixup_label_smoothing.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb
      !python notebook2script.py 10_augmentation.ipynb
      !python notebook2script.py 10b_mixup_label_smoothing.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
      !cp /content/exp/nb_10.py /content/exp.nb_10.py
      !cp /content/exp/nb_10b.py /content/exp.nb_10b.py
    #+END_SRC

*** bn_to_float

    #+BEGIN_SRC python
      def bn_to_float(model):
          if isinstance(model, bn_types): model.float()
          for child in model.children():  bn_to_float(child)
          return model
    #+END_SRC

*** model_to_half

    #+BEGIN_SRC python
      def model_to_half(model):
          model = model.half()
          return bn_to_float(model)
    #+END_SRC

*** check_weights

    #+BEGIN_SRC python
      def check_weights(model):
          for i,t in enumerate([torch.float16, torch.float32, torch.float16]):
              assert model[i].weight.dtype == t
              assert model[i].bias.dtype   == t
    #+END_SRC

*** get_master ver1

    #+BEGIN_SRC python
      from torch.nn.utils import parameters_to_vector

      def get_master(model, flat_master=False):
          model_params = [param for param in model.parameters() if param.requires_grad]
          if flat_master:
              master_param = parameters_to_vector([param.data.float() for param in model_params])
              master_param = torch.nn.Parameter(master_param, requires_grad=True)
              if master_param.grad is None: master_param.grad = master_param.new(*master_param.size())
              return model_params, [master_param]
          else:
              master_params = [param.clone().float().detach() for param in model_params]
              for param in master_params: param.requires_grad_(True)
              return model_params, master_params
    #+END_SRC
    
*** same_lists

    #+BEGIN_SRC python
      def same_lists(ps1, ps2):
          assert len(ps1) == len(ps2)
          for (p1,p2) in zip(ps1,ps2): 
              assert p1.requires_grad == p2.requires_grad
              assert torch.allclose(p1.data.float(), p2.data.float())
    #+END_SRC

*** get_master ver2

    #+BEGIN_SRC python
      def get_master(opt, flat_master=False):
          model_params = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]
          if flat_master:
              master_params = []
              for pg in model_params:
                  mp = parameters_to_vector([param.data.float() for param in pg])
                  mp = torch.nn.Parameter(mp, requires_grad=True)
                  if mp.grad is None: mp.grad = mp.new(*mp.size())
                  master_params.append(mp)
          else:
              master_params = [[param.clone().float().detach() for param in pg] for pg in model_params]
              for pg in master_params:
                  for param in pg: param.requires_grad_(True)
          return model_params, master_params
    #+END_SRC

*** to_master_grads ver1

    #+BEGIN_SRC python
      def to_master_grads(model_params, master_params, flat_master:bool=False)->None:
          if flat_master:
              if master_params[0].grad is None: master_params[0].grad = master_params[0].data.new(*master_params[0].data.size())
              master_params[0].grad.data.copy_(parameters_to_vector([p.grad.data.float() for p in model_params]))
          else:
              for model, master in zip(model_params, master_params):
                  if model.grad is not None:
                      if master.grad is None: master.grad = master.data.new(*master.data.size())
                      master.grad.data.copy_(model.grad.data)
                  else: master.grad = None
    #+END_SRC

*** check_grads
#+BEGIN_SRC python
  def check_grads(m1, m2):
      for p1,p2 in zip(m1,m2): 
          if p1.grad is None: assert p2.grad is None
          else: assert torch.allclose(p1.grad.data, p2.grad.data) 
#+END_SRC

*** to_model_params ver1
#+BEGIN_SRC python
  from torch._utils import _unflatten_dense_tensors

  def to_model_params(model_params, master_params, flat_master:bool=False)->None:
      if flat_master:
          for model, master in zip(model_params, _unflatten_dense_tensors(master_params[0].data, model_params)):
              model.data.copy_(master)
      else:
          for model, master in zip(model_params, master_params): model.data.copy_(master.data)
#+END_SRC

*** get_master ver3

    #+BEGIN_SRC python
      # export 
      def get_master(opt, flat_master=False):
          model_pgs = [[param for param in pg if param.requires_grad] for pg in opt.param_groups]
          if flat_master:
              master_pgs = []
              for pg in model_pgs:
                  mp = parameters_to_vector([param.data.float() for param in pg])
                  mp = torch.nn.Parameter(mp, requires_grad=True)
                  if mp.grad is None: mp.grad = mp.new(*mp.size())
                  master_pgs.append([mp])
          else:
              master_pgs = [[param.clone().float().detach() for param in pg] for pg in model_pgs]
              for pg in master_pgs:
                  for param in pg: param.requires_grad_(True)
          return model_pgs, master_pgs
    #+END_SRC


*** to_master_grads ver2

    #+BEGIN_SRC python
      # export 
      def to_master_grads(model_pgs, master_pgs, flat_master:bool=False)->None:
          for (model_params,master_params) in zip(model_pgs,master_pgs):
              fp16.model_grads_to_master_grads(model_params, master_params, flat_master=flat_master)
    #+END_SRC

*** to_model_params ver2

    #+BEGIN_SRC python
      # export 
      def to_model_params(model_pgs, master_pgs, flat_master:bool=False)->None:
          for (model_params,master_params) in zip(model_pgs,master_pgs):
              fp16.master_params_to_model_params(model_params, master_params, flat_master=flat_master)
    #+END_SRC

*** MixedPrecision ver1

    #+BEGIN_SRC python
      class MixedPrecision(Callback):
          _order = 99
          def __init__(self, loss_scale=512, flat_master=False):
              assert torch.backends.cudnn.enabled, "Mixed precision training requires cudnn."
              self.loss_scale,self.flat_master = loss_scale,flat_master

          def begin_fit(self):
              self.run.model = fp16.convert_network(self.model, dtype=torch.float16)
              self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master)
              #Changes the optimizer so that the optimization step is done in FP32.
              self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner.
              
          def after_fit(self): self.model.float()

          def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision
          def after_pred(self):  self.run.pred = self.run.pred.float() #Compute the loss in FP32
          def after_loss(self):  self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow

          def after_backward(self):
              #Copy the gradients to master and unscale
              to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)
              for master_params in self.master_pgs:
                  for param in master_params:
                      if param.grad is not None: param.grad.div_(self.loss_scale)

          def after_step(self):
              #Zero the gradients of the model since the optimizer is disconnected.
              self.model.zero_grad()
              #Update the params from master to model.
              to_model_params(self.model_pgs, self.master_pgs, self.flat_master)
    #+END_SRC

*** get_learner
#+BEGIN_SRC python
  def get_learner(nfs, data, lr, layer, loss_func=F.cross_entropy,
                  cb_funcs=None, opt_func=adam_opt(), **kwargs):
      model = get_cnn_model(data, nfs, layer, **kwargs)
      init_cnn(model)
      return Learner(model, data, loss_func, lr=lr, cb_funcs=cb_funcs, opt_func=opt_func)
#+END_SRC

*** test_overflow
#+BEGIN_SRC python
  # export 
  def test_overflow(x):
      s = float(x.float().sum())
      return (s == float('inf') or s == float('-inf') or s != s)
#+END_SRC

*** grad_overflow

    #+BEGIN_SRC python
      # export 
      def grad_overflow(param_groups):
          for group in param_groups:
              for p in group:
                  if p.grad is not None:
                      s = float(p.grad.data.float().sum())
                      if s == float('inf') or s == float('-inf') or s != s: return True
          return False
    #+END_SRC

*** MixedPrecision ver2

    #+BEGIN_SRC python
      # export 
      class MixedPrecision(Callback):
          _order = 99
          def __init__(self, loss_scale=512, flat_master=False, dynamic=True, max_loss_scale=2.**24, div_factor=2.,
                       scale_wait=500):
              assert torch.backends.cudnn.enabled, "Mixed precision training requires cudnn."
              self.flat_master,self.dynamic,self.max_loss_scale = flat_master,dynamic,max_loss_scale
              self.div_factor,self.scale_wait = div_factor,scale_wait
              self.loss_scale = max_loss_scale if dynamic else loss_scale

          def begin_fit(self):
              self.run.model = fp16.convert_network(self.model, dtype=torch.float16)
              self.model_pgs, self.master_pgs = get_master(self.opt, self.flat_master)
              #Changes the optimizer so that the optimization step is done in FP32.
              self.run.opt.param_groups = self.master_pgs #Put those param groups inside our runner.
              if self.dynamic: self.count = 0

          def begin_batch(self): self.run.xb = self.run.xb.half() #Put the inputs to half precision
          def after_pred(self):  self.run.pred = self.run.pred.float() #Compute the loss in FP32
          def after_loss(self):  
              if self.in_train: self.run.loss *= self.loss_scale #Loss scaling to avoid gradient underflow

          def after_backward(self):
              #First, check for an overflow
              if self.dynamic and grad_overflow(self.model_pgs):
                  #Divide the loss scale by div_factor, zero the grad (after_step will be skipped)
                  self.loss_scale /= self.div_factor
                  self.model.zero_grad()
                  return True #skip step and zero_grad
              #Copy the gradients to master and unscale
              to_master_grads(self.model_pgs, self.master_pgs, self.flat_master)
              for master_params in self.master_pgs:
                  for param in master_params:
                      if param.grad is not None: param.grad.div_(self.loss_scale)
              #Check if it's been long enough without overflow
              if self.dynamic:
                  self.count += 1
                  if self.count == self.scale_wait:
                      self.count = 0
                      self.loss_scale *= self.div_factor

          def after_step(self):
              #Zero the gradients of the model since the optimizer is disconnected.
              self.model.zero_grad()
              #Update the params from master to model.
              to_model_params(self.model_pgs, self.master_pgs, self.flat_master)
    #+END_SRC

** 11_train_imagenette.ipynb
*** imports

    #+BEGIN_SRC python
            # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
            # install the python dependency necessary for the converting script
            !pip install fire

            # https://forums.fast.ai/t/couldnt-import-apex-fp16-utils-in-10c-fp16-ipynb/69621
            !pip install git+https://github.com/NVIDIA/apex


            # git clone the entire course material under /content
            !git clone https://github.com/fastai/course-v3.git

            # copy the ipynb2py converting python script
            !cp course-v3/nbs/dl2/notebook2script.py /content/

            # copy the jupyter notebooks to convert, under /content
            !cp course-v3/nbs/dl2/00_exports.ipynb /content/
            !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
            !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
            !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
            !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
            !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
            !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
            !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
            !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
            !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
            !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
            !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
            !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
            !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
            !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/
            !cp course-v3/nbs/dl2/10b_mixup_label_smoothing.ipynb /content/
            !cp course-v3/nbs/dl2/10c_fp16.ipynb /content/

            # convert the jupyter notebooks into python modules
            !python notebook2script.py 00_exports.ipynb
            !python notebook2script.py 01_matmul.ipynb
            !python notebook2script.py 02_fully_connected.ipynb
            !python notebook2script.py 03_minibatch_training.ipynb
            !python notebook2script.py 04_callbacks.ipynb
            !python notebook2script.py 05_anneal.ipynb
            !python notebook2script.py 05b_early_stopping.ipynb
            !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
            !python notebook2script.py 07_batchnorm.ipynb
            !python notebook2script.py 07a_lsuv.ipynb
            !python notebook2script.py 08_data_block.ipynb
            !python notebook2script.py 09_optimizers.ipynb
            !python notebook2script.py 09b_learner.ipynb
            !python notebook2script.py 09c_add_progress_bar.ipynb
            !python notebook2script.py 10_augmentation.ipynb
            !python notebook2script.py 10b_mixup_label_smoothing.ipynb
            !python notebook2script.py 10c_fp16.ipynb

            # copy the converted python modules under /content for later use
            !cp /content/exp/nb_00.py /content/exp.nb_00.py
            !cp /content/exp/nb_01.py /content/exp.nb_01.py
            !cp /content/exp/nb_02.py /content/exp.nb_02.py
            !cp /content/exp/nb_03.py /content/exp.nb_03.py
            !cp /content/exp/nb_04.py /content/exp.nb_04.py
            !cp /content/exp/nb_05.py /content/exp.nb_05.py
            !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
            !cp /content/exp/nb_06.py /content/exp.nb_06.py
            !cp /content/exp/nb_07.py /content/exp.nb_07.py
            !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
            !cp /content/exp/nb_08.py /content/exp.nb_08.py
            !cp /content/exp/nb_09.py /content/exp.nb_09.py
            !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
            !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
            !cp /content/exp/nb_10.py /content/exp.nb_10.py
            !cp /content/exp/nb_10b.py /content/exp.nb_10b.py
            !cp /content/exp/nb_10c.py /content/exp.nb_10c.py
    #+END_SRC

*** Flatten

    #+BEGIN_SRC python
      #export
      def noop(x): return x

      class Flatten(nn.Module):
          def forward(self, x): return x.view(x.size(0), -1)

      def conv(ni, nf, ks=3, stride=1, bias=False):
          return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)
    #+END_SRC

*** init_cnn, conv_layer ver4

    #+BEGIN_SRC python
      #export
      act_fn = nn.ReLU(inplace=True)

      def init_cnn(m):
          if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)
          if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)
          for l in m.children(): init_cnn(l)

      def conv_layer(ni, nf, ks=3, stride=1, zero_bn=False, act=True):
          bn = nn.BatchNorm2d(nf)
          nn.init.constant_(bn.weight, 0. if zero_bn else 1.)
          layers = [conv(ni, nf, ks, stride=stride), bn]
          if act: layers.append(act_fn)
          return nn.Sequential(*layers)
    #+END_SRC

*** ResBlock

    #+BEGIN_SRC python
      #export
      class ResBlock(nn.Module):
          def __init__(self, expansion, ni, nh, stride=1):
              super().__init__()
              nf,ni = nh*expansion,ni*expansion
              layers  = [conv_layer(ni, nh, 3, stride=stride),
                         conv_layer(nh, nf, 3, zero_bn=True, act=False)
              ] if expansion == 1 else [
                         conv_layer(ni, nh, 1),
                         conv_layer(nh, nh, 3, stride=stride),
                         conv_layer(nh, nf, 1, zero_bn=True, act=False)
              ]
              self.convs = nn.Sequential(*layers)
              self.idconv = noop if ni==nf else conv_layer(ni, nf, 1, act=False)
              self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True)

          def forward(self, x): return act_fn(self.convs(x) + self.idconv(self.pool(x)))
    #+END_SRC

    - expansion is either 1 or 4.

    - `expansion==1` corresponds to small ResNet (ResNet18, ResNet34) 
      where there are only 2 layers (two 3x3 conv layers)

      #+BEGIN_SRC python
        layers  = [conv_layer(ni, nh, 3, stride=stride),
                   conv_layer(nh, nf, 3, zero_bn=True, act=False)
      #+END_SRC

    - when `expansion` is 1 (`nh` and `nf` are same),
      - the number of channel may or may not increases after the 1st conv layer, and 'face' size may or may not change
      - the number of channel stays SAME after the 2nd conv layer, and 'fase' size stays the same

    - `expansion==4` corresponds to ResNet50 onwards, and
      ResBlock consists of 3 conv layers, called 'bottle neck layer',
      which squishes the number of channels and expand again, 

      #+BEGIN_SRC python
        conv_layer(ni, nh, 1),
        conv_layer(nh, nh, 3, stride=stride),
        conv_layer(nh, nf, 1, zero_bn=True, act=False)
      #+END_SRC

    - when `expansion==4`, `nf == 4*nh`, `ni == 4*ni`
      so if the value of `ni` is same as `nh`, 
      the ResBlock squishes by factor of 1/4 
      and expands by factor of 4

    - when `expansion==4` (nf == 4*nh, ni == 4*ni)
      - the number of channel is squished by 4 after the 1st conv layer
      - the number of channel stays same after the 2nd conv layer
      - the number channel is expanded after the 3rd layer

      for e.g.  64 channels => 16 channels => 16 channels => 64 channels

    - we initialize BatchNorm layer of the 3rd conv_layer
      with zero weight to train deep networks,
      corresponding to ResNet-D in 'back of trick' paper
      (32:00)

    - in either of below 2 cases, we cannot add up `self.conv(x)`
      with an activation for an identity path
      - changing the grid size by stride2
      - changing the number of channels
        (when we use stride2, we generally double the number
        of channels)

    - in order to resolve either cases, we need to insert 
      extra layer(s) to an identity layer depending on 
      
      - stride is 2 => insert AvgPool layer
      - the number of channels changes => insert stride1 conv layer

    - we apply stride2 on the 2nd conv layer since
      applying on the 1st conv layer results in throwing away inputs

    - note that `ni` is multiplied by 4 when `expansion` is 4

*** XResNet

    #+BEGIN_SRC python
      #export
      class XResNet(nn.Sequential):
          @classmethod
          def create(cls, expansion, layers, c_in=3, c_out=1000):
              nfs = [c_in, (c_in+1)*8, 64, 64]
              stem = [conv_layer(nfs[i], nfs[i+1], stride=2 if i==0 else 1)
                  for i in range(3)]

              nfs = [64//expansion,64,128,256,512]
              res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1],
                                            n_blocks=l, stride=1 if i==0 else 2)
                        for i,l in enumerate(layers)]
              res = cls(
                  ,*stem,
                  nn.MaxPool2d(kernel_size=3, stride=2, padding=1),
                  ,*res_layers,
                  nn.AdaptiveAvgPool2d(1), Flatten(),
                  nn.Linear(nfs[-1]*expansion, c_out),
              )
              init_cnn(res)
              return res

          @staticmethod
          def _make_layer(expansion, ni, nf, n_blocks, stride):
              return nn.Sequential(
                  *[ResBlock(expansion, ni if i==0 else nf, nf, stride if i==0 else 1)
                    for i in range(n_blocks)])
    #+END_SRC

    - 1st `nfs` defines the channel numbers of 
      [input, 1st activation, 2nd activation, 3rd activation]

    - note that 2nd `nfs` can be re-written as
      [64//expansion, 64, 2*64, 2*2*64, 2*2*2*64]

    - note that the value of `nh` for a certain laer will be
      `ni` for the successing layer

    - for a certain layer, 
      - the number of the channels of the activation of the
        last ResBlock is 4 times `nh` for the layer

      - the number of the channels of the input for the
        1st ResBlock of the successing layer is 4 times
        `ni` of the successing layer

      - since `nh` for a certain layer is same as `ni` for
        the successing layer, the two numbers above will be same

    - for the concrete example, see below

    - layers is, for example, in the form of
      [3, 4, 6, 3]

      # 1st layer
    - with the example above, for the 1st layer
      (index 0 of `enumerate(layers)`, we have 3 blocks and use 
      - nfs[0]==64//expansion,
      - nfs[1]==64

    - for the 1st block of the 1st layer(with 3 blocks), we use
      - ni==nfs[0]==64//expansion,
      - nf==nfs[1]==64

      => 
      ResBlock::__init__(self, expansion, 64//expansion, 64)

      if `expansion==4`, inside __init__, 
      - `ni` will be 64 (4*ni)
      - `nh` will be 64 
      - `nf` will be 256 (4*nh)

    - for the 2nd & 3rd ResBlock of the 1st layer(with 3 blocks ), we use
      - ni==nfs[1]==64
      - nf==nfs[1]==64

      => 
      ResBlock::__init__(self, expansion, 64, 64)
      
      if `expansion==4`, inside __init__, 
      - `ni` will be 256 (4*ni)
      - `nh` will be 64
      - `nf` will be 256 (4*nh)

      # 2nd layer
    - with the example above, for the 2nd layer
      (index 1 of `enumerate(layers)`, we have 4 blocks and use 
      - nfs[1]==64
      - nfs[2]==128

    - for the 1st block of the 1st layer(with 4 blocks), we use
      - ni==nfs[1]==64
      - nf==nfs[2]==128

      => 
      ResBlock::__init__(self, expansion, 64, 128)

      if `expansion==4`, inside __init__, 
      - `ni` will be 256 (4*ni)
      - `nh` will be 128
      - `nf` will be 512 (4*nh)

    - for the 2nd & 3rd & 4th ResBlocks of the 1st layer(with 4 blocks ), we use
      - ni==nfs[2]==128
      - nf==nfs[2]==128

      => 
      ResBlock::__init__(self, expansion, 128, 128)
      
      if `expansion==4`, inside __init__, 
      - `ni` will be 512 (4*ni)
      - `nh` will be 128 
      - `nf` will be 512 (4*nh)
        
*** how to use Xres

    #+BEGIN_SRC python
      #export
      def xresnet18 (**kwargs): return XResNet.create(1, [2, 2,  2, 2], **kwargs)
      def xresnet34 (**kwargs): return XResNet.create(1, [3, 4,  6, 3], **kwargs)
      def xresnet50 (**kwargs): return XResNet.create(4, [3, 4,  6, 3], **kwargs)
      def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs)
      def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs)
    #+END_SRC


    
*** get_batch ver2

    #+BEGIN_SRC python
      #export
      def get_batch(dl, learn):
          learn.xb,learn.yb = next(iter(dl))
          learn.do_begin_fit(0)
          learn('begin_batch')
          learn('after_fit')
          return learn.xb,learn.yb
    #+END_SRC

*** model_summary

    #+BEGIN_SRC python
      # export
      def model_summary(model, data, find_all=False, print_mod=False):
          xb,yb = get_batch(data.valid_dl, learn)
          mods = find_modules(model, is_lin_layer) if find_all else model.children()
          f = lambda hook,mod,inp,out: print(f"====\n{mod}\n" if print_mod else "", out.shape)
          with Hooks(mods, f) as hooks: learn.model(xb)
    #+END_SRC

*** how to interpret model_summary

    #+BEGIN_SRC python
      loss_func = LabelSmoothingCrossEntropy()
      arch = partial(xresnet18, c_out=10)
      opt_func = adam_opt(mom=0.9, mom_sqr=0.99, eps=1e-6, wd=1e-2)

      learn = Learner(arch(), data, loss_func, lr=1, cb_funcs=cbfs, opt_func=opt_func)
      learn.model = learn.model.cuda()
      model_summary(learn.model, data, print_mod=False)

      '''
       torch.Size([128, 32, 64, 64])
       torch.Size([128, 64, 64, 64])
       torch.Size([128, 64, 64, 64])
       torch.Size([128, 64, 32, 32])
       torch.Size([128, 64, 32, 32])
       torch.Size([128, 128, 16, 16])
       torch.Size([128, 256, 8, 8])
       torch.Size([128, 512, 4, 4])
       torch.Size([128, 512, 1, 1])
       torch.Size([128, 512])
       torch.Size([128, 10])
      '''
    #+END_SRC
    
    - model is xresnet34 with `c_out==10`, `expansion==1`
    - each row of the outputs corresponds to an activation of a layer,
      and the 2nd entry represents the number of the channel of the activation
    - inside XresNet, conv layers a recreated based on 
      the 1st `nfs` and the 2nd `nfs`.
    - a conv layer is created by paring up two consequitive elements
      for `nfs`
    - the number of the channel of an activation of a conv layer
      is same as the 2nd element of the pair for which the 
      conv layer is created
    - in XresNet, there are 
      - 3 layers (from 1st `nfs`)
      - 1 layer  (nn.MaxPool2d)
      - 4 layers (from 2nd `nfs`)
      - 1 layer  (nn.AdaptiveAvgPool2d)
      - 1 layer  (Flatten)
      - 1 layer  (nn.Linear)

*** create_phases(phases)

    #+BEGIN_SRC python
      #export
      def create_phases(phases):
          phases = listify(phases)
          return phases + [1-sum(phases)]

      print(create_phases(0.3))
      print(create_phases([0.3,0.2]))
      '''
      [0.3, 0.7]
      [0.3, 0.2, 0.5]
      '''
    #+END_SRC

*** cnn_learner

    #+BEGIN_SRC python
      #export
      def cnn_learner(arch, data, loss_func, opt_func, c_in=None, c_out=None,
                      lr=1e-2, cuda=True, norm=None, progress=True, mixup=0, xtra_cb=None, **kwargs):
          cbfs = [partial(AvgStatsCallback,accuracy)]+listify(xtra_cb)
          if progress: cbfs.append(ProgressCallback)
          if cuda:     cbfs.append(CudaCallback)
          if norm:     cbfs.append(partial(BatchTransformXCallback, norm))
          if mixup:    cbfs.append(partial(MixUp, mixup))
          arch_args = {}
          if not c_in : c_in  = data.c_in
          if not c_out: c_out = data.c_out
          if c_in:  arch_args['c_in' ]=c_in
          if c_out: arch_args['c_out']=c_out
          return Learner(arch(**arch_args), data, loss_func, opt_func=opt_func, lr=lr, cb_funcs=cbfs, **kwargs)

    #+END_SRC

** 11a_transfer_learning.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # https://forums.fast.ai/t/couldnt-import-apex-fp16-utils-in-10c-fp16-ipynb/69621
      !pip install git+https://github.com/NVIDIA/apex


      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
      !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/
      !cp course-v3/nbs/dl2/10b_mixup_label_smoothing.ipynb /content/
      !cp course-v3/nbs/dl2/10c_fp16.ipynb /content/
      !cp course-v3/nbs/dl2/11_train_imagenette.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb
      !python notebook2script.py 10_augmentation.ipynb
      !python notebook2script.py 10b_mixup_label_smoothing.ipynb
      !python notebook2script.py 10c_fp16.ipynb
      !python notebook2script.py 11_train_imagenette.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
      !cp /content/exp/nb_10.py /content/exp.nb_10.py
      !cp /content/exp/nb_10b.py /content/exp.nb_10b.py
      !cp /content/exp/nb_10c.py /content/exp.nb_10c.py
      !cp /content/exp/nb_11.py /content/exp.nb_11.py

    #+END_SRC

*** sched_1cycle

    #+BEGIN_SRC python
      def sched_1cycle(lr, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):
          phases = create_phases(pct_start)
          sched_lr  = combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))
          sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))
          return [ParamScheduler('lr', sched_lr),
                  ParamScheduler('mom', sched_mom)]
    #+END_SRC

*** random_splitter

    #+BEGIN_SRC python
      #export
      def random_splitter(fn, p_valid): return random.random() < p_valid
    #+END_SRC

*** pet_labeler

    #+BEGIN_SRC python
      def pet_labeler(fn): return re.findall(r'^(.*)_\d+.jpg$', fn.name)[0]
    #+END_SRC

*** TODO custom head

    #+BEGIN_SRC python
      learn = cnn_learner(xresnet18, data, loss_func, opt_func, c_out=10, norm=norm_imagenette)

      st = torch.load(mdl_path/'iw5')

      m = learn.model

      m.load_state_dict(st)

      cut = next(i for i,o in enumerate(m.children()) if isinstance(o,nn.AdaptiveAvgPool2d))
      m_cut = m[:cut]

      xb,yb = get_batch(data.valid_dl, laearn)

      pred = m_cut(xb)

      pred.shape

      ni = pred.shape[1]

      #export
      class AdaptiveConcatPool2d(nn.Module):
          def __init__(self, sz=1):
              super().__init__()
              self.output_size = sz
              self.ap = nn.AdaptiveAvgPool2d(sz)
              self.mp = nn.AdaptiveMaxPool2d(sz)
          def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)

      nh = 40

      m_new = nn.Sequential(
          m_cut, AdaptiveConcatPool2d(), Flatten(),
          nn.Linear(ni*2, data.c_out))

      learn.model = m_new

      learn.fit(5, cbsched)
    #+END_SRC

    - this is customizing the model for imagewoof(c_out=10)
      into the model that can handle PET dataset(c_out=37)

    - it loads the value of the weights for imagewoof model by

      #+BEGIN_SRC python
        st = torch.load(mdl_path/'iw5')
      #+END_SRC

    - [ ] why can we use `norm=norm_imagenette`
      when `data` is for PET?

*** adapt_model

    #+BEGIN_SRC python
      def adapt_model(learn, data):
          cut = next(i for i,o in enumerate(learn.model.children())
                     if isinstance(o,nn.AdaptiveAvgPool2d))
          m_cut = learn.model[:cut]
          xb,yb = get_batch(data.valid_dl, learn)
          pred = m_cut(xb)
          ni = pred.shape[1]
          m_new = nn.Sequential(
              m_cut, AdaptiveConcatPool2d(), Flatten(),
              nn.Linear(ni*2, data.c_out))
          learn.model = m_new
    #+END_SRC


    - this is just a refactoring of the codes in 'custom head'

*** apply_mod, set_grad

    #+BEGIN_SRC python
      def apply_mod(m, f):
          f(m)
          for l in m.children(): apply_mod(l, f)

      def set_grad(m, b):
          if isinstance(m, (nn.Linear,nn.BatchNorm2d)): return
          if hasattr(m, 'weight'):
              for p in m.parameters(): p.requires_grad_(b)
    #+END_SRC

    - `set_grad` freeze layers which is not
      the last layer(=nn.Linear) nor batch norm layer
    - 

*** bn_splitter

    #+BEGIN_SRC python
      def bn_splitter(m):
          def _bn_splitter(l, g1, g2):
              if isinstance(l, nn.BatchNorm2d): g2 += l.parameters()
              elif hasattr(l, 'weight'): g1 += l.parameters()
              for ll in l.children(): _bn_splitter(ll, g1, g2)
              
          g1,g2 = [],[]
          _bn_splitter(m[0], g1, g2)
          
          g2 += m[1:].parameters()
          return g1,g2
    #+END_SRC
    
    - `bn_splitter` sets the learning rate to zero for
      layers, which has the same effect as freezing the layers

*** cb_types

    #+BEGIN_SRC python
      from types import SimpleNamespace
      cb_types = SimpleNamespace(**{o:o for o in Learner.ALL_CBS})
    #+END_SRC

*** DebugCallback

    #+BEGIN_SRC python
      #export
      class DebugCallback(Callback):
          _order = 999
          def __init__(self, cb_name, f=None): self.cb_name,self.f = cb_name,f
          def __call__(self, cb_name):
              if cb_name==self.cb_name:
                  if self.f: self.f(self.run)
                  else:      set_trace()
    #+END_SRC

*** sched_1cycle

    #+BEGIN_SRC python
      #export
      def sched_1cycle(lrs, pct_start=0.3, mom_start=0.95, mom_mid=0.85, mom_end=0.95):
          phases = create_phases(pct_start)
          sched_lr  = [combine_scheds(phases, cos_1cycle_anneal(lr/10., lr, lr/1e5))
                       for lr in lrs]
          sched_mom = combine_scheds(phases, cos_1cycle_anneal(mom_start, mom_mid, mom_end))
          return [ParamScheduler('lr', sched_lr),
                  ParamScheduler('mom', sched_mom)]
    #+END_SRC

** 12_text.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # https://forums.fast.ai/t/couldnt-import-apex-fp16-utils-in-10c-fp16-ipynb/69621
      !pip install git+https://github.com/NVIDIA/apex


      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
      !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/
      !cp course-v3/nbs/dl2/10b_mixup_label_smoothing.ipynb /content/
      !cp course-v3/nbs/dl2/10c_fp16.ipynb /content/
      !cp course-v3/nbs/dl2/11_train_imagenette.ipynb /content/
      !cp course-v3/nbs/dl2/11a_transfer_learning.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb
      !python notebook2script.py 10_augmentation.ipynb
      !python notebook2script.py 10b_mixup_label_smoothing.ipynb
      !python notebook2script.py 10c_fp16.ipynb
      !python notebook2script.py 11_train_imagenette.ipynb
      !python notebook2script.py 11a_transfer_learning.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
      !cp /content/exp/nb_10.py /content/exp.nb_10.py
      !cp /content/exp/nb_10b.py /content/exp.nb_10b.py
      !cp /content/exp/nb_10c.py /content/exp.nb_10c.py
      !cp /content/exp/nb_11.py /content/exp.nb_11.py
      !cp /content/exp/nb_11a.py /content/exp.nb_11a.py

    #+END_SRC

*** read_file, TextList

    #+BEGIN_SRC python
      #export
      def read_file(fn): 
          with open(fn, 'r', encoding = 'utf8') as f: return f.read()
          
      class TextList(ItemList):
          @classmethod
          def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):
              return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)
          
          def get(self, i):
              if isinstance(i, Path): return read_file(i)
              return i
    #+END_SRC

*** replace_all_caps, deal_caps, add_eos_bos

    #+BEGIN_SRC python
      #export
      def replace_all_caps(x):
          "Replace tokens in ALL CAPS by their lower version and add `TK_UP` before."
          res = []
          for t in x:
              if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())
              else: res.append(t)
          return res

      def deal_caps(x):
          "Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before."
          res = []
          for t in x:
              if t == '': continue
              if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)
              res.append(t.lower())
          return res

      def add_eos_bos(x): return [BOS] + x + [EOS]

      default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]
    #+END_SRC

*** parallel

    #+BEGIN_SRC python
      #export
      from spacy.symbols import ORTH
      from concurrent.futures import ProcessPoolExecutor

      def parallel(func, arr, max_workers=4):
          if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))
          else:
              with ProcessPoolExecutor(max_workers=max_workers) as ex:
                  return list(progress_bar(ex.map(func, enumerate(arr)), total=len(arr)))
          if any([o is not None for o in results]): return results
    #+END_SRC

*** TokenizeProcessor

#+BEGIN_SRC python
  #export
  class TokenizeProcessor(Processor):
      def __init__(self, lang="en", chunksize=2000, pre_rules=None, post_rules=None, max_workers=4): 
          self.chunksize,self.max_workers = chunksize,max_workers
          self.tokenizer = spacy.blank(lang).tokenizer
          for w in default_spec_tok:
              self.tokenizer.add_special_case(w, [{ORTH: w}])
          self.pre_rules  = default_pre_rules  if pre_rules  is None else pre_rules
          self.post_rules = default_post_rules if post_rules is None else post_rules

      def proc_chunk(self, args):
          i,chunk = args
          chunk = [compose(t, self.pre_rules) for t in chunk]
          docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]
          docs = [compose(t, self.post_rules) for t in docs]
          return docs

      def __call__(self, items): 
          toks = []
          if isinstance(items[0], Path): items = [read_file(i) for i in items]
          chunks = [items[i: i+self.chunksize] for i in (range(0, len(items), self.chunksize))]
          toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)
          return sum(toks, [])
      
      def proc1(self, item): return self.proc_chunk([item])[0]
      
      def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]
      def deproc1(self, tok):    return " ".join(tok)
#+END_SRC

*** NumericalizeProcessor
#+BEGIN_SRC python
  #export
  import collections

  class NumericalizeProcessor(Processor):
      def __init__(self, vocab=None, max_vocab=60000, min_freq=2): 
          self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq
      
      def __call__(self, items):
          #The vocab is defined on the first use.
          if self.vocab is None:
              freq = Counter(p for o in items for p in o)
              self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c >= self.min_freq]
              for o in reversed(default_spec_tok):
                  if o in self.vocab: self.vocab.remove(o)
                  self.vocab.insert(0, o)
          if getattr(self, 'otoi', None) is None:
              self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)}) 
          return [self.proc1(o) for o in items]
      def proc1(self, item):  return [self.otoi[o] for o in item]
      
      def deprocess(self, idxs):
          assert self.vocab is not None
          return [self.deproc1(idx) for idx in idxs]
      def deproc1(self, idx): return [self.vocab[i] for i in idx]
#+END_SRC

*** using TokenizeProcessor & NumericalizeProcessor creating LabeledData

    #+BEGIN_SRC python
      %time ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])
    #+END_SRC

*** FOR language modeling, how to create batches from documents
    - for example, batch-size=6, bptt=5

    - FOR language modeling, we concatnate the whole documents
      into single giant document,

    - then create a grid with 
      height = batch-size = 6, 
      width = Math.floor(length//height)
      
    - store the single giant document in the grid

      | xxbos | \n | xxmaj | in | this | noteook | , | we | ... |
      | ...   |    |       |    |      |         |   |    |     |
      |       |    |       |    |      |         |   |    |     |
      |       |    |       |    |      |         |   |    |     |
      |       |    |       |    |      |         |   |    |     |
      |       |    |       |    |      |         |   |    |     |

    - slice the grid into smaller grids with width=bptt(=5)

    - each small grid is the batch.

*** LM_PreLoader (LM_Dataset)

    #+BEGIN_SRC python
      #export
      class LM_PreLoader():
          def __init__(self, data, bs=64, bptt=70, shuffle=False):
              self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle
              total_len = sum([len(t) for t in data.x])
              self.n_batch = total_len // bs
              self.batchify()
          
          def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs
          
          def __getitem__(self, idx):
              source = self.batched_data[idx % self.bs]
              seq_idx = (idx // self.bs) * self.bptt
              return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1]
          
          def batchify(self):
              texts = self.data.x
              if self.shuffle: texts = texts[torch.randperm(len(texts))]
              stream = torch.cat([tensor(t) for t in texts])
              self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)
    #+END_SRC

    # an overview of LM_PreLoader
    - `LM_PreLoader` is a data set similar to `DataSet`,
      which will be passed to `DataLoader` later.

    - to understand the behavior of `LM_PreLoader`, imagine
      a grid with the number of its row&column equal to
      `bs` & `total_len/bs/bptt`
      (for simplicity, suppose `total_len/bs/bptt` is an integer)
      
    - each cell of the grid is an element that `LM_PreLoader`
      returns, and is a list which contains `bptt` numericalized
      words.

    - for  some number `i`, it will determine the indexes for
      row & columns such that for consecuitive numbers 
      it returns two consecuitive cells in the same column

    - as the label, `LM_PreLoader` returns a list
      of the elements that are contained in the
      the 'cell' when it is shifted to the right by 1

    # __init__
    - `data` will be an instance of `LabeledData` instance
      that contains a list of texts and a list of dummy labels

    - `total_len` is the total length of all the texts 
      contained in `data`

    - `n_batch` is the maximum integer less than `total_len/bs`

    # batchify
    - `torch.cat` and `tensor(t)` works as below

      #+BEGIN_SRC python
        import torch
        my_tensor1 = torch.tensor([1,2,3])
        my_tensor2 = torch.tensor([4,5,6])
        my_cat = torch.cat([my_tensor1, my_tensor2])
        my_cat

        '''
        tensor([1,2,3,4,5,6])
        '''
      #+END_SRC

    - `stream = torch.cat([tensor(t) for t in texts])` creates
      a huge tensor made by concatnating all the individual
      numericalized texts in `data`

    - `stream[:self.n_batch * self.bs]` grabs the elements 
      up to `self.n_batch * self.bs`, and throw away the remaining.

    - `view(self.bs, self.n_batch)` reshape the huge tensor
      into a rectangular shaped tensor whose 'height' is `self.bs`,
      and 'width' is `n_batch`

    # __getitem__
    - recall the explanation of how `LM_PreLoader` behaves above.

    - `__getitem__` chooses the indexes for 'row' & 'column'
      of the grid, and returns the 'cell' (a list)

    - the index for the row of the 'grid' is determined by
      `idx % bs`

    - the index for the column of the 'grid' is determined by
      `idx//bs`

    - now, we construct the corresponding cell (a list)

    - each 'cell' is a list of length `bptt`, so for 
      a cell whose index is (i, j), the cell contains
      (bptt*j)-th element up-to&not including (bptt*j+bptt)-th 
      element in `batched_data[i]`

    - the label for the 'cell' will be elements contained
      when shifting the 'cell' by 1.

    - note that for the label for the cell in the right edge
      of the grid, the number of the elements contained 
      in the label will be 1 less than `bptt`, 
      but python array indexing [l:l+m] works fine even when
      the array has less than `m` elements

*** get_lm_dls, lm_databunchify

    #+BEGIN_SRC python
      #export
      def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):
          return (DataLoader(LM_PreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),
                  DataLoader(LM_PreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))

      def lm_databunchify(sd, bs, bptt, **kwargs):
          return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))
    #+END_SRC

*** FOR language modeling, create data loader

    #+BEGIN_SRC python
      bs,bptt = 64,70
      data = lm_databunchify(ll, bs, bptt)
    #+END_SRC

*** FOR classification, how to create batches
    - note that for classification, we create batches
      in a completely different way from that for
      language modeling

    - in concrete, for classification, each batch
      contains a group of documents whereas 
      for language modeling, a batch contains
      a list of numericalized words

    - for text classification, batches has different 'width'
      where 'width' is the length of the longest document
      in the batch

    - each batch is constructed so that it contains 
      documents of about the same length.

    - the first batch contains the longest documents
      and the last batch conntains the shortest documents

*** SortSampler

    #+BEGIN_SRC python
      #export
      from torch.utils.data import Sampler

      class SortSampler(Sampler):
          def __init__(self, data_source, key): self.data_source,self.key = data_source,key
          def __len__(self): return len(self.data_source)
          def __iter__(self):
              return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))
    #+END_SRC

    - NOTE that `Sampler` is from PyTorch, 
      and not 'home-made' `Sampler`

    - `SortSampler` is expected to  be used with PyTorch version
      of `DataLoader`, so it should returns a list of indexes

    - `SortSampler` is for validation set

    - `range(len(self.data_source))` returns a list containing
      the indexes of `data_source`

    - `key` is a function which extracts a key for sorting from
      each element of `list(range(len(self.data_source))`

    - for example, `key` is as below, which returns the length
      of a document at index `t` of `data_source`

      #+BEGIN_SRC python
        `key=lambda t: len(valid_ds.x[t]))`
      #+END_SRC

    - `sorted` 

*** SortishSampler

    #+BEGIN_SRC python
      #export
      class SortishSampler(Sampler):
          def __init__(self, data_source, key, bs):
              self.data_source,self.key,self.bs = data_source,key,bs

          def __len__(self) -> int: return len(self.data_source)

          def __iter__(self):
              idxs = torch.randperm(len(self.data_source))
              megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]
              sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])
              batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]
              max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,
              batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.
              batch_idxs = torch.randperm(len(batches)-2)
              sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])
              sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])
              return iter(sorted_idx)
    #+END_SRC

*** pad_collate

    #+BEGIN_SRC python
      #export
      def pad_collate(samples, pad_idx=1, pad_first=False):
          max_len = max([len(s[0]) for s in samples])
          res = torch.zeros(len(samples), max_len).long() + pad_idx
          for i,s in enumerate(samples):
              if pad_first: res[i, -len(s[0]):] = LongTensor(s[0])
              else:         res[i, :len(s[0]) ] = LongTensor(s[0])
          return res, tensor([s[1] for s in samples])
    #+END_SRC

    - `samples` is a list of tuples for creating a batch, 
      each of which is a pair of (document, label),

    - `res` is a rectangular shaped tensor (a rank2 tensor)
      which represents a batch
      - 'height'(=batch size) is len(samples)
      - 'width' is `max_len`
      - i-th row will be later populated with the document in the
        i-th element of `samples` & padding `pad_idx`

    - `.long()` converts the elements in a tensor from float to integer

    - `pad_idx` is a scalar, and `+ pad_idx` adds `pad_idx` to
      all the elements of a tensor by broadcasting

    - `res` is first filled with 0s and padding

    - `tensor([s[1] for s in samples])` is a tensor for the labels

*** creating DataLoader for training data for lauguage classification

    #+BEGIN_SRC python
      #
      proc_cat = CategoryProcessor()
      il = TextList.from_files(path, include=['train', 'test'])
      sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))
      ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)

      #
      bs = 64
      train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs)
      train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate)
    #+END_SRC

    - 

*** get_clas_dls, clas_dls

    #+BEGIN_SRC python
      #export
      def get_clas_dls(train_ds, valid_ds, bs, **kwargs):
          train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)
          valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))
          return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),
                  DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))

      def clas_databunchify(sd, bs, **kwargs):
          return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))
    #+END_SRC

    - note that `DataLoader` is PyTorch version, whose imeplementation
      is similar to 'home-made' `DataLoader` ver2,

    - PyTorch version of `DataLoader` is slightly different from
      'home-made' version, and the usage is slightly different in that
      - need to specify batch size
      - samplers should be a function which returns a list of indexes

** 12a_awd_lstm.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # https://forums.fast.ai/t/couldnt-import-apex-fp16-utils-in-10c-fp16-ipynb/69621
      !pip install git+https://github.com/NVIDIA/apex


      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
      !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/
      !cp course-v3/nbs/dl2/10b_mixup_label_smoothing.ipynb /content/
      !cp course-v3/nbs/dl2/10c_fp16.ipynb /content/
      !cp course-v3/nbs/dl2/11_train_imagenette.ipynb /content/
      !cp course-v3/nbs/dl2/11a_transfer_learning.ipynb /content/
      !cp course-v3/nbs/dl2/12_text.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb
      !python notebook2script.py 10_augmentation.ipynb
      !python notebook2script.py 10b_mixup_label_smoothing.ipynb
      !python notebook2script.py 10c_fp16.ipynb
      !python notebook2script.py 11_train_imagenette.ipynb
      !python notebook2script.py 11a_transfer_learning.ipynb
      !python notebook2script.py 12_text.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
      !cp /content/exp/nb_10.py /content/exp.nb_10.py
      !cp /content/exp/nb_10b.py /content/exp.nb_10b.py
      !cp /content/exp/nb_10c.py /content/exp.nb_10c.py
      !cp /content/exp/nb_11.py /content/exp.nb_11.py
      !cp /content/exp/nb_11a.py /content/exp.nb_11a.py
      !cp /content/exp/nb_12.py /content/exp.nb_12.py

    #+END_SRC

*** LSTMCell

    #+BEGIN_SRC python
      class LSTMCell(nn.Module):
          def __init__(self, ni, nh):
              super().__init__()
              self.ih = nn.Linear(ni,4*nh)
              self.hh = nn.Linear(nh,4*nh)

          def forward(self, input, state):
              h,c = state
              #One big multiplication for all the gates is better than 4 smaller ones
              gates = (self.ih(input) + self.hh(h)).chunk(4, 1)
              ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3])
              cellgate = gates[3].tanh()

              c = (forgetgate*c) + (ingate*cellgate)
              h = outgate * c.tanh()
              return h, (h,c)
    #+END_SRC

    # overview
    - each single `LSTMCell` accepts
      - `input`
      - `state` which is a comination of 
        'hidden state' and 'cell state'

      and calculates 
      - h: new hidden state
      - c: new cell state

      and returns
      - h
      - (h,c)

    # forward
    - `chunk(4,1)`
      - ref: https://pytorch.org/docs/master/generated/torch.chunk.html

      - it splits an tensor into 4 chunks along the 2nd axis (along row)

    - `forgetgate*c`,  `ingate*cellgate` does 
      element-wise multiplication between tensors

    - the size of each variable is as follows;
      - the size of `input` is torch.Size([bs, ni]), 
        where `bs` is batch size, and `ni` is the 
        embedding size

        |     | word1 vec         |
        |-----+-------------------|
        |   1 | (S1, S2,...,S300) |
        |-----+-------------------|
        |   2 |                   |
        |-----+-------------------|
        | ... |                   |
        |-----+-------------------|
        |  64 |                   |

      - the size of `h` is torch.Size([bs,nh]),

      - the size of `self.ih(input)` is torch.Size([bs,4*nh]),

      - the size of `self.hh(h)` is also torch.Size([bs,4*nh])

      - the size of each element of `gates` is torch.Size([bs,nh])

      - so the size of `ingate`, `forgetgate`, `outgate`
        is torch.Size([1, nh])

      - the size of `cellgate` is torch.Size([bs,nh]),

      - the size of `c` is torch.Size([bs,nh]), 

      - the size of `c` is same as `ingate*cellgate`

    - newly calculated `h` will be the output of each cell

    - `state` is a pair of (h,c)
    
*** TODO LSTMLayer

    #+BEGIN_SRC python
      class LSTMLayer(nn.Module):
          def __init__(self, cell, *cell_args):
              super().__init__()
              self.cell = cell(*cell_args)

          def forward(self, input, state):
              inputs = input.unbind(1)
              outputs = []
              for i in range(len(inputs)):
                  out, state = self.cell(inputs[i], state)
                  outputs += [out]
              return torch.stack(outputs, dim=1), state
    #+END_SRC

    # overview
    - `LSTMLayer` does below;
      - records the history of hidden state(`out`) 
        which is returned by each `cell`, and stack 
        them together in the end.

      - keeps track of the latest state which is a combination of
        hidden state & cell state and updated by every `cell` 

    - `input` is a batch of sequences of embedding word vectors

      |     | word1 vec | word2 vec | ... | word70 vec |
      |-----+-----------+-----------+-----+------------|
      |   1 |           |           |     |            |
      |-----+-----------+-----------+-----+------------|
      |   2 |           |           |     |            |
      |-----+-----------+-----------+-----+------------|
      | ... |           |           |     |            |
      |-----+-----------+-----------+-----+------------|
      |  64 |           |           |     |            |

    - the shape of `outputs` is similar to `input`, and looks like
      torch.Size([64, 70, 300]) where
      - 64: batch size
      - 70: the length of sequence
      - 300: the 'width' of hidden state 

    - the shape of `outputs` will be same when embedding size
      is same as the 'width' of hidden state

    # pytorch 
    - `unbind`

      #+BEGIN_SRC python
        # 64: batch size
        # 70: length of the sequences in a single batch ()
        # 300: embedding size of each embedding word vector in a batch

        import torch
        my_input = torch.randn(64, 70, 300)
        my_input.size()
        my_unbind = my_input.unbind(1)
        type(my_unbind), len(my_unbind), my_unbind[0].size()

        '''
        (tuple, 70, torch.Size([64, 300]))
        '''
      #+END_SRC


    # forward
    - `input` is a single batch of sequences of 
      embedding word vectors

    #+BEGIN_SRC python
      # 64: batch size
      # 70: the length of a sequence
      # 300: width of embedding


      [
          # 1st element of the single batch
          [S1_1_1, S1_1_2, ..., S1_1_300],
          [S1_2_1, S1_2_2, ..., S1_2_300],
          ...,
          [S1_70_1, S1_70_2, ..., S1_70_300],

          # 2nd element of the single batch
          [S2_1_1, S2_1_2, ..., S2_1_300],
          [S2_2_1, S2_2_2, ..., S2_2_300],
          ...,
          [S2_70_1, S2_70_2, ..., S2_70_300],

          ...,

          # 64th element of the single batch    
          [S64_1_1, S64_1_2, ..., S64_1_300],
          [S64_2_1, S64_2_2, ..., S64_2_300],
          ...,
          [S64_70_1, S64_70_2, ..., S64_70_300],

      ]
    #+END_SRC

    - `input.unbind(1)` splits a tensor into a tuple along the 
      dim=1 (2nd axis), so each element in the tuple corresponds to
      a bundle of embedding word vectors
      (e.g. bundle of 64 embedding word vectors with embedding
      size 300)

      |     | word1 vec | word2 vec | word3 vec | ... | word70 vec |
      |-----+-----------+-----------+-----------+-----+------------|
      |   1 |           |           |           |     |            |
      |-----+-----------+-----------+-----------+-----+------------|
      |   2 |           |           |           |     |            |
      |-----+-----------+-----------+-----------+-----+------------|
      | ... |           |           |           |     |            |
      |-----+-----------+-----------+-----------+-----+------------|
      |  64 |           |           |           |     |            |


    - the resulting TUPLE of `input.unbind(1)`
    #+BEGIN_SRC python
      (
          # 1st words of the sequence
          [S1_1_1, S1_1_2, ..., S1_1_300],
          [S2_1_1, S2_1_2, ..., S2_1_300],
          ...,
          [S64_1_1, S64_1_2, ..., S64_1_300],

          # 2nd words of the sequence
          [S1_2_1, S1_2_2, ..., S1_2_300],
          [S2_2_1, S2_2_2, ..., S2_2_300],
          ...,
          [S64_2_1, S64_2_2, ..., S64_2_300],

          ...,

          # 64th words of the sequence
          [S1_70_1, S1_70_2, ..., S1_70_300],
          [S2_70_1, S2_70_2, ..., S2_70_300],
          ...,
          [S64_70_1, S64_70_2, ..., S64_70_300],    
      )
    #+END_SRC

    - for each element of the tuples,
      `forward` calculates `out`, `state`, and record 
      the history of `out` on `outputs`

    - `outputs` is an array of rank2 tensors with
      the size of each tensor torch.Size([bs, nh])


    - in the end, `forward` stacks the elements in the 
      output history array, along 'sequence' axis, to 
      make the final output, and return
      - the final output
      - the final state
        
      #+BEGIN_SRC python
        #memo
        import torch
        my_input = torch.randn(3, 4, 5)
        my_input.size()
        my_unbind = my_input.unbind(1)
        type(my_unbind), len(my_unbind), my_unbind[0].size()
        '''
        (tuple, 4, torch.Size([3, 5]))
        '''

        my_unbind_list = [my_unbind[0], my_unbind[1], my_unbind[2], my_unbind[3]]
        my_stacked = torch.stack(my_unbind_list, dim=1)
        my_stacked.size()

        '''
        torch.Size([3, 4, 5])
        '''

      #+END_SRC


    - this way, the final output is a batch of sequences of 
      hidden states where the length of a sequence is equal to 
      the length of a sequence of embedding word vectors 
      in input

    - [ ] ? 1:51:50 why will it be slow to use home made `LSTMLayer`
      - does GPU do special thing when executing a loop?

*** nn.LSTM
    - `nn.LSTM::init` takes arguments `ni` & `nh`
      - `ni` specifies the 'width' of an embedding word vector
      - `nh` specifies the 'width' of a hidden state vector

*** TODO dropout_mask

    #+BEGIN_SRC python
      #export
      def dropout_mask(x, sz, p):
          return x.new(*sz).bernoulli_(1-p).div_(1-p)
    #+END_SRC

    - `new` creates a tensor of the same type as `x`
      whose size is (*sz)
      https://pytorch.org/docs/0.3.1/tensors.html?highlight=new#torch.Tensor.new
      

    - [ ] does std of an activation not change 
      after applying the mask?

*** TODO RNNDropout

    #+BEGIN_SRC python
      #export
      class RNNDropout(nn.Module):
          def __init__(self, p=0.5):
              super().__init__()
              self.p=p

          def forward(self, x):
              if not self.training or self.p == 0.: return x
              m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)
              return x * m
    #+END_SRC

    # forward
    - `RNNDropout` applys dropout to either 
      - input: a batch of sequences of embedding word vectors
      - output: a a batch of sequences of outputs

    - so, `RNNDropout` should be applied after converting
      a batch of sequences of NUMERICALIZED words
      to a batch of sequences of embedding word vectors

    - `forward` create `dropout_mask` instance 
      whose dimension is `(x.size(0), 1, x.size(2))`
      - `x.size(0)` is batch size
      - `x.size(2)` is the embedding size 

    - this way, `x * m` will be an tensor such that
      the position of 0s within an word vector
      is same across a row of the sigle batch)
      (across an element of the single batch)

       |     | word1      | word2      | ...        | word70     |
       |-----+------------+------------+------------+------------|
       |   1 | 2nd, 5th=0 | 2nd, 5th=0 | 2nd, 5th=0 | 2nd, 5th=0 |
       |-----+------------+------------+------------+------------|
       |   2 |            |            |            |            |
       |-----+------------+------------+------------+------------|
       | ... |            |            |            |            |
       |-----+------------+------------+------------+------------|
       |  64 | 1st=0      | 1st=0      | 1st=0      | 1st=0      |
        
    - [ ] what the point of masking part of a word vector
      https://medium.com/@bingobee01/a-review-of-dropout-as-applied-to-rnns-72e79ecd5b7b

*** WeightDropout

    #+BEGIN_SRC python
      #export
      import warnings

      WEIGHT_HH = 'weight_hh_l0'

      class WeightDropout(nn.Module):
          def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):
              super().__init__()
              self.module,self.weight_p,self.layer_names = module,weight_p,layer_names
              for layer in self.layer_names:
                  #Makes a copy of the weights of the selected layers.
                  w = getattr(self.module, layer)
                  self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))
                  self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training=False)

          def _setweights(self):
              for layer in self.layer_names:
                  raw_w = getattr(self, f'{layer}_raw')
                  self.module._parameters[layer] = F.dropout(raw_w, p=self.weight_p, training=self.training)

          def forward(self, *args):
              self._setweights()
              with warnings.catch_warnings():
                  #To avoid the warning that comes because the weights aren't flattened.
                  warnings.simplefilter("ignore")
                  return self.module.forward(*args)
    #+END_SRC

    - WeightDropout applys the dropout to the weights
      (not activations)

    - `WeightDropout` receives a module, 
      and calculates dropout-ed version of `forward` of the module

*** EmbeddingDropout

    #+BEGIN_SRC python
      #export
      class EmbeddingDropout(nn.Module):
          "Applies dropout in the embedding layer by zeroing out some elements of the embedding vector."
          def __init__(self, emb, embed_p):
              super().__init__()
              self.emb,self.embed_p = emb,embed_p
              self.pad_idx = self.emb.padding_idx
              if self.pad_idx is None: self.pad_idx = -1

          def forward(self, words, scale=None):
              if self.training and self.embed_p != 0:
                  size = (self.emb.weight.size(0),1)
                  mask = dropout_mask(self.emb.weight.data, size, self.embed_p)
                  masked_embed = self.emb.weight * mask
              else: masked_embed = self.emb.weight
              if scale: masked_embed.mul_(scale)
              return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,
                                 self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)
    #+END_SRC

    - `EmbeddingDropout` applys dropout to the embedding MATRIX,
      which converts a word to an embedding word vector

    - `EmbeddingDropout` accepts an embedding matrix,
      creates dropouted-ed version of the embedding matrix,
      and then converts numericalized words to a stack of
      embedding word vectors

    - `F.embedding` is just a look up which looks up
      the corresponding embedding word vector from
      an embedding matrix based on an word index.
      
      https://pytorch.org/docs/stable/nn.functional.html

*** to_detach

    #+BEGIN_SRC python
      #export
      def to_detach(h):
          "Detaches `h` from its history."
          return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)
    #+END_SRC

*** AWD_LSTM

    #+BEGIN_SRC python
      #export
      class AWD_LSTM(nn.Module):
          "AWD-LSTM inspired by https://arxiv.org/abs/1708.02182."
          initrange=0.1

          def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,
                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):
              super().__init__()
              self.bs,self.emb_sz,self.n_hid,self.n_layers = 1,emb_sz,n_hid,n_layers
              self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)
              self.emb_dp = EmbeddingDropout(self.emb, embed_p)
              self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,
                                   batch_first=True) for l in range(n_layers)]
              self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])
              self.emb.weight.data.uniform_(-self.initrange, self.initrange)
              self.input_dp = RNNDropout(input_p)
              self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])

          def forward(self, input):
              bs,sl = input.size()
              if bs!=self.bs:
                  self.bs=bs
                  self.reset()
              raw_output = self.input_dp(self.emb_dp(input))
              new_hidden,raw_outputs,outputs = [],[],[]
              for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):
                  raw_output, new_h = rnn(raw_output, self.hidden[l])
                  new_hidden.append(new_h)
                  raw_outputs.append(raw_output)
                  if l != self.n_layers - 1: raw_output = hid_dp(raw_output)
                  outputs.append(raw_output) 
              self.hidden = to_detach(new_hidden)
              return raw_outputs, outputs

          def _one_hidden(self, l):
              "Return one hidden state."
              nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz
              return next(self.parameters()).new(1, self.bs, nh).zero_()

          def reset(self):
              "Reset the hidden states."
              self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]
    #+END_SRC
    
    # init
    - `n_layers` specifies the number of `nn.LSTM` layers

    # reset, _one_hidden
    - `reset` initialize `self.hidden` by calling `_one_hidden`

    # forward
    - `input` is a batch of sequences of NUMERICALIZED words
      |     | word1 | word2 | ... | word70 |
      |-----+-------+-------+-----+--------|
      |   1 |       |       |     |        |
      |-----+-------+-------+-----+--------|
      |   2 |       |       |     |        |
      |-----+-------+-------+-----+--------|
      | ... |       |       |     |        |
      |-----+-------+-------+-----+--------|
      |  64 |       |       |     |        |
      
    - `sl` is the length of a sequence (e.g. 70) in a batch
      - when `AWD-LSTM` is called for laungage modeling,
        `sl` is same as `bptt`
      - when `AWD-LSTM` is called for classification,
        `sl` may be different from `bptt`

    - in the beggining, `reset` is called, which 
      sets the initial state (H0, C0)

    - input (a batch of sequences of NUMERICALIZED words)
      is converted to a batch of sequences of embedding
      word vectors using dropout-ed version of the
      embedding matrix

    - then, `input_dp` (`RNNDropout`) is applied 

    - at this point, `raw_output` looks like
      |   1 | (...) | (...) | ... | (...) | (...) |
      | ... | ...   |       |     | ...   | ...   |
      |  64 | (...) | (...) | ... | (...) | (...) |


    - then, a seriese of `nn.LSTM` is applied
      - `nn.LSTM::__call__` accepts 
        input (a batch of sequences of embedding word vectors),
        and state (Ht, Ct)
        https://pytorch.org/docs/master/generated/torch.nn.LSTM.html

    - for each output of a single `nn.LSTM` layer,
      `RNNDropout` layer (`hid_dp`) with dropout probaility
      `hidden_p` is also applied except for the last
      `nn.LSTM` layer.
      - for the output for the last `nn.LSTM` layer, 
        dropout with different dropout probaility `output_p`
        will be later applied by `LinearDecoder` layer which
        accepts the output for the last `nn.LSTM` layer.

    - for each `nn.LSTM` layer, output & the droput-ed output
      is recorded separately on list `row_outputs` & list `outputs`
      which contains output & the droput-ed output for
      all the `nn.LSTM` layers.

    - for each `nn.LSTM` layer, newly calculated state is
      also recorded separately on list `new_hidden` which contains
      states for all the `nn.LSTM` layers

    - `new_h` is truly a pair of (hidden state, cell state)

    - in the end, `new_hidden is stored on `self.hidden` 
      so that it will provide the initial hidden state 
      for each of `nn.LSTM` layer for the next batch.

    - the output will be like below;

      |     | prob dist1 | prob dist2 | ... | prob dist70 |
      |-----+------------+------------+-----+-------------|
      |   1 |            |            |     |             |
      |-----+------------+------------+-----+-------------|
      |   2 |            |            |     |             |
      |-----+------------+------------+-----+-------------|
      | ... |            |            |     |             |
      |-----+------------+------------+-----+-------------|
      |  64 |            |            |     |             |
      |-----+------------+------------+-----+-------------|

    - note that dropout is not applied to the very last output
      of `AWD-LSTM`, because dropout will be applied by
      the successing layer
      - for language modeling, dropout happens at `LinearDecoder` layer
      - for language classification, dropout happens at `PoolingLinearClassifier` layer

    # reset
    - `reset` creates a list of tuples, and each tuple corresponds to
      (hidden state, cell state)

    # _one_hidden
    - `_one_hidden` creates a hidden state which looks like
      |   1 | (...) |
      | ... | ...   |
      |  64 | (...) |

    - `next(self.parameters()).new(1, self.bs, nh).zero_()`
      creates a tensor filled by '1' with the size
      torch.Size([self.bs, nh])

*** LinearDecoder

    #+BEGIN_SRC python
      #export
      class LinearDecoder(nn.Module):
          def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):
              super().__init__()
              self.output_dp = RNNDropout(output_p)
              self.decoder = nn.Linear(n_hid, n_out, bias=bias)
              if bias: self.decoder.bias.data.zero_()
              if tie_encoder: self.decoder.weight = tie_encoder.weight
              else: init.kaiming_uniform_(self.decoder.weight)

          def forward(self, input):
              raw_outputs, outputs = input
              output = self.output_dp(outputs[-1]).contiguous()
              decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))
              return decoded, raw_outputs, outputs
    #+END_SRC

    # overview
    - `LinearDecoder` picks up the output for the last
      `nn.LSTM` layer in `AWD_LSTM` layer,
      applys RNN dropout.

    # init
    - note that `nn.Linear` creates a weight matrix 
      in the opposite way to what we expect, and
      the weight matrix for `nn.Linear(n_hid, n_out, bias=bias)` 
      is torch.Size([n_out, n_hid]) which is exactly the
      same as the size of the embedding matrix,
      so it is reasonale to do 
      `self.decoder.weight = tie_encoder.weight`

      (see lesson8, 1:41:30)

    # forward
    - `forward` accepts `raw_outputs` & `outputs` which 
      are returned by `AWD_LSTM` layer

    - then picks up the last element of `outputs`, which is
      the output of the last `nn.LSTM` layer in `AWD_LSTM`

    - then applies `output_dp` and makes a copy by `contiguous`
      https://pytorch.org/docs/stable/tensors.html#torch.Tensor.contiguous

    - the size of `output` looks like;
      torch.Size([64, 70, 300])
      - 64: batch size
      - 70: the length of the sequences
      - 300: the 'width' of hidden state

    - `output.view(output.size(0)*output.size(1), output.size(2))`
      reshapes `output` so that all the elements in a batch
      are aligned in one column

    - `decoder` is applied to reshaped `output`

    - `decoder` converts each element in the reshaped `output`
      to probatilities for words

    - `forward` returns 3 things;
      - `decoded`: a batch of sequence of numericalized words
      - raw_outputs: a list of raw outputs by `nn.LSTM` layers
      - outputs: a list of outputs by `nn.LSTM` layers

*** SequentialRNN

    #+BEGIN_SRC python
      #export
      class SequentialRNN(nn.Sequential):
          "A sequential module that passes the reset call to its children."
          def reset(self):
              for c in self.children():
                  if hasattr(c, 'reset'): c.reset()
    #+END_SRC

    - `SequentialRNN` is inheritting from `nn.Sequential`,
      so creates a sequence of layers

*** get_language_model
    #+BEGIN_SRC python
      #export
      def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6, 
                             embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):
          rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,
                             hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)
          enc = rnn_enc.emb if tie_weights else None
          return SequentialRNN(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))
    #+END_SRC

    - note that `get_language_model` is not dependent of 
      `bptt` which specifies the length of the sequence in a batch
      for language modeling

*** GradientClipping

    #+BEGIN_SRC python
      #export
      class GradientClipping(Callback):
          def __init__(self, clip=None): self.clip = clip
          def after_backward(self):
              if self.clip:  nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)
    #+END_SRC

*** RNNTrainer

    #+BEGIN_SRC python
      #export
      class RNNTrainer(Callback):
          def __init__(self, α, β): self.α,self.β = α,β
          
          def after_pred(self):
              #Save the extra outputs for later and only returns the true output.
              self.raw_out,self.out = self.pred[1],self.pred[2]
              self.run.pred = self.pred[0]
          
          def after_loss(self):
              #AR and TAR
              if self.α != 0.:  self.run.loss += self.α * self.out[-1].float().pow(2).mean()
              if self.β != 0.:
                  h = self.raw_out[-1]
                  if h.size(1)>1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean()
                      
          def begin_epoch(self):
              #Shuffle the texts at the beginning of the epoch
              if hasattr(self.dl.dataset, "batchify"): self.dl.dataset.batchify()
    #+END_SRC

    - `RNNTrainer` is for regularlization using the activation
      by the LSTM layer

    - `self.pred` refers `self.runner.pred` thanks to
      `__getattr__` defined on `Callback` ver3

      #+BEGIN_SRC python
        def __getattr__(self, k): return getattr(self.run, k)
      #+END_SRC

    - `self.runner.pred` is the returned value of the last layer
      of the laugnage model, which is a `LinearDecoder` layer,
      and consists of 3 elements;
      - pred[0]: `decoded`
      - pred[1]: `row_outputs`
      - pred[2]: `outputs`
        
          #+BEGIN_SRC python
      #export
      class LinearDecoder(nn.Module):
          def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias=True):
              super().__init__()
              self.output_dp = RNNDropout(output_p)
              self.decoder = nn.Linear(n_hid, n_out, bias=bias)
              if bias: self.decoder.bias.data.zero_()
              if tie_encoder: self.decoder.weight = tie_encoder.weight
              else: init.kaiming_uniform_(self.decoder.weight)

          def forward(self, input):
              raw_outputs, outputs = input
              output = self.output_dp(outputs[-1]).contiguous()
              decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))
              return decoded, raw_outputs, outputs
    #+END_SRC

    # after_pred
    - `self.out` (==self.pred[2] == `outputs` of `nn.LinearDecoder`)
      contains the outputs for all the `nn.LSTM` layer,
      so `self.out[-1]` is the output for the last `nn.LSTM` layer,
      which looks like torch.Size([64, 70, 300]) where
      - 64: batch size
      - 70: the length of sequence
      - 300: the 'width' of hidden state 

    # after_loss
    - `after_loss` penalizes the activation by the last `nn.LSTM` layer
      in `AWD_LSTM` layer of the language model

*** cross_entropy_flat, accuracy_flat

    #+BEGIN_SRC python
      #export
      def cross_entropy_flat(input, target):
          bs,sl = target.size()
          return F.cross_entropy(input.view(bs * sl, -1), target.view(bs * sl))

      def accuracy_flat(input, target):
          bs,sl = target.size()
          return accuracy(input.view(bs * sl, -1), target.view(bs * sl))
    #+END_SRC

    - `input` is a batch of sequence of probabilities which looks like

      |     | prob dist1 | prob dist2 | ... | prob dist70 |
      |-----+------------+------------+-----+-------------|
      |   1 |            |            |     |             |
      |-----+------------+------------+-----+-------------|
      |   2 |            |            |     |             |
      |-----+------------+------------+-----+-------------|
      | ... |            |            |     |             |
      |-----+------------+------------+-----+-------------|
      |  64 |            |            |     |             |
      |-----+------------+------------+-----+-------------|

      where
      - 64: batch size
      - 70: the length of sequence
      - the size of 'prob dist' is torch.Size([1, vocab_size])

** 12b_lm_pretrain.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # https://forums.fast.ai/t/couldnt-import-apex-fp16-utils-in-10c-fp16-ipynb/69621
      !pip install git+https://github.com/NVIDIA/apex


      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
      !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/
      !cp course-v3/nbs/dl2/10b_mixup_label_smoothing.ipynb /content/
      !cp course-v3/nbs/dl2/10c_fp16.ipynb /content/
      !cp course-v3/nbs/dl2/11_train_imagenette.ipynb /content/
      !cp course-v3/nbs/dl2/11a_transfer_learning.ipynb /content/
      !cp course-v3/nbs/dl2/12_text.ipynb /content/
      !cp course-v3/nbs/dl2/12a_awd_lstm.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb
      !python notebook2script.py 10_augmentation.ipynb
      !python notebook2script.py 10b_mixup_label_smoothing.ipynb
      !python notebook2script.py 10c_fp16.ipynb
      !python notebook2script.py 11_train_imagenette.ipynb
      !python notebook2script.py 11a_transfer_learning.ipynb
      !python notebook2script.py 12_text.ipynb
      !python notebook2script.py 12a_awd_lstm.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
      !cp /content/exp/nb_10.py /content/exp.nb_10.py
      !cp /content/exp/nb_10b.py /content/exp.nb_10b.py
      !cp /content/exp/nb_10c.py /content/exp.nb_10c.py
      !cp /content/exp/nb_11.py /content/exp.nb_11.py
      !cp /content/exp/nb_11a.py /content/exp.nb_11a.py
      !cp /content/exp/nb_12.py /content/exp.nb_12.py
      !cp /content/exp/nb_12a.py /content/exp.nb_12a.py

    #+END_SRC

*** istitle

    #+BEGIN_SRC python
      def istitle(line):
          return len(re.findall(r'^ = [^=]* = $', line)) != 0
    #+END_SRC

*** read_wiki

    #+BEGIN_SRC python
      def read_wiki(filename):
          articles = []
          with open(filename, encoding='utf8') as f:
              lines = f.readlines()
          current_article = ''
          for i,line in enumerate(lines):
              current_article += line
              if i < len(lines)-2 and lines[i+1] == ' \n' and istitle(lines[i+2]):
                  current_article = current_article.replace('<unk>', UNK)
                  articles.append(current_article)
                  current_article = ''
          current_article = current_article.replace('<unk>', UNK)
          articles.append(current_article)
          return articles
    #+END_SRC

*** [language modeling] create a data for pre-training a language model from wiki text

    #+BEGIN_SRC python
      # read wiki text
      path = datasets.Config().data_path()/'wikitext-103'

      # split wiki text into training & validation set 
      train = TextList(read_wiki(path/'train.txt'), path=path) #+read_file(path/'test.txt')
      valid = TextList(read_wiki(path/'valid.txt'), path=path)

      # store training & validation set
      sd = SplitData(train, valid)

      # tokenize & numericalize the texts, and give fake labels for language modeling
      proc_tok,proc_num = TokenizeProcessor(),NumericalizeProcessor()
      ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])

      # save and reload `ll` for later use
      pickle.dump(ll, open(path/'ld.pkl', 'wb'))
      ll = pickle.load( open(path/'ld.pkl', 'rb'))

      # create Databunch for language modeling 
      bs,bptt = 128,70
      data = lm_databunchify(ll, bs, bptt)


    #+END_SRC

*** [language modeling] create & pre-train & save a language model

    #+BEGIN_SRC python
            #
            dps = np.array([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.2
            tok_pad = vocab.index(PAD)

            #
            emb_sz, nh, nl = 300, 300, 2
            model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps)

            #
            cbs = [partial(AvgStatsCallback,accuracy_flat),
                   CudaCallback, Recorder,
                   partial(GradientClipping, clip=0.1),
                   partial(RNNTrainer, α=2., β=1.),
                   ProgressCallback]

            #
            learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbs, opt_func=adam_opt())

            #
            lr = 5e-3
            sched_lr  = combine_scheds([0.3,0.7], cos_1cycle_anneal(lr/10., lr, lr/1e5))
            sched_mom = combine_scheds([0.3,0.7], cos_1cycle_anneal(0.8, 0.7, 0.8))
            cbsched = [ParamScheduler('lr', sched_lr), ParamScheduler('mom', sched_mom)]

            # train the model
            learn.fit(10, cbs=cbsched)

            # save weights 
            torch.save(learn.model.state_dict(), path/'pretrained.pth')

            # save vocabs as a pickle
            pickle.dump(vocab, open(path/'vocab.pkl', 'wb'))
    #+END_SRC

    - torch.save
      `torch.save` saves an object to a file
      https://pytorch.org/docs/master/generated/torch.save.html

    - .pth
      https://realpython.com/lessons/module-search-path/

    - which of `torch.save` & `pickle.dump` to use
      https://forums.fast.ai/t/which-format-is-better-for-pytorch-saved-model/5018/3
      https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch

** 12c_ulmfit.ipynb
*** imports

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # https://forums.fast.ai/t/couldnt-import-apex-fp16-utils-in-10c-fp16-ipynb/69621
      !pip install git+https://github.com/NVIDIA/apex


      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
      !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/
      !cp course-v3/nbs/dl2/10b_mixup_label_smoothing.ipynb /content/
      !cp course-v3/nbs/dl2/10c_fp16.ipynb /content/
      !cp course-v3/nbs/dl2/11_train_imagenette.ipynb /content/
      !cp course-v3/nbs/dl2/11a_transfer_learning.ipynb /content/
      !cp course-v3/nbs/dl2/12_text.ipynb /content/
      !cp course-v3/nbs/dl2/12a_awd_lstm.ipynb /content/
      !cp course-v3/nbs/dl2/12b_lm_pretrain.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb
      !python notebook2script.py 10_augmentation.ipynb
      !python notebook2script.py 10b_mixup_label_smoothing.ipynb
      !python notebook2script.py 10c_fp16.ipynb
      !python notebook2script.py 11_train_imagenette.ipynb
      !python notebook2script.py 11a_transfer_learning.ipynb
      !python notebook2script.py 12_text.ipynb
      !python notebook2script.py 12a_awd_lstm.ipynb
      !python notebook2script.py 12b_lm_pretrain.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
      !cp /content/exp/nb_10.py /content/exp.nb_10.py
      !cp /content/exp/nb_10b.py /content/exp.nb_10b.py
      !cp /content/exp/nb_10c.py /content/exp.nb_10c.py
      !cp /content/exp/nb_11.py /content/exp.nb_11.py
      !cp /content/exp/nb_11a.py /content/exp.nb_11a.py
      !cp /content/exp/nb_12.py /content/exp.nb_12.py
      !cp /content/exp/nb_12a.py /content/exp.nb_12a.py
      !cp /content/exp/nb_12b.py /content/exp.nb_12b.py

    #+END_SRC

*** [language modeling] create a Databunch & vocabs from domain specific texts (IMDB)

    #+BEGIN_SRC python
      # untar IMDB data
      path = datasets.untar_data(datasets.URLs.IMDB)

      # ll_lm.pkl does not exist, so we create `ll` manually
      # ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))

      il = TextList.from_files(path, include=['train', 'test', 'unsup'])
      sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))
      proc_tok,proc_num = TokenizeProcessor(max_workers=8),NumericalizeProcessor()
      ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok,proc_num])

      #
      bs,bptt = 128,70
      data = lm_databunchify(ll, bs, bptt)

      #
      vocab = ll.train.proc_x[1].vocab

    #+END_SRC

*** match_embeds

    #+BEGIN_SRC python
      def match_embeds(old_wgts, old_vocab, new_vocab):
          wgts = old_wgts['0.emb.weight']
          bias = old_wgts['1.decoder.bias']
          wgts_m,bias_m = wgts.mean(dim=0),bias.mean()
          new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1))
          new_bias = bias.new_zeros(len(new_vocab))
          otoi = {v:k for k,v in enumerate(old_vocab)}
          for i,w in enumerate(new_vocab): 
              if w in otoi:
                  idx = otoi[w]
                  new_wgts[i],new_bias[i] = wgts[idx],bias[idx]
              else: new_wgts[i],new_bias[i] = wgts_m,bias_m
          old_wgts['0.emb.weight']    = new_wgts
          old_wgts['0.emb_dp.emb.weight'] = new_wgts
          old_wgts['1.decoder.weight']    = new_wgts
          old_wgts['1.decoder.bias']      = new_bias
          return old_wgts
    #+END_SRC

    - `match_embeds` is for updating the embedding matrix
      and the weight of the `LinearDecoder`
      of a language model

*** [language modeling] update embedding matrix & vocas of the pretrained model

    #+BEGIN_SRC python
      # 
      dps = tensor([0.1, 0.15, 0.25, 0.02, 0.2]) * 0.5
      tok_pad = vocab.index(PAD)

      #
      emb_sz, nh, nl = 300, 300, 2
      model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, *dps)

      # read weights for the pretrained model
      old_wgts  = torch.load(path/'pretrained'/'pretrained.pth')

      # read vocabs for the pretrained model
      old_vocab = pickle.load(open(path/'pretrained'/'vocab.pkl', 'rb'))

      # update the embedding matrix & vocabs
      wgts = match_embeds(old_wgts, old_vocab, vocab)

      # load the new weights & new vocabs
      model.load_state_dict(wgts)

      # model summary
      '''
      SequentialRNN(
        (0): AWD_LSTM(
          (emb): Embedding(60003, 300, padding_idx=2)
          (emb_dp): EmbeddingDropout(
            (emb): Embedding(60003, 300, padding_idx=2)
          )
          (rnns): ModuleList(
            (0): WeightDropout(
              (module): LSTM(300, 300, batch_first=True)
            )
            (1): WeightDropout(
              (module): LSTM(300, 300, batch_first=True)
            )
          )
          (input_dp): RNNDropout()
          (hidden_dps): ModuleList(
            (0): RNNDropout()
            (1): RNNDropout()
          )
        )
        (1): LinearDecoder(
          (output_dp): RNNDropout()
          (decoder): Linear(in_features=300, out_features=60003, bias=True)
        )
      )
      '''


    #+END_SRC

*** lm_splitter

    #+BEGIN_SRC python
      def lm_splitter(m):
          groups = []
          for i in range(len(m[0].rnns)): groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))
          groups += [nn.Sequential(m[0].emb, m[0].emb_dp, m[0].input_dp, m[1])]
          return [list(o.parameters()) for o in groups]
    #+END_SRC

    - `lm_splitter` is for creating layer groups for discriminative learning rate

    - the 1st group cotains
      - m[0].rnns[i]
      - m[0].hidden_dps[i]

    - the 2nd group contains 
      # AWD_LSTM embedding, 
      - m[0].emb
      - m[0].emb_dp
      - m[0].input_dp

      # LinearDecoder
      - m[1]



*** [language modeling] train the pretrained language modeling with domain specific text data

    #+BEGIN_SRC python
      # model summary
      '''
      SequentialRNN(
        (0): AWD_LSTM(
          (emb): Embedding(60003, 300, padding_idx=2)
          (emb_dp): EmbeddingDropout(
            (emb): Embedding(60003, 300, padding_idx=2)
          )
          (rnns): ModuleList(
            (0): WeightDropout(
              (module): LSTM(300, 300, batch_first=True)
            )
            (1): WeightDropout(
              (module): LSTM(300, 300, batch_first=True)
            )
          )
          (input_dp): RNNDropout()
          (hidden_dps): ModuleList(
            (0): RNNDropout()
            (1): RNNDropout()
          )
        )
        (1): LinearDecoder(
          (output_dp): RNNDropout()
          (decoder): Linear(in_features=300, out_features=60003, bias=True)
        )
      )
      '''

      # freeze rnn parameters
      for rnn in model[0].rnns:
          for p in rnn.parameters(): p.requires_grad_(False)

      #
      cbs = [partial(AvgStatsCallback,accuracy_flat),
             CudaCallback, Recorder,
             partial(GradientClipping, clip=0.1),
             partial(RNNTrainer, α=2., β=1.),
             ProgressCallback]

      # create a `Learner` with discriminative learning rate
      learn = Learner(model, data, cross_entropy_flat, opt_func=adam_opt(),
                      cb_funcs=cbs, splitter=lm_splitter)

      #
      lr = 2e-2
      cbsched = sched_1cycle([lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)

      # train layers excluding `model[0].rnns`
      learn.fit(1, cbs=cbsched)

      # unfreeze `model[0].rnns`
      for rnn in model[0].rnns:
          for p in rnn.parameters(): p.requires_grad_(True)

      #
      lr = 2e-3
      cbsched = sched_1cycle([lr/2., lr/2., lr], pct_start=0.5, mom_start=0.8, mom_mid=0.7, mom_end=0.8)

      # train the entire layers
      learn.fit(10, cbs=cbsched)

      # save the encoding part of the finetuned language model
      torch.save(learn.model[0].state_dict(), path/'finetuned_enc.pth')

      # save the new vocabs
      pickle.dump(vocab, open(path/'vocab_lm.pkl', 'wb'))

      # save the entire finetuned language modeling
      torch.save(learn.model.state_dict(), path/'finetuned.pth')

    #+END_SRC
    
    - note that for classification, we do not neet `LearnerDecoder`
      part so we do not really have to save that part

*** [classification] create a Databunch for classification

    #+BEGIN_SRC python
      #
      vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))
      proc_tok,proc_num,proc_cat = TokenizeProcessor(),NumericalizeProcessor(vocab=vocab),CategoryProcessor()

      # load training & validation data for classification
      il = TextList.from_files(path, include=['train', 'test'])
      sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))
      ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)

      # save & load `ll` for classification
      pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))
      ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))

      # create a dabatunch for classification
      bs,bptt = 64,70
      data = clas_databunchify(ll, bs)
    #+END_SRC

*** TODO pack_padded_sequence
    https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html
    https://www.programcreek.com/python/example/91300/torch.nn.utils.rnn.pack_padded_sequence

    https://pytorch.org/docs/master/generated/torch.nn.utils.rnn.pack_padded_sequence.html
    https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch

    - [ ] 

*** check pack_padded_sequence works as expected

    #+BEGIN_SRC python
      #export
      from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

      #
      x,y = next(iter(data.train_dl))

      #
      x.size()

      #
      lengths = x.size(1) - (x == 1).sum(1)
      lengths[:5]

      #
      tst_emb = nn.Embedding(len(vocab), 300)

      #
      packed = pack_padded_sequence(tst_emb(x), lengths, batch_first=True)

      #
      tst = nn.LSTM(300, 300, 2)
      y,h = tst(packed)
      unpack = pad_packed_sequence(y, batch_first=True)
      unpack[0].shape

      unpack[1]

      #
      emb_sz, nh, nl = 300, 300, 2
      tok_pad = vocab.index(PAD)

      #
      enc = AWD_LSTM1(len(vocab), emb_sz, n_hid=nh, n_layers=nl, pad_token=tok_pad)
      pool = Pooling()
      enc.bs = bs
      enc.reset()

      #
      x,y = next(iter(data.train_dl))
      output,c = pool(enc(x))
    #+END_SRC

    - `x` looks like

      |   1 | 433 |  432 | ... |   1 |   1 |
      | ... | ... |      |     | ... | ... |
      |  64 | 436 | 4378 | ... | 987 |   1 |

    - so `(x == 1)

      |   1 | false | false | ... | true  | true |
      | ... | ...   |       |     | ...   | ...  |
      |  64 | false | false | ... | false | true |

    - `(x == 1).sum(1)` looks 

      |   1 |   3 |
      | ... | ... |
      |  64 |   1 |

*** TODO AWD_LSTM1

    #+BEGIN_SRC python
      #export
      class AWD_LSTM1(nn.Module):
          "AWD-LSTM inspired by https://arxiv.org/abs/1708.02182."
          initrange=0.1

          def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,
                       hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):
              super().__init__()
              self.bs,self.emb_sz,self.n_hid,self.n_layers,self.pad_token = 1,emb_sz,n_hid,n_layers,pad_token
              self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)
              self.emb_dp = EmbeddingDropout(self.emb, embed_p)
              self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,
                                   batch_first=True) for l in range(n_layers)]
              self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])
              self.emb.weight.data.uniform_(-self.initrange, self.initrange)
              self.input_dp = RNNDropout(input_p)
              self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])

          def forward(self, input):
              bs,sl = input.size()
              mask = (input == self.pad_token)
              lengths = sl - mask.long().sum(1)
              n_empty = (lengths == 0).sum()
              if n_empty > 0:
                  input = input[:-n_empty]
                  lengths = lengths[:-n_empty]
                  self.hidden = [(h[0][:,:input.size(0)], h[1][:,:input.size(0)]) for h in self.hidden]
              raw_output = self.input_dp(self.emb_dp(input))
              new_hidden,raw_outputs,outputs = [],[],[]
              for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):
                  raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True)
                  raw_output, new_h = rnn(raw_output, self.hidden[l])
                  raw_output = pad_packed_sequence(raw_output, batch_first=True)[0]
                  raw_outputs.append(raw_output)
                  if l != self.n_layers - 1: raw_output = hid_dp(raw_output)
                  outputs.append(raw_output)
                  new_hidden.append(new_h)
              self.hidden = to_detach(new_hidden)
              return raw_outputs, outputs, mask

          def _one_hidden(self, l):
              "Return one hidden state."
              nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz
              return next(self.parameters()).new(1, self.bs, nh).zero_()

          def reset(self):
              "Reset the hidden states."
              self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]
    #+END_SRC


    # forward
    - `mask` is a tensor which looks like

      |   1 | false | false | ... | true  | true |
      | ... | ...   |       |     | ...   | ...  |
      |  64 | false | false | ... | false | true |

    - 


    - `forward` is modified so that it removes padding from an input so that rnn (`nn.LSTM` instance)
      does not have to perform calculation for padding part

    - in concrete, `raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True)`
      removes the paddings

    - note that rnn (`nn.LSTM` instance) works fine regardless of the length of input sentence

*** Pooling

    #+BEGIN_SRC python
      class Pooling(nn.Module):
          def forward(self, input):
              raw_outputs,outputs,mask = input
              output = outputs[-1]
              lengths = output.size(1) - mask.long().sum(dim=1)
              avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)
              avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])
              max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]
              x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.
              return output,x
    #+END_SRC

    - `output` is the output of the last `nn.LSTM` layer, 
      so it is a rank3 tensor which looks like

      |   1 | (...) | (...) | ... | (...) | (...) |
      | ... | ...   |       |     | ...   | ...   |
      |  64 | (...) | (...) | ... | (...) | (...) |

    - `output` is originally the output for language modeling
      for predicting next words so we do not directly use it
      for classification.

    - for classification, we use 3 things out of `output`
      - the last element of the sequence
      - average in the sequence
      - maximum element of the sequence

    - `outpu.size(1)` is a scalar

    - `mask` is a tensor which looks like
      |   1 | false | false | ... | true  | true |
      | ... | ...   |       |     | ...   | ...  |
      |  64 | false | false | ... | false | true |

    - `mask.long().sum(dim=1) looks like
      |   1 |   3 |
      | ... | ... |
      |  64 |   1 |

    - `output.size(1) - mask.long().sum(dim=1)`
      is (scalar) - (tensor), so broadcasting happens

      #+BEGIN_SRC python
        # memo
        import torch
        my_output = tensor([[1,2,3],[4,5,6],[7,8,9]])
        my_mask = tensor([[False, False, True], [False, False, False], [False,True, True]])
        my_sum = my_mask.long().sum(dim=1)
        my_output.size(1) - my_sum

        '''
        tensor([2, 3, 1])
        '''
      #+END_SRC

    - as a result, `lengths` is a tensor which looks like
      |   1 | 4327 |
      | ... |  ... |
      |  64 | 4328 |

    - `output.masked_fill` fill elements of `output` with 0
      where `mask` is true.
      https://pytorch.org/docs/stable/tensors.html

    - since `output` is a rank3 tensor whereas `mask` is
      a rank2 tensor, we need to unsqueeze `mask`
      so that the ranks will match

    - `mask[:,:,None]` does unsqueeze, and
      it inserts an unit axis to the last dimension.

      #+BEGIN_SRC python
        #memo
        import torch
        my_tensor = torch.tensor([[[1,2],[3,4],[5,6]],
                     [[11,12],[13,14],[15,16]],
                     [[21,22],[23,24],[25,26]]])
        my_tensor2 = my_tensor[:,:,None]
        my_tensor2

        '''
        tensor([[[[ 1,  2]],

                 [[ 3,  4]],

                 [[ 5,  6]]],


                [[[11, 12]],

                 [[13, 14]],

                 [[15, 16]]],


                [[[21, 22]],

                 [[23, 24]],

                 [[25, 26]]]])
        '''
      #+END_SRC

    - the result of `output.masked_fill(mask[:,:,None], 0)` 
      looks like

      |   1 | (...) | (...) | ... | (0,0,...,0) | (0,0,...,0) |
      | ... | ...   |       |     | ...         | ...         |
      |  64 | (...) | (...) | ... | (...)       | (0,0,...,0) |
      
      
    - `output.masked_fill(mask[:,:,None], 0).sum(dim=1) 
      perform summation along 'sequence' axis

    #+BEGIN_SRC python
      #memo
      import torch
      my_tensor = torch.tensor([[[1,2],[3,4],[5,6]],
                   [[11,12],[13,14],[15,16]],
                   [[21,22],[23,24],[25,26]]])
      my_tensor_dim0 = my_tensor.sum(dim=0)
      my_tensor_dim1 = my_tensor.sum(dim=1)
      my_tensor_dim2 = my_tensor.sum(dim=2)
      my_tensor, my_tensor_dim0, my_tensor_dim1, my_tensor_dim2

      '''
      # my_tensor
      tensor([[[ 1,  2],
               [ 3,  4],
               [ 5,  6]],

              [[11, 12],
               [13, 14],
               [15, 16]],

              [[21, 22],
               [23, 24],
               [25, 26]]])

      # my_tensor_dim0
      tensor([[33, 36],
              [39, 42],
              [45, 48]])

      # my_tensor_dim1
      tensor([[ 9, 12],
              [39, 42],
              [69, 72]])

      # my_tensor_dim2
      tensor([[ 3,  7, 11],
              [23, 27, 31],
              [43, 47, 51]])

      '''
    #+END_SRC

    - `output` is 

*** bn_drop_lin

    #+BEGIN_SRC python
      def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):
          layers = [nn.BatchNorm1d(n_in)] if bn else []
          if p != 0: layers.append(nn.Dropout(p))
          layers.append(nn.Linear(n_in, n_out))
          if actn is not None: layers.append(actn)
          return layers
    #+END_SRC

    # overview
    - `bn_drop_lin` creates a list of layers consists of
      - a batch norm layer
      - a dropout layer
      - a linear layer
      - an relu layer

    - note `actn` can be None

*** TODO PoolingLinearClassifier

    #+BEGIN_SRC python
      class PoolingLinearClassifier(nn.Module):
          "Create a linear classifier with pooling."

          def __init__(self, layers, drops):
              super().__init__()
              mod_layers = []
              activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]
              for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):
                  mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)
              self.layers = nn.Sequential(*mod_layers)

          def forward(self, input):
              raw_outputs,outputs,mask = input
              output = outputs[-1]
              lengths = output.size(1) - mask.long().sum(dim=1)
              avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)
              avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])
              max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]
              x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.
              x = self.layers(x)
              return x
    #+END_SRC

    # overview
    - `PoolingLinearClassifier` is a layer which recieves
      the output of `nn.AWD-LSTM` layer and ouputs a
      tensor for language classification
    
    # __init__
    - `layers` specifies the 'width' of activations after
      each linear layer

    - the last element of `activs` is `None`,
      - `len(layers) - 2` is equal to number of 
        `mod_layers` miunus 1
      - `activs` consists of `ReLU` layers and
        one `None` layer


    # __forward__
    - `PoolingLinearClassifier::forward` does
      what `Pooling::forward` does plus
      applying `layers` which is a seriese of
      (batch norm, dropout, linear, relu)

    - [ ] the last layer is `None` instead of `ReLU` layer

*** pad_tensor

    #+BEGIN_SRC python
      def pad_tensor(t, bs, val=0.):
          if t.size(0) < bs:
              return torch.cat([t, val + t.new_zeros(bs-t.size(0), *t.shape[1:])])
          return t
    #+END_SRC

*** SentenceEncoder

    #+BEGIN_SRC python
      class SentenceEncoder(nn.Module):
          def __init__(self, module, bptt, pad_idx=1):
              super().__init__()
              self.bptt,self.module,self.pad_idx = bptt,module,pad_idx

          def concat(self, arrs, bs):
              return [torch.cat([pad_tensor(l[si],bs) for l in arrs], dim=1) for si in range(len(arrs[0]))]
          
          def forward(self, input):
              bs,sl = input.size()
              self.module.bs = bs
              self.module.reset()
              raw_outputs,outputs,masks = [],[],[]
              for i in range(0, sl, self.bptt):
                  r,o,m = self.module(input[:,i: min(i+self.bptt, sl)])
                  masks.append(pad_tensor(m, bs, 1))
                  raw_outputs.append(r)
                  outputs.append(o)
              return self.concat(raw_outputs, bs),self.concat(outputs, bs),torch.cat(masks,dim=1)
    #+END_SRC

    # overview
    - `SentenceEncoder` is a wrapper module to apply
      `AWD-LSTM1` layer to a batch chunk by chunk 
      to avoid out of memory error

    # __init__
    - `module` is an `AWD-LSTM1` layer

    - `bptt` is the length of the sequence of words for 
      language modeling
    

    # forward
    - `sl` is the length of the sequence of words for 
      classification, and varies from a batch to a batch

    - since `sl` is the length of a single document,
      it is usually much larger than `bptt`

    - for a single batch, `SentenceEncoder::forward` 
      splits the batch to smaller pieces of size `bptt`
      and then apply `forward` of `module` (`AWD-LSTM1`)
      to avoid out of memory error
      - `sl` is usually too large for `AWD-LSTM1` layer
        to perform computation at once

      - since `bptt` is used when running `AWD-LSTM1`
        for language modeling, it should work

      - in the end, it concatnates the resulting list
        getting the equivalent result as applying
        `AWD-LSTM1` layer to the batch at one go.

    - `range(0, sl, self.bptt)` sweep from 0 to sl
      with step of self.bptt
      - ref
        https://www.geeksforgeeks.org/python-range-function/

      - for example, `range(0, 1000, 70)` sweeps from 0 to 1000
        with a step of 70, so it goes
        0, 70, 140, 210, ..., 910, 980

    - after sweeping through the small chunks created from a 
      single batch, `forward` concats `row_outputs`
      
      https://pytorch.org/docs/stable/torch.html

*** get_text_classifier

    #+BEGIN_SRC python
      def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, output_p=0.4, hidden_p=0.2, 
                              input_p=0.6, embed_p=0.1, weight_p=0.5, layers=None, drops=None):
          "To create a full AWD-LSTM"
          rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,
                              hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)
          enc = SentenceEncoder(rnn_enc, bptt)
          if layers is None: layers = [50]
          if drops is None:  drops = [0.1] * len(layers)
          layers = [3 * emb_sz] + layers + [n_out] 
          drops = [output_p] + drops
          return SequentialRNN(enc, PoolingLinearClassifier(layers, drops))
    #+END_SRC

    - `bptt` is the length of the sequence of words for 
      language modeling

    - the created text classifier consists of
      - encoder by `AWD_LSTM1` layer wrapped by SentenceEncoder

      - a seriese of (batch norm, dropout, linear, ReLU)
        which is created by `PoolingLinearClassifier`

      - the first dropout 

      - `layers` specifies the 'width' of the output of
        each linear layer in `PoolingLinearClassifier`,
        and looks like;
        [900, 50, 2]

*** class_splitter

    #+BEGIN_SRC python
      def class_splitter(m):
          enc = m[0].module
          groups = [nn.Sequential(enc.emb, enc.emb_dp, enc.input_dp)]
          for i in range(len(enc.rnns)): groups.append(nn.Sequential(enc.rnns[i], enc.hidden_dps[i]))
          groups.append(m[1])
          return [list(o.parameters()) for o in groups]
    #+END_SRC

*** [classification] create a model for classification

    #+BEGIN_SRC python
      #export
      from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence

      # create a model for classification
      emb_sz, nh, nl = 300, 300, 2
      dps = tensor([0.4, 0.3, 0.4, 0.05, 0.5]) * 0.25
      model = get_text_classifier(len(vocab), emb_sz, nh, nl, 2, 1, bptt, *dps)

    #+END_SRC

*** [classification] train the model for classification

    #+BEGIN_SRC python

      # freeze all the layers(emb, emb_dp, rnns, input_dp, hidden_dps) of the encoder 
      for p in model[0].parameters(): p.requires_grad_(False)

      #
      cbs = [partial(AvgStatsCallback,accuracy),
             CudaCallback, Recorder,
             partial(GradientClipping, clip=0.1),
             ProgressCallback]

      # for encoder part, load the weight of the pretrained language model
      model[0].module.load_state_dict(torch.load(path/'finetuned_enc.pth'))

      #
      learn = Learner(model, data, F.cross_entropy, opt_func=adam_opt(), cb_funcs=cbs, splitter=class_splitter)

      #
      lr = 1e-2
      cbsched = sched_1cycle([lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)

      # train the `PoolingLinearClassifier` layer
      learn.fit(1, cbs=cbsched)

      # unfreeze the last rnn layer
      for p in model[0].module.rnns[-1].parameters(): p.requires_grad_(True)

      #
      lr = 5e-3
      cbsched = sched_1cycle([lr/2., lr/2., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)

      # train the last rnn layer
      learn.fit(1, cbs=cbsched)

      # unfreeze all the layer of the encoder
      for p in model[0].parameters(): p.requires_grad_(True)

      #
      lr = 1e-3
      cbsched = sched_1cycle([lr/8., lr/4., lr/2., lr], mom_start=0.8, mom_mid=0.7, mom_end=0.8)

      # train the entire layers
      learn.fit(2, cbs=cbsched)

    #+END_SRC


* audio lesson
*** install notebooks

    #+BEGIN_SRC python
      # importing nb00, nb_01, nb02, nb03, nb04, nb05, nb05b, nb06
      # install the python dependency necessary for the converting script
      !pip install fire

      # https://forums.fast.ai/t/couldnt-import-apex-fp16-utils-in-10c-fp16-ipynb/69621
      !pip install git+https://github.com/NVIDIA/apex

      # git clone the entire course material under /content
      !git clone https://github.com/fastai/course-v3.git

      # copy the ipynb2py converting python script
      !cp course-v3/nbs/dl2/notebook2script.py /content/

      # copy the jupyter notebooks to convert, under /content
      !cp course-v3/nbs/dl2/00_exports.ipynb /content/
      !cp course-v3/nbs/dl2/01_matmul.ipynb /content/
      !cp course-v3/nbs/dl2/02_fully_connected.ipynb /content/
      !cp course-v3/nbs/dl2/03_minibatch_training.ipynb /content/
      !cp course-v3/nbs/dl2/04_callbacks.ipynb /content/
      !cp course-v3/nbs/dl2/05_anneal.ipynb /content/
      !cp course-v3/nbs/dl2/05b_early_stopping.ipynb /content/
      !cp course-v3/nbs/dl2/06_cuda_cnn_hooks_init.ipynb /content/
      !cp course-v3/nbs/dl2/07_batchnorm.ipynb /content/
      !cp course-v3/nbs/dl2/07a_lsuv.ipynb /content/
      !cp course-v3/nbs/dl2/08_data_block.ipynb /content/
      !cp course-v3/nbs/dl2/09_optimizers.ipynb /content/
      !cp course-v3/nbs/dl2/09b_learner.ipynb /content/
      !cp course-v3/nbs/dl2/09c_add_progress_bar.ipynb /content/
      !cp course-v3/nbs/dl2/10_augmentation.ipynb /content/
      !cp course-v3/nbs/dl2/10b_mixup_label_smoothing.ipynb /content/
      !cp course-v3/nbs/dl2/10c_fp16.ipynb /content/
      !cp course-v3/nbs/dl2/11_train_imagenette.ipynb /content/
      !cp course-v3/nbs/dl2/11a_transfer_learning.ipynb /content/
      !cp course-v3/nbs/dl2/12_text.ipynb /content/
      !cp course-v3/nbs/dl2/12a_awd_lstm.ipynb /content/
      !cp course-v3/nbs/dl2/12b_lm_pretrain.ipynb /content/

      # convert the jupyter notebooks into python modules
      !python notebook2script.py 00_exports.ipynb
      !python notebook2script.py 01_matmul.ipynb
      !python notebook2script.py 02_fully_connected.ipynb
      !python notebook2script.py 03_minibatch_training.ipynb
      !python notebook2script.py 04_callbacks.ipynb
      !python notebook2script.py 05_anneal.ipynb
      !python notebook2script.py 05b_early_stopping.ipynb
      !python notebook2script.py 06_cuda_cnn_hooks_init.ipynb
      !python notebook2script.py 07_batchnorm.ipynb
      !python notebook2script.py 07a_lsuv.ipynb
      !python notebook2script.py 08_data_block.ipynb
      !python notebook2script.py 09_optimizers.ipynb
      !python notebook2script.py 09b_learner.ipynb
      !python notebook2script.py 09c_add_progress_bar.ipynb
      !python notebook2script.py 10_augmentation.ipynb
      !python notebook2script.py 10b_mixup_label_smoothing.ipynb
      !python notebook2script.py 10c_fp16.ipynb
      !python notebook2script.py 11_train_imagenette.ipynb
      !python notebook2script.py 11a_transfer_learning.ipynb
      !python notebook2script.py 12_text.ipynb
      !python notebook2script.py 12a_awd_lstm.ipynb
      !python notebook2script.py 12b_lm_pretrain.ipynb

      # copy the converted python modules under /content for later use
      !cp /content/exp/nb_00.py /content/exp.nb_00.py
      !cp /content/exp/nb_01.py /content/exp.nb_01.py
      !cp /content/exp/nb_02.py /content/exp.nb_02.py
      !cp /content/exp/nb_03.py /content/exp.nb_03.py
      !cp /content/exp/nb_04.py /content/exp.nb_04.py
      !cp /content/exp/nb_05.py /content/exp.nb_05.py
      !cp /content/exp/nb_05b.py /content/exp.nb_05b.py
      !cp /content/exp/nb_06.py /content/exp.nb_06.py
      !cp /content/exp/nb_07.py /content/exp.nb_07.py
      !cp /content/exp/nb_07a.py /content/exp.nb_07a.py
      !cp /content/exp/nb_08.py /content/exp.nb_08.py
      !cp /content/exp/nb_09.py /content/exp.nb_09.py
      !cp /content/exp/nb_09b.py /content/exp.nb_09b.py
      !cp /content/exp/nb_09c.py /content/exp.nb_09c.py
      !cp /content/exp/nb_10.py /content/exp.nb_10.py
      !cp /content/exp/nb_10b.py /content/exp.nb_10b.py
      !cp /content/exp/nb_10c.py /content/exp.nb_10c.py
      !cp /content/exp/nb_11.py /content/exp.nb_11.py
      !cp /content/exp/nb_11a.py /content/exp.nb_11a.py
      !cp /content/exp/nb_12.py /content/exp.nb_12.py
      !cp /content/exp/nb_12a.py /content/exp.nb_12a.py
      !cp /content/exp/nb_12b.py /content/exp.nb_12b.py
    #+END_SRC

*** install torchaudio

    #+BEGIN_SRC python
      !pip install torchaudio
    #+END_SRC


    # works
    https://github.com/pytorch/audio/issues/356

    # not works
    https://discuss.pytorch.org/t/installing-torchaudio-on-google-colab/18512/2

*** import exp.nb_12a

    #+BEGIN_SRC python
      #export
      from exp.nb_12a import *
    #+END_SRC

*** import torchaudio, torchaudio.transforms

    #+BEGIN_SRC python
      import torchaudio
      from torchaudio import transforms
    #+END_SRC

*** AudioList

    #+BEGIN_SRC python
      #export
      class AudioList(ItemList):
          @classmethod
          def from_files(cls, path, extensions=None, recurse=True, include=None, **kwargs):
              if extensions is None: extensions = AUDIO_EXTS
              return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)
          
          def get(self, fn): 
              sig, sr = torchaudio.load(fn)
              assert sig.size(0) == 1, "Non-mono audio detected, mono only supported for now."
              return (sig, sr)
    #+END_SRC

    - `sig` is signal
    - `sr` is sampling rate

*** re_labeler

    #+BEGIN_SRC python
      #export
      def re_labeler(fn, pat): return re.findall(pat, str(fn))[0]
    #+END_SRC

*** show_audio
    #+BEGIN_SRC python
      #export
      from IPython.display import Audio
      def show_audio(ad):
          sig,sr=ad
          display(Audio(data=sig, rate=sr))
    #+END_SRC

    - `show_audio` is to display mini audio player
    
*** show_audio_in_out

    #+BEGIN_SRC python
      #export
      def show_audio_in_out(orig, trans):
          """Helper to plot input and output signal in different colors"""
          osig,osr = orig
          tsig,tsr = trans
          print("↓ Original ↓")
          show_audio(orig)
          print("↓ Transformed ↓")
          show_audio(trans)
          if orig is not None: plt.plot(osig[0], 'm', label="Orig.")
          if trans is not None: plt.plot(tsig[0], 'c', alpha=0.5, label="Transf.")
          plt.legend()
          plt.show()
    #+END_SRC

    - `show_audio_in_out` do two things for each of original&transformed
      audio 
      - displays mini audio player by `show_audio`
      - plot the spectgram by `plt.plot(...)`

*** ToCuda

    #+BEGIN_SRC python
      #export
      class ToCuda(Transform):
          _order=10
          def __call__(self, ad):
              sig,sr = ad
              return (sig.cuda(), sr)
    #+END_SRC

*** PadOrTrim

    #+BEGIN_SRC python
      #export
      class PadOrTrim(Transform):
          _order=11
          def __init__(self,msecs): self.msecs = msecs
          def __call__(self, ad): 
              sig,sr = ad
              mx = sr//1000 * self.msecs
              return (transforms.PadTrim(mx)(sig), sr)
    #+END_SRC

    - `msecs` is the duration in milliseconds after pading/trimming
    - `sr` is a number of samples per second, 
      so sr//1000 is a number of samples per milliseconds
    - `mx` will be number of samples in the duration `msecs`

*** PadOrTrim modified
    #+BEGIN_SRC python
      #export
      class PadOrTrim(Transform):
          _order=11
          def __init__(self,msecs): self.msecs = msecs
          def __call__(self, ad): 
              sig,sr = ad
              mx = sr//1000 * self.msecs
              number_of_pad = mx-len(sig[0])
              return (torch.nn.functional.pad(sig, (0, number_of_pad), 'constant', 0), sr)
     #+END_SRC

    - `msecs` is a duration in milliseconds after padding/trimming
    - `sr` is a number of samples per 1 second
    - `sr//1000` is a number of samples per milliseconds
    - `mx` is a number of samples after padding/trimming
    - len(sig[0]) is a numbber of samples in the signal
    - `mx-len(sig[0])` is a number of samples to pad

    https://github.com/pytorch/audio/releases
    https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.pad

*** using PadOrTrim

    #+BEGIN_SRC python
      pt = PadOrTrim(3000) ## duration in milliseconds
      show_audio_in_out(ll.train[0][0], pt(ll.train[0][0]))
    #+END_SRC

*** SignalShift

    #+BEGIN_SRC python
      #export
      class SignalShift(Transform):
          _order=20
          def __init__(self, max_shift_pct=.6): self.max_shift_pct = max_shift_pct
          def __call__(self, ad):
              sig,sr = ad
              roll_by = int(random.random()*self.max_shift_pct*len(sig[0]))
              return (sig.roll(roll_by), sr)
    #+END_SRC

*** how to use SignalShift

    #+BEGIN_SRC python
      shifter = SignalShift()
      show_audio_in_out(ll.train[0][0], shifter(ll.train[0][0]))
    #+END_SRC

*** what spectrogram is
    # intro to spectrogram
    https://www.youtube.com/watch?v=_FatxGN3vAM


*** TODO Spectrogrammer modified with AmplitudeToDB

    #+BEGIN_SRC python
      #export
      class Spectrogrammer(Transform):
          _order=90
          def __init__(self, to_mel=True, to_db=True, n_fft=400, ws=None, hop=None, 
                       f_min=0.0, f_max=None, pad=0, n_mels=128, top_db=None, normalize=False):
              self.to_mel, self.to_db, self.n_fft, self.ws, self.hop, self.f_min, self.f_max, \
              self.pad, self.n_mels, self.top_db, self.normalize = to_mel, to_db, n_fft, \
              ws, hop, f_min, f_max, pad, n_mels, top_db, normalize

          def __call__(self, ad):
              sig,sr = ad
              if self.to_mel:
                  spec = transforms.MelSpectrogram(sr, self.n_fft, self.ws, self.hop, self.f_min, 
                                                   self.f_max, self.pad, self.n_mels)(sig)
              else: 
                  spec = transforms.Spectrogram(self.n_fft, self.ws, self.hop, self.pad, 
                                                normalize=self.normalize)(sig)
              if self.to_db:
                  spec = transforms.AmplitudeToDB(top_db=self.top_db)(spec)
              spec = spec.permute(0,1,2) # reshape so it looks good to humans
              return spec
    #+END_SRC

    - https://github.com/pytorch/audio/releases

    - `Spectrogram`
      Create a spectrogram from a audio signal.
      - `n_ftt`: size of FFT
      - `win_length`: Window size
      - `hop_length`: length of hop between STFT windows
      - `pad` (int, optional) – Two sided padding of signal. (Default: 0)
      - window_fn (Callable[.., Tensor], optional) – 
        A function to create a window tensor that is 
        applied/multiplied to each frame/window. 
        (Default: torch.hann_window)
      - power (float or None, optional) – Exponent for the 
        magnitude spectrogram, (must be > 0) e.g., 
        1 for energy, 2 for power, etc.
        If None, then the complex spectrum is returned 
        instead. (Default: 2)
      - normalized (bool, optional) – Whether to normalize by 
        magnitude after stft. (Default: False)

    - `MelSpectrogram`
      https://pytorch.org/audio/transforms.html

    - MelScale
      Turn a normal STFT into a mel frequency STFT, 
      using a conversion matrix. This uses triangular filter banks.

    - `AmplitudeToDB`
      Turn a tensor from the power/amplitude scale to the 
      decibel scale.
      This output depends on the maximum value in the input 
      tensor, and so may return different values for an audio 
      clip split into snippets vs. a a full clip.

*** show_spectro

    #+BEGIN_SRC python
      #export
      def show_spectro(img, ax=None, figsize=(6,6), with_shape=True):
          if hasattr(img,"device") & str(img.device).startswith("cuda"): img = img.cpu()
          if ax is None: _,ax = plt.subplots(1, 1, figsize=figsize)
          ax.imshow(img if (img.shape[0]==3) else img.squeeze(0))
          if with_shape: display(f'Tensor shape={img.shape}')
    #+END_SRC



*** TODO SpecAugment

    #+BEGIN_SRC python
      #export
      class SpecAugment(Transform):
          _order=99
          def __init__(self, max_mask_pct=0.2, freq_masks=1, time_masks=1, replace_with_zero=False):
              self.max_mask_pct, self.freq_masks, self.time_masks, self.replace_with_zero = \
              max_mask_pct, freq_masks, time_masks, replace_with_zero
              if not 0 <= self.max_mask_pct <= 1.0: 
                  raise ValueError( f"max_mask_pct must be between 0.0 and 1.0, but it's {self.max_mask_pct}")

          def __call__(self, spec):
              _, n_mels, n_steps = spec.shape
              F = math.ceil(n_mels * self.max_mask_pct) # rounding up in case of small %
              T = math.ceil(n_steps * self.max_mask_pct)
              fill = 0 if self.replace_with_zero else spec.mean()
              for i in range(0, self.freq_masks):
                  f = random.randint(0, F)
                  f0 = random.randint(0, n_mels-f)
                  spec[0][f0:f0+f] = fill
              for i in range(0, self.time_masks):
                  t = random.randint(0, T)
                  t0 = random.randint(0, n_steps-t)
                  spec[0][:,t0:t0+t] = fill
              return spec
    #+END_SRC

    - explanation 
      https://ai.googleblog.com/2019/04/specaugment-new-data-augmentation.html

    - 

*** show_batch

    #+BEGIN_SRC python
      #export
      def show_batch(x, c=4, r=None, figsize=None, shower=show_image):
          n = len(x)
          if r is None: r = int(math.ceil(n/c))
          if figsize is None: figsize=(c*3,r*3)
          fig,axes = plt.subplots(r,c, figsize=figsize)
          for xi,ax in zip(x,axes.flat): shower(xi, ax)
    #+END_SRC

    - `c` is the number of the columns to display
    - `r` is the number of the rows to display
    - `n` is the batch size

    - `shower`: example is `partial(show_spectro, with_shape=False)`


* NLP course
  https://www.fast.ai/2019/07/08/fastai-nlp/

* NLP playlist
  https://www.youtube.com/playlist?list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9

* NLP resource
*** list of NLP tasks
    https://engineering.linecorp.com/ja/blog/overview-2018-language-models/#

*** ULMFiT case study
    https://ariseanalytics.com/activities/report/20190707/
*** article about bert
    https://xtech.nikkei.com/atcl/nxt/mag/nc/18/120400145/120400002/
*** copus
    https://www.jstage.jst.go.jp/article/jnlp/17/5/17_5_5_75/_pdf
    https://pj.ninjal.ac.jp/corpus_center/csj/
* Small Data Learning
*** A Close Look at Deep Learning with Small Data
    https://arxiv.org/abs/2003.12843

*** specialized deep learning

*** breaking the curse of small datasets in machine learning
    # part1
    https://towardsdatascience.com/breaking-the-curse-of-small-datasets-in-machine-learning-part-1-36f28b0c044d

    # part2
    https://towardsdatascience.com/breaking-the-curse-of-small-data-sets-in-machine-learning-part-2-894aa45277f4


*** a
    https://towardsdatascience.com/how-to-use-deep-learning-even-with-small-data-e7f34b673987

*** Three Tricks to Amplify Small Data for Deep Learning
    https://www.datanami.com/2020/03/10/three-tricks-to-amplify-small-data-for-deep-learning/

*** Small Data Deep Learning: AI Applied to Domain Datasets
    https://www.nist.gov/system/files/documents/2019/08/27/workshopslides-small_data_convnets.pdf

*** Using deep neural network with small dataset to predict material defects
    https://www.sciencedirect.com/science/article/pii/S0264127518308682

* video classification
*** a
    https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/

* math resource
*** 3Blue1Brown ()
    https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw/videos

*** Fourier Transform
    # Learn Fourier Transform with Yaruo
    http://www.ic.is.tohoku.ac.jp/~swk/lecture/yaruodsp/main.html

    # 3b1b Fourie Transform. A visual introduction
    https://www.youtube.com/watch?v=spUNpyF58BY

    # 3b1b Fourier seriese
    https://www.youtube.com/watch?v=r6sGWTCMz2k


    # The Discrete Fourier Transform
    http://www.robots.ox.ac.uk/~sjrob/Teaching/SP/l7.pdf

    # Steve Brunton, Discrete Fourier Transform
    https://www.youtube.com/watch?v=KTj1YgeN2sY


    # Steve Brunton, Discrete Fourie Transform
    https://www.youtube.com/watch?v=nl9TZanwbBk
    
    # Steve Brunton, Fast Fourier Transform Algorithm
    https://www.youtube.com/watch?v=toj_IoCQE-4


    # Short-time Fourie Transform and Spectrogram
    https://www.youtube.com/watch?v=NA0TwPsECUQ

    # Simon Xu, Discrete Fourier Transform
    https://www.youtube.com/watch?v=mkGsMWi_j4Q

    # Simon Xu, Fast Fourier Transform
    https://www.youtube.com/watch?v=htCj9exbGo0

    # toward data science Fast Fourier Transform
    https://towardsdatascience.com/fast-fourier-transform-937926e591cb

    # LeiosOS Fourier Transform
    https://www.youtube.com/watch?v=ykNtIbtCR-8

    # LeiosOS What is a Fast Fourier Transform (FFT)? The Cooley-Tukey Algorithm
    https://www.youtube.com/watch?v=XtypWS8HZco

    # Brian Douglas, introduction to the Fourie Transform (Part1)
    https://www.youtube.com/watch?v=1JnayXHhjlg






* ========
* fastai 2019 courses
    https://course19.fast.ai/part2

* fastai 2020 courses
  https://course.fast.ai/

* andres ng lecture videos on youtube
** playlists
   https://www.youtube.com/channel/UCcIXc5mJsHVYTZR1maL5l9w/playlists

** course2 
*** playlists
    https://www.youtube.com/watch?v=tNIpEZLv_eg&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=27

*** BatchNorm 


*** regularization (C2W1L04)
    https://www.youtube.com/watch?v=6g0t3Phly2M&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=4 

*** vanishing/exploding gradients (C2W1L10)
    https://www.youtube.com/watch?v=qhXZsFVxGKo

    https://www.youtube.com/watch?v=qO_NLVjD6zE


*** exponentially weighted average (C2W2L03)
    https://www.youtube.com/watch?v=lAq96T8FkTw&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=17


*** debiasing (bias correction) (C2W2L05)
    https://www.youtube.com/watch?v=lWzo8CajF5s&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=19



* misc resource    
*** how you should read research papers
    https://link.medium.com/JPM8q4SSN7

*** cosine loss
    https://towardsdatascience.com/how-to-use-deep-learning-even-with-small-data-e7f34b673987

*** fine-grained classification
    https://books.google.co.jp/books?id=y0yuDwAAQBAJ&pg=PA57&lpg=PA57&dq=detect+subtle+difference+deep+learning&source=bl&ots=IrSDz4-3x_&sig=ACfU3U3TPcO8CM496-NSusruJ_ptadyfFQ&hl=en&sa=X&redir_esc=y#v=onepage&q=detect%20subtle%20difference%20deep%20learning&f=false

*** train GAN with less data
    https://analyticsindiamag.com/can-we-train-gans-with-less-data/


*** attention and memory
    https://www.youtube.com/watch?v=AIiwuClvH6k&feature=youtu.be


